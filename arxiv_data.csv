titles,summaries,terms,word_count,unique_word_count
Neural ODEs for Image Segmentation with Level Sets,"We propose a novel approach for image segmentation that combines Neural
Ordinary Differential Equations (NODEs) and the Level Set method. Our approach
parametrizes the evolution of an initial contour with a NODE that implicitly
learns from data a speed function describing the evolution. In addition, for
cases where an initial contour is not available and to alleviate the need for
careful choice or design of contour embedding functions, we propose a
NODE-based method that evolves an image embedding into a dense per-pixel
semantic label space. We evaluate our methods on kidney segmentation (KiTS19)
and on salient object detection (PASCAL-S, ECSSD and HKU-IS). In addition to
improving initial contours provided by deep learning models while using a
fraction of their number of parameters, our approach achieves F scores that are
higher than several state-of-the-art deep learning algorithms.","['cs.CV', 'cs.LG', 'eess.IV']",143,99
A Graph-CNN for 3D Point Cloud Classification,"Graph convolutional neural networks (Graph-CNNs) extend traditional CNNs to
handle data that is supported on a graph. Major challenges when working with
data on graphs are that the support set (the vertices of the graph) do not
typically have a natural ordering, and in general, the topology of the graph is
not regular (i.e., vertices do not all have the same number of neighbors).
Thus, Graph-CNNs have huge potential to deal with 3D point cloud data which has
been obtained from sampling a manifold. In this paper, we develop a Graph-CNN
for classifying 3D point cloud data, called PointGCN. The architecture combines
localized graph convolutions with two types of graph downsampling operations
(also known as pooling). By the effective exploration of the point cloud local
structure using the Graph-CNN, the proposed architecture achieves competitive
performance on the 3D object classification benchmark ModelNet, and our
architecture is more stable than competing schemes.","['cs.CV', 'cs.LG', 'stat.ML']",156,102
Filter Design and Performance Evaluation for Fingerprint Image Segmentation,"Fingerprint recognition plays an important role in many commercial
applications and is used by millions of people every day, e.g. for unlocking
mobile phones. Fingerprint image segmentation is typically the first processing
step of most fingerprint algorithms and it divides an image into foreground,
the region of interest, and background. Two types of error can occur during
this step which both have a negative impact on the recognition performance:
'true' foreground can be labeled as background and features like minutiae can
be lost, or conversely 'true' background can be misclassified as foreground and
spurious features can be introduced. The contribution of this paper is
threefold: firstly, we propose a novel factorized directional bandpass (FDB)
segmentation method for texture extraction based on the directional Hilbert
transform of a Butterworth bandpass (DHBB) filter interwoven with
soft-thresholding. Secondly, we provide a manually marked ground truth
segmentation for 10560 images as an evaluation benchmark. Thirdly, we conduct a
systematic performance comparison between the FDB method and four of the most
often cited fingerprint segmentation algorithms showing that the FDB
segmentation method clearly outperforms these four widely used methods. The
benchmark and the implementation of the FDB method are made publicly available.",['cs.CV'],199,125
Boosting Few-Shot Classification with View-Learnable Contrastive Learning,"The goal of few-shot classification is to classify new categories with few
labeled examples within each class. Nowadays, the excellent performance in
handling few-shot classification problems is shown by metric-based
meta-learning methods. However, it is very hard for previous methods to
discriminate the fine-grained sub-categories in the embedding space without
fine-grained labels. This may lead to unsatisfactory generalization to
fine-grained subcategories, and thus affects model interpretation. To tackle
this problem, we introduce the contrastive loss into few-shot classification
for learning latent fine-grained structure in the embedding space. Furthermore,
to overcome the drawbacks of random image transformation used in current
contrastive learning in producing noisy and inaccurate image pairs (i.e.,
views), we develop a learning-to-learn algorithm to automatically generate
different views of the same image. Extensive experiments on standard few-shot
learning benchmarks demonstrate the superiority of our method.",['cs.CV'],151,99
Optimizing Quantum Variational Circuits with Deep Reinforcement Learning,"Quantum Machine Learning (QML) is considered to be one of the most promising
applications of near term quantum devices. However, the optimization of quantum
machine learning models presents numerous challenges arising from the
imperfections of hardware and the fundamental obstacles in navigating an
exponentially scaling Hilbert space. In this work, we evaluate the potential of
contemporary methods in deep reinforcement learning to augment gradient based
optimization routines in quantum variational circuits. We find that
reinforcement learning augmented optimizers consistently outperform gradient
descent in noisy environments. All code and pretrained weights are available to
replicate the results or deploy the models at
https://github.com/lockwo/rl_qvc_opt.","['cs.LG', 'quant-ph']",106,82
Constrained Policy Optimization,"For many applications of reinforcement learning it can be more convenient to
specify both a reward function and constraints, rather than trying to design
behavior through the reward function. For example, systems that physically
interact with or around humans should satisfy safety constraints. Recent
advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015,
Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in
high-dimensional control, but do not consider the constrained setting.
  We propose Constrained Policy Optimization (CPO), the first general-purpose
policy search algorithm for constrained reinforcement learning with guarantees
for near-constraint satisfaction at each iteration. Our method allows us to
train neural network policies for high-dimensional control while making
guarantees about policy behavior all throughout training. Our guarantees are
based on a new theoretical result, which is of independent interest: we prove a
bound relating the expected returns of two policies to an average divergence
between them. We demonstrate the effectiveness of our approach on simulated
robot locomotion tasks where the agent must satisfy constraints motivated by
safety.",['cs.LG'],180,131
Multi-modal Visual Tracking: Review and Experimental Comparison,"Visual object tracking, as a fundamental task in computer vision, has drawn
much attention in recent years. To extend trackers to a wider range of
applications, researchers have introduced information from multiple modalities
to handle specific scenes, which is a promising research prospect with emerging
methods and benchmarks. To provide a thorough review of multi-modal track-ing,
we summarize the multi-modal tracking algorithms, especially visible-depth
(RGB-D) tracking and visible-thermal (RGB-T) tracking in a unified taxonomy
from different aspects. Second, we provide a detailed description of the
related benchmarks and challenges. Furthermore, we conduct extensive
experiments to analyze the effectiveness of trackers on five datasets: PTB,
VOT19-RGBD, GTOT, RGBT234, and VOT19-RGBT. Finally, we discuss various future
directions from different perspectives, including model design and dataset
construction for further research.",['cs.CV'],136,99
Boost Image Captioning with Knowledge Reasoning,"Automatically generating a human-like description for a given image is a
potential research in artificial intelligence, which has attracted a great of
attention recently. Most of the existing attention methods explore the mapping
relationships between words in sentence and regions in image, such
unpredictable matching manner sometimes causes inharmonious alignments that may
reduce the quality of generated captions. In this paper, we make our efforts to
reason about more accurate and meaningful captions. We first propose word
attention to improve the correctness of visual attention when generating
sequential descriptions word-by-word. The special word attention emphasizes on
word importance when focusing on different regions of the input image, and
makes full use of the internal annotation knowledge to assist the calculation
of visual attention. Then, in order to reveal those incomprehensible intentions
that cannot be expressed straightforwardly by machines, we introduce a new
strategy to inject external knowledge extracted from knowledge graph into the
encoder-decoder framework to facilitate meaningful captioning. Finally, we
validate our model on two freely available captioning benchmarks: Microsoft
COCO dataset and Flickr30k dataset. The results demonstrate that our approach
achieves state-of-the-art performance and outperforms many of the existing
approaches.",['cs.CV'],199,134
A Novel Visual Fault Detection and Classification System for Semiconductor Manufacturing Using Stacked Hybrid Convolutional Neural Networks,"Automated visual inspection in the semiconductor industry aims to detect and
classify manufacturing defects utilizing modern image processing techniques.
While an earliest possible detection of defect patterns allows quality control
and automation of manufacturing chains, manufacturers benefit from an increased
yield and reduced manufacturing costs. Since classical image processing systems
are limited in their ability to detect novel defect patterns, and machine
learning approaches often involve a tremendous amount of computational effort,
this contribution introduces a novel deep neural network based hybrid approach.
Unlike classical deep neural networks, a multi-stage system allows the
detection and classification of the finest structures in pixel size within
high-resolution imagery. Consisting of stacked hybrid convolutional neural
networks (SH-CNN) and inspired by current approaches of visual attention, the
realized system draws the focus over the level of detail from its structures to
more task-relevant areas of interest. The results of our test environment show
that the SH-CNN outperforms current approaches of learning-based automated
visual inspection, whereas a distinction depending on the level of detail
enables the elimination of defect patterns in earlier stages of the
manufacturing process.","['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML']",188,119
A self-adapting super-resolution structures framework for automatic design of GAN,"With the development of deep learning, the single super-resolution image
reconstruction network models are becoming more and more complex. Small changes
in hyperparameters of the models have a greater impact on model performance. In
the existing works, experts have gradually explored a set of optimal model
parameters based on empirical values or performing brute-force search. In this
paper, we introduce a new super-resolution image reconstruction generative
adversarial network framework, and a Bayesian optimization method used to
optimizing the hyperparameters of the generator and discriminator. The
generator is made by self-calibrated convolution, and discriminator is made by
convolution lays. We have defined the hyperparameters such as the number of
network layers and the number of neurons. Our method adopts Bayesian
optimization as a optimization policy of GAN in our model. Not only can find
the optimal hyperparameter solution automatically, but also can construct a
super-resolution image reconstruction network, reducing the manual workload.
Experiments show that Bayesian optimization can search the optimal solution
earlier than the other two optimization algorithms.","['cs.CV', 'cs.LG', 'eess.IV']",173,103
Towards Domain-Agnostic Contrastive Learning,"Despite recent success, most contrastive self-supervised learning methods are
domain-specific, relying heavily on data augmentation techniques that require
knowledge about a particular domain, such as image cropping and rotation. To
overcome such limitation, we propose a novel domain-agnostic approach to
contrastive learning, named DACL, that is applicable to domains where
invariances, and thus, data augmentation techniques, are not readily available.
Key to our approach is the use of Mixup noise to create similar and dissimilar
examples by mixing data samples differently either at the input or hidden-state
levels. To demonstrate the effectiveness of DACL, we conduct experiments across
various domains such as tabular data, images, and graphs. Our results show that
DACL not only outperforms other domain-agnostic noising methods, such as
Gaussian-noise, but also combines well with domain-specific methods, such as
SimCLR, to improve self-supervised visual representation learning. Finally, we
theoretically analyze our method and show advantages over the Gaussian-noise
based contrastive learning approach.","['cs.LG', 'cs.AI', 'stat.ML']",163,105
Explaining Algorithmic Fairness Through Fairness-Aware Causal Path Decomposition,"Algorithmic fairness has aroused considerable interests in data mining and
machine learning communities recently. So far the existing research has been
mostly focusing on the development of quantitative metrics to measure algorithm
disparities across different protected groups, and approaches for adjusting the
algorithm output to reduce such disparities. In this paper, we propose to study
the problem of identification of the source of model disparities. Unlike
existing interpretation methods which typically learn feature importance, we
consider the causal relationships among feature variables and propose a novel
framework to decompose the disparity into the sum of contributions from
fairness-aware causal paths, which are paths linking the sensitive attribute
and the final predictions, on the graph. We also consider the scenario when the
directions on certain edges within those paths cannot be determined. Our
framework is also model agnostic and applicable to a variety of quantitative
disparity measures. Empirical evaluations on both synthetic and real-world data
sets are provided to show that our method can provide precise and comprehensive
explanations to the model disparities.",['cs.LG'],174,117
DeGraF-Flow: Extending DeGraF Features for accurate and efficient sparse-to-dense optical flow estimation,"Modern optical flow methods make use of salient scene feature points detected
and matched within the scene as a basis for sparse-to-dense optical flow
estimation. Current feature detectors however either give sparse, non uniform
point clouds (resulting in flow inaccuracies) or lack the efficiency for
frame-rate real-time applications. In this work we use the novel Dense Gradient
Based Features (DeGraF) as the input to a sparse-to-dense optical flow scheme.
This consists of three stages: 1) efficient detection of uniformly distributed
Dense Gradient Based Features (DeGraF); 2) feature tracking via robust local
optical flow; and 3) edge preserving flow interpolation to recover overall
dense optical flow. The tunable density and uniformity of DeGraF features yield
superior dense optical flow estimation compared to other popular feature
detectors within this three stage pipeline. Furthermore, the comparable speed
of feature detection also lends itself well to the aim of real-time optical
flow recovery. Evaluation on established real-world benchmark datasets show
test performance in an autonomous vehicle setting where DeGraF-Flow shows
promising results in terms of accuracy with competitive computational
efficiency among non-GPU based methods, including a marked increase in speed
over the conceptually similar EpicFlow approach.","['cs.CV', 'cs.AI']",202,131
Analysing domain shift factors between videos and images for object detection,"Object detection is one of the most important challenges in computer vision.
Object detectors are usually trained on bounding-boxes from still images.
Recently, video has been used as an alternative source of data. Yet, for a
given test domain (image or video), the performance of the detector depends on
the domain it was trained on. In this paper, we examine the reasons behind this
performance gap. We define and evaluate different domain shift factors: spatial
location accuracy, appearance diversity, image quality and aspect distribution.
We examine the impact of these factors by comparing performance before and
after factoring them out. The results show that all four factors affect the
performance of the detectors and their combined effect explains nearly the
whole performance gap.",['cs.CV'],124,90
Deep Tiered Image Segmentation For Detecting Internal Ice Layers in Radar Imagery,"Understanding the structure of Earth's polar ice sheets is important for
modeling how global warming will impact polar ice and, in turn, the Earth's
climate. Ground-penetrating radar is able to collect observations of the
internal structure of snow and ice, but the process of manually labeling these
observations is slow and laborious. Recent work has developed automatic
techniques for finding the boundaries between the ice and the bedrock, but
finding internal layers - the subtle boundaries that indicate where one year's
ice accumulation ended and the next began - is much more challenging because
the number of layers varies and the boundaries often merge and split. In this
paper, we propose a novel deep neural network for solving a general class of
tiered segmentation problems. We then apply it to detecting internal layers in
polar ice, evaluating on a large-scale dataset of polar ice radar data with
human-labeled annotations as ground truth.",['cs.CV'],156,102
Color-based Segmentation of Sky/Cloud Images From Ground-based Cameras,"Sky/cloud images captured by ground-based cameras (a.k.a. whole sky imagers)
are increasingly used nowadays because of their applications in a number of
fields, including climate modeling, weather prediction, renewable energy
generation, and satellite communications. Due to the wide variety of cloud
types and lighting conditions in such images, accurate and robust segmentation
of clouds is challenging. In this paper, we present a supervised segmentation
framework for ground-based sky/cloud images based on a systematic analysis of
different color spaces and components, using partial least squares (PLS)
regression. Unlike other state-of-the-art methods, our proposed approach is
entirely learning-based and does not require any manually-defined parameters.
In addition, we release the Singapore Whole Sky IMaging SEGmentation Database
(SWIMSEG), a large database of annotated sky/cloud images, to the research
community.",['cs.CV'],138,101
WAX-ML: A Python library for machine learning and feedback loops on streaming data,"Wax is what you put on a surfboard to avoid slipping. It is an essential tool
to go surfing... We introduce WAX-ML a research-oriented Python library
providing tools to design powerful machine learning algorithms and feedback
loops working on streaming data. It strives to complement JAX with tools
dedicated to time series. WAX-ML makes JAX-based programs easy to use for
end-users working with pandas and xarray for data manipulation. It provides a
simple mechanism for implementing feedback loops, allows the implementation of
online learning and reinforcement learning algorithms with functions, and makes
them easy to integrate by end-users working with the object-oriented
reinforcement learning framework from the Gym library. It is released with an
Apache open-source license on GitHub at https://github.com/eserie/wax-ml.","['cs.LG', 'cs.CL']",134,87
Alternating linear scheme in a Bayesian framework for low-rank tensor approximation,"Multiway data often naturally occurs in a tensorial format which can be
approximately represented by a low-rank tensor decomposition. This is useful
because complexity can be significantly reduced and the treatment of
large-scale data sets can be facilitated. In this paper, we find a low-rank
representation for a given tensor by solving a Bayesian inference problem. This
is achieved by dividing the overall inference problem into sub-problems where
we sequentially infer the posterior distribution of one tensor decomposition
component at a time. This leads to a probabilistic interpretation of the
well-known iterative algorithm alternating linear scheme (ALS). In this way,
the consideration of measurement noise is enabled, as well as the incorporation
of application-specific prior knowledge and the uncertainty quantification of
the low-rank tensor estimate. To compute the low-rank tensor estimate from the
posterior distributions of the tensor decomposition components, we present an
algorithm that performs the unscented transform in tensor train format.","['cs.LG', 'cs.NA', 'math.NA']",161,99
A Novel Image Segmentation Enhancement Technique based on Active Contour and Topological Alignments,"Topological alignments and snakes are used in image processing, particularly
in locating object boundaries. Both of them have their own advantages and
limitations. To improve the overall image boundary detection system, we focused
on developing a novel algorithm for image processing. The algorithm we propose
to develop will based on the active contour method in conjunction with
topological alignments method to enhance the image detection approach. The
algorithm presents novel technique to incorporate the advantages of both
Topological Alignments and snakes. Where the initial segmentation by
Topological Alignments is firstly transformed into the input of the snake model
and begins its evolvement to the interested object boundary. The results show
that the algorithm can deal with low contrast images and shape cells,
demonstrate the segmentation accuracy under weak image boundaries, which
responsible for lacking accuracy in image detecting techniques. We have
achieved better segmentation and boundary detecting for the image, also the
ability of the system to improve the low contrast and deal with over and under
segmentation.",['cs.CV'],168,95
MAGNeto: An Efficient Deep Learning Method for the Extractive Tags Summarization Problem,"In this work, we study a new image annotation task named Extractive Tags
Summarization (ETS). The goal is to extract important tags from the context
lying in an image and its corresponding tags. We adjust some state-of-the-art
deep learning models to utilize both visual and textual information. Our
proposed solution consists of different widely used blocks like convolutional
and self-attention layers, together with a novel idea of combining auxiliary
loss functions and the gating mechanism to glue and elevate these fundamental
components and form a unified architecture. Besides, we introduce a loss
function that aims to reduce the imbalance of the training data and a simple
but effective data augmentation technique dedicated to alleviates the effect of
outliers on the final results. Last but not least, we explore an unsupervised
pre-training strategy to further boost the performance of the model by making
use of the abundant amount of available unlabeled data. Our model shows the
good results as 90% $F_\text{1}$ score on the public NUS-WIDE benchmark, and
50% $F_\text{1}$ score on a noisy large-scale real-world private dataset.
Source code for reproducing the experiments is publicly available at:
https://github.com/pixta-dev/labteam","['cs.CV', 'cs.AI']",205,148
Function-Space Distributions over Kernels,"Gaussian processes are flexible function approximators, with inductive biases
controlled by a covariance kernel. Learning the kernel is the key to
representation learning and strong predictive performance. In this paper, we
develop functional kernel learning (FKL) to directly infer functional
posteriors over kernels. In particular, we place a transformed Gaussian process
over a spectral density, to induce a non-parametric distribution over kernel
functions. The resulting approach enables learning of rich representations,
with support for any stationary kernel, uncertainty over the values of the
kernel, and an interpretable specification of a prior directly over kernels,
without requiring sophisticated initialization or manual intervention. We
perform inference through elliptical slice sampling, which is especially well
suited to marginalizing posteriors with the strongly correlated priors typical
to function space modelling. We develop our approach for non-uniform,
large-scale, multi-task, and multidimensional data, and show promising
performance in a wide range of settings, including interpolation,
extrapolation, and kernel recovery experiments.","['cs.LG', 'stat.ML']",158,109
Graph Convolutional Neural Networks via Motif-based Attention,"Many real-world problems can be represented as graph-based learning problems.
In this paper, we propose a novel framework for learning spatial and
attentional convolution neural networks on arbitrary graphs. Different from
previous convolutional neural networks on graphs, we first design a
motif-matching guided subgraph normalization method to capture neighborhood
information. Then we implement subgraph-level self-attentional layers to learn
different importances from different subgraphs to solve graph classification
problems. Analogous to image-based attentional convolution networks that
operate on locally connected and weighted regions of the input, we also extend
graph normalization from one-dimensional node sequence to two-dimensional node
grid by leveraging motif-matching, and design self-attentional layers without
requiring any kinds of cost depending on prior knowledge of the graph
structure. Our results on both bioinformatics and social network datasets show
that we can significantly improve graph classification benchmarks over
traditional graph kernel and existing deep models.",['cs.LG'],155,103
Fine-grained Semantic Constraint in Image Synthesis,"In this paper, we propose a multi-stage and high-resolution model for image
synthesis that uses fine-grained attributes and masks as input. With a
fine-grained attribute, the proposed model can detailedly constrain the
features of the generated image through rich and fine-grained semantic
information in the attribute. With mask as prior, the model in this paper is
constrained so that the generated images conform to visual senses, which will
reduce the unexpected diversity of samples generated from the generative
adversarial network. This paper also proposes a scheme to improve the
discriminator of the generative adversarial network by simultaneously
discriminating the total image and sub-regions of the image. In addition, we
propose a method for optimizing the labeled attribute in datasets, which
reduces the manual labeling noise. Extensive quantitative results show that our
image synthesis model generates more realistic images.",['cs.CV'],144,87
A Public Ground-Truth Dataset for Handwritten Circuit Diagram Images,"The development of digitization methods for line drawings (especially in the
area of electrical engineering) relies on the availability of publicly
available training and evaluation data. This paper presents such an image set
along with annotations. The dataset consists of 1152 images of 144 circuits by
12 drafters and 48 563 annotations. Each of these images depicts an electrical
circuit diagram, taken by consumer grade cameras under varying lighting
conditions and perspectives. A variety of different pencil types and surface
materials has been used. For each image, all individual electrical components
are annotated with bounding boxes and one out of 45 class labels. In order to
simplify a graph extraction process, different helper symbols like junction
points and crossovers are introduced, while texts are annotated as well. The
geometric and taxonomic problems arising from this task as well as the classes
themselves and statistics of their appearances are stated. The performance of a
standard Faster RCNN on the dataset is provided as an object detection
baseline.",['cs.CV'],166,123
Soft-IntroVAE: Analyzing and Improving the Introspective Variational Autoencoder,"The recently introduced introspective variational autoencoder (IntroVAE)
exhibits outstanding image generations, and allows for amortized inference
using an image encoder. The main idea in IntroVAE is to train a VAE
adversarially, using the VAE encoder to discriminate between generated and real
data samples. However, the original IntroVAE loss function relied on a
particular hinge-loss formulation that is very hard to stabilize in practice,
and its theoretical convergence analysis ignored important terms in the loss.
In this work, we take a step towards better understanding of the IntroVAE
model, its practical implementation, and its applications. We propose the
Soft-IntroVAE, a modified IntroVAE that replaces the hinge-loss terms with a
smooth exponential loss on generated samples. This change significantly
improves training stability, and also enables theoretical analysis of the
complete algorithm. Interestingly, we show that the IntroVAE converges to a
distribution that minimizes a sum of KL distance from the data distribution and
an entropy term. We discuss the implications of this result, and demonstrate
that it induces competitive image generation and reconstruction. Finally, we
describe two applications of Soft-IntroVAE to unsupervised image translation
and out-of-distribution detection, and demonstrate compelling results. Code and
additional information is available on the project website --
https://taldatech.github.io/soft-intro-vae-web","['cs.LG', 'cs.AI', 'cs.CV']",213,131
TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding,"Recently proposed fine-grained 3D visual grounding is an essential and
challenging task, whose goal is to identify the 3D object referred by a natural
language sentence from other distractive objects of the same category. Existing
works usually adopt dynamic graph networks to indirectly model the
intra/inter-modal interactions, making the model difficult to distinguish the
referred object from distractors due to the monolithic representations of
visual and linguistic contents. In this work, we exploit Transformer for its
natural suitability on permutation-invariant 3D point clouds data and propose a
TransRefer3D network to extract entity-and-relation aware multimodal context
among objects for more discriminative feature learning. Concretely, we devise
an Entity-aware Attention (EA) module and a Relation-aware Attention (RA)
module to conduct fine-grained cross-modal feature matching. Facilitated by
co-attention operation, our EA module matches visual entity features with
linguistic entity features while RA module matches pair-wise visual relation
features with linguistic relation features, respectively. We further integrate
EA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and
stack several ERCBs to form our TransRefer3D for hierarchical multimodal
context modeling. Extensive experiments on both Nr3D and Sr3D datasets
demonstrate that our proposed model significantly outperforms existing
approaches by up to 10.6% and claims the new state-of-the-art. To the best of
our knowledge, this is the first work investigating Transformer architecture
for fine-grained 3D visual grounding task.","['cs.CV', 'cs.AI']",242,145
Low-dimensional statistical manifold embedding of directed graphs,"We propose a novel node embedding of directed graphs to statistical
manifolds, which is based on a global minimization of pairwise relative entropy
and graph geodesics in a non-linear way. Each node is encoded with a
probability density function over a measurable space. Furthermore, we analyze
the connection between the geometrical properties of such embedding and their
efficient learning procedure. Extensive experiments show that our proposed
embedding is better in preserving the global geodesic information of graphs, as
well as outperforming existing embedding models on directed graphs in a variety
of evaluation metrics, in an unsupervised setting.","['cs.LG', 'stat.ML']",98,71
Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation,"Accurate delineation of pathological lungs from computed tomography (CT)
images remains mostly unsolved because available methods fail to provide a
reliable generic solution due to high variability of abnormality appearance.
Local descriptor-based classification methods have shown to work well in
annotating pathologies; however, these methods are usually computationally
intensive which restricts their widespread use in real-time or near-real-time
clinical applications. In this paper, we present a novel approach for fast,
accurate, reliable segmentation of pathological lungs from CT scans by
combining region-based segmentation method with local descriptor classification
that is performed on an optimized sampling grid. Our method works in two
stages; during stage one, we adapted the fuzzy connectedness (FC) image
segmentation algorithm to perform initial lung parenchyma extraction. In the
second stage, texture-based local descriptors are utilized to segment abnormal
imaging patterns using a near optimal keypoint analysis by employing centroid
of supervoxel as grid points. The quantitative results show that our
pathological lung segmentation method is fast, robust, and improves on current
standards and has potential to enhance the performance of routine clinical
tasks.",['cs.CV'],183,134
GradTS: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks,"A key problem in multi-task learning (MTL) research is how to select
high-quality auxiliary tasks automatically. This paper presents GradTS, an
automatic auxiliary task selection method based on gradient calculation in
Transformer-based models. Compared to AUTOSEM, a strong baseline method, GradTS
improves the performance of MT-DNN with a bert-base-cased backend model, from
0.33% to 17.93% on 8 natural language understanding (NLU) tasks in the GLUE
benchmarks. GradTS is also time-saving since (1) its gradient calculations are
based on single-task experiments and (2) the gradients are re-used without
additional experiments when the candidate task set changes. On the 8 GLUE
classification tasks, for example, GradTS costs on average 21.32% less time
than AUTOSEM with comparable GPU consumption. Further, we show the robustness
of GradTS across various task settings and model selections, e.g. mixed
objectives among candidate tasks. The efficiency and efficacy of GradTS in
these case studies illustrate its general applicability in MTL research without
requiring manual task filtering or costly parameter tuning.","['cs.LG', 'cs.CL']",175,124
Deep Q-Learning for Directed Acyclic Graph Generation,"We present a method to generate directed acyclic graphs (DAGs) using deep
reinforcement learning, specifically deep Q-learning. Generating graphs with
specified structures is an important and challenging task in various
application fields, however most current graph generation methods produce
graphs with undirected edges. We demonstrate that this method is capable of
generating DAGs with topology and node types satisfying specified criteria in
highly sparse reward environments.","['cs.LG', 'stat.ML']",67,54
CSCAD: Correlation Structure-based Collective Anomaly Detection in Complex System,"Detecting anomalies in large complex systems is a critical and challenging
task. The difficulties arise from several aspects. First, collecting ground
truth labels or prior knowledge for anomalies is hard in real-world systems,
which often lead to limited or no anomaly labels in the dataset. Second,
anomalies in large systems usually occur in a collective manner due to the
underlying dependency structure among devices or sensors. Lastly, real-time
anomaly detection for high-dimensional data requires efficient algorithms that
are capable of handling different types of data (i.e. continuous and discrete).
We propose a correlation structure-based collective anomaly detection (CSCAD)
model for high-dimensional anomaly detection problem in large systems, which is
also generalizable to semi-supervised or supervised settings. Our framework
utilize graph convolutional network combining a variational autoencoder to
jointly exploit the feature space correlation and reconstruction deficiency of
samples to perform anomaly detection. We propose an extended mutual information
(EMI) metric to mine the internal correlation structure among different data
features, which enhances the data reconstruction capability of CSCAD. The
reconstruction loss and latent standard deviation vector of a sample obtained
from reconstruction network can be perceived as two natural anomalous degree
measures. An anomaly discriminating network can then be trained using low
anomalous degree samples as positive samples, and high anomalous degree samples
as negative samples. Experimental results on five public datasets demonstrate
that our approach consistently outperforms all the competing baselines.",['cs.LG'],239,149
Channel Attention and Multi-level Features Fusion for Single Image Super-Resolution,"Convolutional neural networks (CNNs) have demonstrated superior performance
in super-resolution (SR). However, most CNN-based SR methods neglect the
different importance among feature channels or fail to take full advantage of
the hierarchical features. To address these issues, this paper presents a novel
recursive unit. Firstly, at the beginning of each unit, we adopt a compact
channel attention mechanism to adaptively recalibrate the channel importance of
input features. Then, the multi-level features, rather than only deep-level
features, are extracted and fused. Additionally, we find that it will force our
model to learn more details by using the learnable upsampling method (i.e.,
transposed convolution) only on residual branch (instead of using it both on
residual branch and identity branch) while using the bicubic interpolation on
the other branch. Analytic experiments show that our method achieves
competitive results compared with the state-of-the-art methods and maintains
faster speed as well.",['cs.CV'],154,113
Retrofitting Distributional Embeddings to Knowledge Graphs with Functional Relations,"Knowledge graphs are a versatile framework to encode richly structured data
relationships, but it can be challenging to combine these graphs with
unstructured data. Methods for retrofitting pre-trained entity representations
to the structure of a knowledge graph typically assume that entities are
embedded in a connected space and that relations imply similarity. However,
useful knowledge graphs often contain diverse entities and relations (with
potentially disjoint underlying corpora) which do not accord with these
assumptions. To overcome these limitations, we present Functional Retrofitting,
a framework that generalizes current retrofitting methods by explicitly
modeling pairwise relations. Our framework can directly incorporate a variety
of pairwise penalty functions previously developed for knowledge graph
completion. Further, it allows users to encode, learn, and extract information
about relation semantics. We present both linear and neural instantiations of
the framework. Functional Retrofitting significantly outperforms existing
retrofitting methods on complex knowledge graphs and loses no accuracy on
simpler graphs (in which relations do imply similarity). Finally, we
demonstrate the utility of the framework by predicting new drug--disease
treatment pairs in a large, complex health knowledge graph.","['stat.ML', 'cs.CL', 'cs.LG']",181,116
Automatic Estimation of Live Coffee Leaf Infection based on Image Processing Techniques,"Image segmentation is the most challenging issue in computer vision
applications. And most difficulties for crops management in agriculture are the
lack of appropriate methods for detecting the leaf damage for pests treatment.
In this paper we proposed an automatic method for leaf damage detection and
severity estimation of coffee leaf by avoiding defoliation. After enhancing the
contrast of the original image using LUT based gamma correction, the image is
processed to remove the background, and the output leaf is clustered using
Fuzzy c-means segmentation in V channel of YUV color space to maximize all leaf
damage detection, and finally, the severity of leaf is estimated in terms of
ratio for leaf pixel distribution between the normal and the detected leaf
damage. The results in each proposed method was compared to the current
researches and the accuracy is obvious either in the background removal or
damage detection.",['cs.CV'],148,90
How many labeled license plates are needed?,"Training a good deep learning model often requires a lot of annotated data.
As a large amount of labeled data is typically difficult to collect and even
more difficult to annotate, data augmentation and data generation are widely
used in the process of training deep neural networks. However, there is no
clear common understanding on how much labeled data is needed to get
satisfactory performance. In this paper, we try to address such a question
using vehicle license plate character recognition as an example application. We
apply computer graphic scripts and Generative Adversarial Networks to generate
and augment a large number of annotated, synthesized license plate images with
realistic colors, fonts, and character composition from a small number of real,
manually labeled license plate images. Generated and augmented data are mixed
and used as training data for the license plate recognition network modified
from DenseNet. The experimental results show that the model trained from the
generated mixed training data has good generalization ability, and the proposed
approach achieves a new state-of-the-art accuracy on Dataset-1 and AOLP, even
with a very limited number of original real license plates. In addition, the
accuracy improvement caused by data generation becomes more significant when
the number of labeled images is reduced. Data augmentation also plays a more
significant role when the number of labeled images is increased.",['cs.CV'],227,133
S2FGAN: Semantically Aware Interactive Sketch-to-Face Translation,"Interactive facial image manipulation attempts to edit single and multiple
face attributes using a photo-realistic face and/or semantic mask as input. In
the absence of the photo-realistic image (only sketch/mask available), previous
methods only retrieve the original face but ignore the potential of aiding
model controllability and diversity in the translation process. This paper
proposes a sketch-to-image generation framework called S2FGAN, aiming to
improve users' ability to interpret and flexibility of face attribute editing
from a simple sketch. The proposed framework modifies the constrained latent
space semantics trained on Generative Adversarial Networks (GANs). We employ
two latent spaces to control the face appearance and adjust the desired
attributes of the generated face. Instead of constraining the translation
process by using a reference image, the users can command the model to retouch
the generated images by involving the semantic information in the generation
process. In this way, our method can manipulate single or multiple face
attributes by only specifying attributes to be changed. Extensive experimental
results on CelebAMask-HQ dataset empirically shows our superior performance and
effectiveness on this task. Our method successfully outperforms
state-of-the-art methods on attribute manipulation by exploiting greater
control of attribute intensity.",['cs.CV'],204,119
Adaptable GAN Encoders for Image Reconstruction via Multi-type Latent Vectors with Two-scale Attentions,"Although current deep generative adversarial networks (GANs) could synthesize
high-quality (HQ) images, discovering novel GAN encoders for image
reconstruction is still favorable. When embedding images to latent space,
existing GAN encoders work well for aligned images (such as the human face),
but they do not adapt to more generalized GANs. To our knowledge, current
state-of-the-art GAN encoders do not have a proper encoder to reconstruct
high-fidelity images from most misaligned HQ synthesized images on different
GANs. Their performances are limited, especially on non-aligned and real
images. We propose a novel method (named MTV-TSA) to handle such problems.
Creating multi-type latent vectors (MTV) from latent space and two-scale
attentions (TSA) from images allows designing a set of encoders that can be
adaptable to a variety of pre-trained GANs. We generalize two sets of loss
functions to optimize the encoders. The designed encoders could make GANs
reconstruct higher fidelity images from most synthesized HQ images. In
addition, the proposed method can reconstruct real images well and process them
based on learned attribute directions. The designed encoders have unified
convolutional blocks and could match well in current GAN architectures (such as
PGGAN, StyleGANs, and BigGAN) by fine-tuning the corresponding normalization
layers and the last block. Such well-designed encoders can also be trained to
converge more quickly.","['cs.CV', 'eess.IV']",225,134
Feature Alignment and Restoration for Domain Generalization and Adaptation,"For domain generalization (DG) and unsupervised domain adaptation (UDA),
cross domain feature alignment has been widely explored to pull the feature
distributions of different domains in order to learn domain-invariant
representations. However, the feature alignment is in general task-ignorant and
could result in degradation of the discrimination power of the feature
representation and thus hinders the high performance. In this paper, we propose
a unified framework termed Feature Alignment and Restoration (FAR) to
simultaneously ensure high generalization and discrimination power of the
networks for effective DG and UDA. Specifically, we perform feature alignment
(FA) across domains by aligning the moments of the distributions of attentively
selected features to reduce their discrepancy. To ensure high discrimination,
we propose a Feature Restoration (FR) operation to distill task-relevant
features from the residual information and use them to compensate for the
aligned features. For better disentanglement, we enforce a dual ranking entropy
loss constraint in the FR step to encourage the separation of task-relevant and
task-irrelevant features. Extensive experiments on multiple classification
benchmarks demonstrate the high performance and strong generalization of our
FAR framework for both domain generalization and unsupervised domain
adaptation.",['cs.CV'],193,107
Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds,"We study the complexity of training neural network models with one hidden
nonlinear activation layer and an output weighted sum layer. We analyze
Gradient Descent applied to learning a bounded target function on $n$
real-valued inputs. We give an agnostic learning guarantee for GD: starting
from a randomly initialized network, it converges in mean squared loss to the
minimum error (in $2$-norm) of the best approximation of the target function
using a polynomial of degree at most $k$. Moreover, for any $k$, the size of
the network and number of iterations needed are both bounded by
$n^{O(k)}\log(1/\epsilon)$. In particular, this applies to training networks of
unbiased sigmoids and ReLUs. We also rigorously explain the empirical finding
that gradient descent discovers lower frequency Fourier components before
higher frequency components.
  We complement this result with nearly matching lower bounds in the
Statistical Query model. GD fits well in the SQ framework since each training
step is determined by an expectation over the input distribution. We show that
any SQ algorithm that achieves significant improvement over a constant function
with queries of tolerance some inverse polynomial in the input dimensionality
$n$ must use $n^{\Omega(k)}$ queries even when the target functions are
restricted to a set of $n^{O(k)}$ degree-$k$ polynomials, and the input
distribution is uniform over the unit sphere; for this class the
information-theoretic lower bound is only $\Theta(k \log n)$.
  Our approach for both parts is based on spherical harmonics. We view gradient
descent as an operator on the space of functions, and study its dynamics. An
essential tool is the Funk-Hecke theorem, which explains the eigenfunctions of
this operator in the case of the mean squared loss.","['cs.LG', 'stat.ML']",291,170
A genetic algorithm to discover flexible motifs with support,"Finding repeated patterns or motifs in a time series is an important
unsupervised task that has still a number of open issues, starting by the
definition of motif. In this paper, we revise the notion of motif support,
characterizing it as the number of patterns or repetitions that define a motif.
We then propose GENMOTIF, a genetic algorithm to discover motifs with support
which, at the same time, is flexible enough to accommodate other motif
specifications and task characteristics. GENMOTIF is an anytime algorithm that
easily adapts to many situations: searching in a range of segment lengths,
applying uniform scaling, dealing with multiple dimensions, using different
similarity and grouping criteria, etc. GENMOTIF is also parameter-friendly: it
has only two intuitive parameters which, if set within reasonable bounds, do
not substantially affect its performance. We demonstrate the value of our
approach in a number of synthetic and real-world settings, considering traffic
volume measurements, accelerometer signals, and telephone call records.","['cs.LG', 'cs.NE']",160,113
Random Pairwise Shapelets Forest,"Shapelet is a discriminative subsequence of time series. An advanced
shapelet-based method is to embed shapelet into accurate and fast random
forest. However, it shows several limitations. First, random shapelet forest
requires a large training cost for split threshold searching. Second, a single
shapelet provides limited information for only one branch of the decision tree,
resulting in insufficient accuracy and interpretability. Third, randomized
ensemble causes interpretability declining. For that, this paper presents
Random Pairwise Shapelets Forest (RPSF). RPSF combines a pair of shapelets from
different classes to construct random forest. It omits threshold searching to
be more efficient, includes more information for each node of the forest to be
more effective. Moreover, a discriminability metric, Decomposed Mean Decrease
Impurity (DMDI), is proposed to identify influential region for every class.
Extensive experiments show RPSF improves the accuracy and training speed of
shapelet-based forest. Case studies demonstrate the interpretability of our
method.","['cs.LG', 'stat.ML']",152,105
An End-to-End HW/SW Co-Design Methodology to Design Efficient Deep Neural Network Systems using Virtual Models,"End-to-end performance estimation and measurement of deep neural network
(DNN) systems become more important with increasing complexity of DNN systems
consisting of hardware and software components. The methodology proposed in
this paper aims at a reduced turn-around time for evaluating different design
choices of hardware and software components of DNN systems. This reduction is
achieved by moving the performance estimation from the implementation phase to
the concept phase by employing virtual hardware models instead of gathering
measurement results from physical prototypes. Deep learning compilers introduce
hardware-specific transformations and are, therefore, considered a part of the
design flow of virtual system models to extract end-to-end performance
estimations. To validate the run-time accuracy of the proposed methodology, a
system processing the DilatedVGG DNN is realized both as virtual system model
and as hardware implementation. The results show that up to 92 % accuracy can
be reached in predicting the processing time of the DNN inference.","['cs.LG', 'cs.DC', 'stat.ML']",159,93
Residual Conv-Deconv Grid Network for Semantic Segmentation,"This paper presents GridNet, a new Convolutional Neural Network (CNN)
architecture for semantic image segmentation (full scene labelling). Classical
neural networks are implemented as one stream from the input to the output with
subsampling operators applied in the stream in order to reduce the feature maps
size and to increase the receptive field for the final prediction. However, for
semantic image segmentation, where the task consists in providing a semantic
class to each pixel of an image, feature maps reduction is harmful because it
leads to a resolution loss in the output prediction. To tackle this problem,
our GridNet follows a grid pattern allowing multiple interconnected streams to
work at different resolutions. We show that our network generalizes many well
known networks such as conv-deconv, residual or U-Net networks. GridNet is
trained from scratch and achieves competitive results on the Cityscapes
dataset.",['cs.CV'],144,104
Extreme Consistency: Overcoming Annotation Scarcity and Domain Shifts,"Supervised learning has proved effective for medical image analysis. However,
it can utilize only the small labeled portion of data; it fails to leverage the
large amounts of unlabeled data that is often available in medical image
datasets. Supervised models are further handicapped by domain shifts, when the
labeled dataset, despite being large enough, fails to cover different protocols
or ethnicities. In this paper, we introduce \emph{extreme consistency}, which
overcomes the above limitations, by maximally leveraging unlabeled data from
the same or a different domain in a teacher-student semi-supervised paradigm.
Extreme consistency is the process of sending an extreme transformation of a
given image to the student network and then constraining its prediction to be
consistent with the teacher network's prediction for the untransformed image.
The extreme nature of our consistency loss distinguishes our method from
related works that yield suboptimal performance by exercising only mild
prediction consistency. Our method is 1) auto-didactic, as it requires no extra
expert annotations; 2) versatile, as it handles both domain shift and limited
annotation problems; 3) generic, as it is readily applicable to classification,
segmentation, and detection tasks; and 4) simple to implement, as it requires
no adversarial training. We evaluate our method for the tasks of lesion and
retinal vessel segmentation in skin and fundus images. Our experiments
demonstrate a significant performance gain over both modern supervised networks
and recent semi-supervised models. This performance is attributed to the strong
regularization enforced by extreme consistency, which enables the student
network to learn how to handle extreme variants of both labeled and unlabeled
images. This enhances the network's ability to tackle the inevitable same- and
cross-domain data variability during inference.","['cs.CV', 'cs.LG']",284,166
BoundarySqueeze: Image Segmentation as Boundary Squeezing,"We propose a novel method for fine-grained high-quality image segmentation of
both objects and scenes. Inspired by dilation and erosion from morphological
image processing techniques, we treat the pixel level segmentation problems as
squeezing object boundary. From this perspective, we propose \textbf{Boundary
Squeeze} module: a novel and efficient module that squeezes the object boundary
from both inner and outer directions which leads to precise mask
representation. To generate such squeezed representation, we propose a new
bidirectionally flow-based warping process and design specific loss signals to
supervise the learning process. Boundary Squeeze Module can be easily applied
to both instance and semantic segmentation tasks as a plug-and-play module by
building on top of existing models. We show that our simple yet effective
design can lead to high qualitative results on several different datasets and
we also provide several different metrics on boundary to prove the
effectiveness over previous work. Moreover, the proposed module is
light-weighted and thus has potential for practical usage. Our method yields
large gains on COCO, Cityscapes, for both instance and semantic segmentation
and outperforms previous state-of-the-art PointRend in both accuracy and speed
under the same setting. Code and model will be available.",['cs.CV'],205,134
Cosmetic-Aware Makeup Cleanser,"Face verification aims at determining whether a pair of face images belongs
to the same identity. Recent studies have revealed the negative impact of
facial makeup on the verification performance. With the rapid development of
deep generative models, this paper proposes a semanticaware makeup cleanser
(SAMC) to remove facial makeup under different poses and expressions and
achieve verification via generation. The intuition lies in the fact that makeup
is a combined effect of multiple cosmetics and tailored treatments should be
imposed on different cosmetic regions. To this end, we present both
unsupervised and supervised semantic-aware learning strategies in SAMC. At
image level, an unsupervised attention module is jointly learned with the
generator to locate cosmetic regions and estimate the degree. At feature level,
we resort to the effort of face parsing merely in training phase and design a
localized texture loss to serve complements and pursue superior synthetic
quality. The experimental results on four makeuprelated datasets verify that
SAMC not only produces appealing de-makeup outputs at a resolution of 256*256,
but also facilitates makeup-invariant face verification through image
generation.",['cs.CV'],183,125
Prior-Knowledge and Attention-based Meta-Learning for Few-Shot Learning,"Recently, meta-learning has been shown as a promising way to solve few-shot
learning. In this paper, inspired by the human cognition process which utilizes
both prior-knowledge and vision attention in learning new knowledge, we present
a novel paradigm of meta-learning approach with three developments to introduce
attention mechanism and prior-knowledge for meta-learning. In our approach,
prior-knowledge is responsible for helping meta-learner expressing the input
data into high-level representation space, and attention mechanism enables
meta-learner focusing on key features of the data in the representation space.
Compared with existing meta-learning approaches that pay little attention to
prior-knowledge and vision attention, our approach alleviates the
meta-learner's few-shot cognition burden. Furthermore, a Task-Over-Fitting
(TOF) problem, which indicates that the meta-learner has poor generalization on
different K-shot learning tasks, is discovered and we propose a Cross-Entropy
across Tasks (CET) metric to model and solve the TOF problem. Extensive
experiments demonstrate that we improve the meta-learner with state-of-the-art
performance on several few-shot learning benchmarks, and at the same time the
TOF problem can also be released greatly.","['cs.CV', 'cs.LG']",197,110
Generalised Interpretable Shapelets for Irregular Time Series,"The shapelet transform is a form of feature extraction for time series, in
which a time series is described by its similarity to each of a collection of
`shapelets'. However it has previously suffered from a number of limitations,
such as being limited to regularly-spaced fully-observed time series, and
having to choose between efficient training and interpretability. Here, we
extend the method to continuous time, and in doing so handle the general case
of irregularly-sampled partially-observed multivariate time series.
Furthermore, we show that a simple regularisation penalty may be used to train
efficiently without sacrificing interpretability. The continuous-time
formulation additionally allows for learning the length of each shapelet
(previously a discrete object) in a differentiable manner. Finally, we
demonstrate that the measure of similarity between time series may be
generalised to a learnt pseudometric. We validate our method by demonstrating
its performance and interpretability on several datasets; for example we
discover (purely from data) that the digits 5 and 6 may be distinguished by the
chirality of their bottom loop, and that a kind of spectral gap exists in
spoken audio classification.","['cs.LG', 'stat.ML']",187,115
Geometric Image Synthesis,"The task of generating natural images from 3D scenes has been a long standing
goal in computer graphics. On the other hand, recent developments in deep
neural networks allow for trainable models that can produce natural-looking
images with little or no knowledge about the scene structure. While the
generated images often consist of realistic looking local patterns, the overall
structure of the generated images is often inconsistent. In this work we
propose a trainable, geometry-aware image generation method that leverages
various types of scene information, including geometry and segmentation, to
create realistic looking natural images that match the desired scene structure.
Our geometrically-consistent image synthesis method is a deep neural network,
called Geometry to Image Synthesis (GIS) framework, which retains the
advantages of a trainable method, e.g., differentiability and adaptiveness,
but, at the same time, makes a step towards the generalizability, control and
quality output of modern graphics rendering engines. We utilize the GIS
framework to insert vehicles in outdoor driving scenes, as well as to generate
novel views of objects from the Linemod dataset. We qualitatively show that our
network is able to generalize beyond the training set to novel scene
geometries, object shapes and segmentations. Furthermore, we quantitatively
show that the GIS framework can be used to synthesize large amounts of training
data which proves beneficial for training instance segmentation models.",['cs.CV'],227,143
Realistic Image Synthesis with Configurable 3D Scene Layouts,"Recent conditional image synthesis approaches provide high-quality
synthesized images. However, it is still challenging to accurately adjust image
contents such as the positions and orientations of objects, and synthesized
images often have geometrically invalid contents. To provide users with rich
controllability on synthesized images in the aspect of 3D geometry, we propose
a novel approach to realistic-looking image synthesis based on a configurable
3D scene layout. Our approach takes a 3D scene with semantic class labels as
input and trains a 3D scene painting network that synthesizes color values for
the input 3D scene. With the trained painting network, realistic-looking images
for the input 3D scene can be rendered and manipulated. To train the painting
network without 3D color supervision, we exploit an off-the-shelf 2D semantic
image synthesis method. In experiments, we show that our approach produces
images with geometrically correct structures and supports geometric
manipulation such as the change of the viewpoint and object poses as well as
manipulation of the painting style.",['cs.CV'],169,97
A Compression-Compilation Framework for On-mobile Real-time BERT Applications,"Transformer-based deep learning models have increasingly demonstrated high
accuracy on many natural language processing (NLP) tasks. In this paper, we
propose a compression-compilation co-design framework that can guarantee the
identified model to meet both resource and real-time specifications of mobile
devices. Our framework applies a compiler-aware neural architecture
optimization method (CANAO), which can generate the optimal compressed model
that balances both accuracy and latency. We are able to achieve up to 7.8x
speedup compared with TensorFlow-Lite with only minor accuracy loss. We present
two types of BERT applications on mobile devices: Question Answering (QA) and
Text Generation. Both can be executed in real-time with latency as low as 45ms.
Videos for demonstrating the framework can be found on
https://www.youtube.com/watch?v=_WIRvK_2PZI","['cs.LG', 'cs.AI']",133,103
Unsupervised Learning of Shape and Pose with Differentiable Point Clouds,"We address the problem of learning accurate 3D shape and camera pose from a
collection of unlabeled category-specific images. We train a convolutional
network to predict both the shape and the pose from a single image by
minimizing the reprojection error: given several views of an object, the
projections of the predicted shapes to the predicted camera poses should match
the provided views. To deal with pose ambiguity, we introduce an ensemble of
pose predictors which we then distill to a single ""student"" model. To allow for
efficient learning of high-fidelity shapes, we represent the shapes by point
clouds and devise a formulation allowing for differentiable projection of
these. Our experiments show that the distilled ensemble of pose predictors
learns to estimate the pose accurately, while the point cloud representation
allows to predict detailed shape models. The supplementary video can be found
at https://www.youtube.com/watch?v=LuIGovKeo60","['cs.CV', 'cs.LG']",152,98
Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming,"Lately, post-training quantization methods have gained considerable
attention, as they are simple to use, and require only a small unlabeled
calibration set. This small dataset cannot be used to fine-tune the model
without significant over-fitting. Instead, these methods only use the
calibration set to set the activations' dynamic ranges. However, such methods
always resulted in significant accuracy degradation, when used below 8-bits
(except on small datasets). Here we aim to break the 8-bit barrier. To this
end, we minimize the quantization errors of each layer separately by optimizing
its parameters over the calibration set. We empirically demonstrate that this
approach is: (1) much less susceptible to over-fitting than the standard
fine-tuning approaches, and can be used even on a very small calibration set;
and (2) more powerful than previous methods, which only set the activations'
dynamic ranges. Furthermore, we demonstrate how to optimally allocate the
bit-widths for each layer, while constraining accuracy degradation or model
compression by proposing a novel integer programming formulation. Finally, we
suggest model global statistics tuning, to correct biases introduced during
quantization. Together, these methods yield state-of-the-art results for both
vision and text models. For instance, on ResNet50, we obtain less than 1\%
accuracy degradation --- with 4-bit weights and activations in all layers, but
the smallest two. We open-sourced our code.","['cs.LG', 'stat.ML']",228,144
Rethinking Cross-modal Interaction from a Top-down Perspective for Referring Video Object Segmentation,"Referring video object segmentation (RVOS) aims to segment video objects with
the guidance of natural language reference. Previous methods typically tackle
RVOS through directly grounding linguistic reference over the image lattice.
Such bottom-up strategy fails to explore object-level cues, easily leading to
inferior results. In this work, we instead put forward a two-stage, top-down
RVOS solution. First, an exhaustive set of object tracklets is constructed by
propagating object masks detected from several sampled frames to the entire
video. Second, a Transformer-based tracklet-language grounding module is
proposed, which models instance-level visual relations and cross-modal
interactions simultaneously and efficiently. Our model ranks first place on
CVPR2021 Referring Youtube-VOS challenge.",['cs.CV'],116,95
Multi-Agent Deep Reinforcement Learning for Large-scale Traffic Signal Control,"Reinforcement learning (RL) is a promising data-driven approach for adaptive
traffic signal control (ATSC) in complex urban traffic networks, and deep
neural networks further enhance its learning power. However, centralized RL is
infeasible for large-scale ATSC due to the extremely high dimension of the
joint action space. Multi-agent RL (MARL) overcomes the scalability issue by
distributing the global control to each local RL agent, but it introduces new
challenges: now the environment becomes partially observable from the viewpoint
of each local agent due to limited communication among agents. Most existing
studies in MARL focus on designing efficient communication and coordination
among traditional Q-learning agents. This paper presents, for the first time, a
fully scalable and decentralized MARL algorithm for the state-of-the-art deep
RL agent: advantage actor critic (A2C), within the context of ATSC. In
particular, two methods are proposed to stabilize the learning procedure, by
improving the observability and reducing the learning difficulty of each local
agent. The proposed multi-agent A2C is compared against independent A2C and
independent Q-learning algorithms, in both a large synthetic traffic grid and a
large real-world traffic network of Monaco city, under simulated peak-hour
traffic dynamics. Results demonstrate its optimality, robustness, and sample
efficiency over other state-of-the-art decentralized MARL algorithms.","['cs.LG', 'stat.ML']",219,134
Optimal and Efficient Algorithms for General Mixable Losses against Switching Oracles,"We investigate the problem of online learning, which has gained significant
attention in recent years due to its applicability in a wide range of fields
from machine learning to game theory. Specifically, we study the online
optimization of mixable loss functions in a dynamic environment. We introduce
online mixture schemes that asymptotically achieves the performance of the best
dynamic estimation sequence of the switching oracle with optimal regret
redundancies. The best dynamic estimation sequence that we compete against is
selected in hindsight with full observation of the loss functions and is
allowed to select different optimal estimations in different time intervals
(segments). We propose two mixtures in our work. Firstly, we propose a
tractable polynomial time complexity algorithm that can achieve the optimal
redundancy of the intractable brute force approach. Secondly, we propose an
efficient logarithmic time complexity algorithm that can achieve the optimal
redundancy up to a constant multiplicity gap. Our results are guaranteed to
hold in a strong deterministic sense in an individual sequence manner.","['cs.LG', 'stat.ML']",167,103
Scene Flow from Point Clouds with or without Learning,"Scene flow is the three-dimensional (3D) motion field of a scene. It provides
information about the spatial arrangement and rate of change of objects in
dynamic environments. Current learning-based approaches seek to estimate the
scene flow directly from point clouds and have achieved state-of-the-art
performance. However, supervised learning methods are inherently domain
specific and require a large amount of labeled data. Annotation of scene flow
on real-world point clouds is expensive and challenging, and the lack of such
datasets has recently sparked interest in self-supervised learning methods. How
to accurately and robustly learn scene flow representations without labeled
real-world data is still an open problem. Here we present a simple and
interpretable objective function to recover the scene flow from point clouds.
We use the graph Laplacian of a point cloud to regularize the scene flow to be
""as-rigid-as-possible"". Our proposed objective function can be used with or
without learning---as a self-supervisory signal to learn scene flow
representations, or as a non-learning-based method in which the scene flow is
optimized during runtime. Our approach outperforms related works in many
datasets. We also show the immediate applications of our proposed method for
two applications: motion segmentation and point cloud densification.","['cs.CV', 'cs.LG', 'cs.RO']",214,123
SISE-PC: Semi-supervised Image Subsampling for Explainable Pathology,"Although automated pathology classification using deep learning (DL) has
proved to be predictively efficient, DL methods are found to be data and
compute cost intensive. In this work, we aim to reduce DL training costs by
pre-training a Resnet feature extractor using SimCLR contrastive loss for
latent encoding of OCT images. We propose a novel active learning framework
that identifies a minimal sub-sampled dataset containing the most uncertain OCT
image samples using label propagation on the SimCLR latent encodings. The
pre-trained Resnet model is then fine-tuned with the labelled minimal
sub-sampled data and the underlying pathological sites are visually explained.
Our framework identifies upto 2% of OCT images to be most uncertain that need
prioritized specialist attention and that can fine-tune a Resnet model to
achieve upto 97% classification accuracy. The proposed method can be extended
to other medical images to minimize prediction costs.","['cs.CV', 'cs.AI', 'cs.LG']",150,99
On the Regularity of Attention,"Attention is a powerful component of modern neural networks across a wide
variety of domains. In this paper, we seek to quantify the regularity (i.e. the
amount of smoothness) of the attention operation. To accomplish this goal, we
propose a new mathematical framework that uses measure theory and integral
operators to model attention. We show that this framework is consistent with
the usual definition, and that it captures the essential properties of
attention. Then we use this framework to prove that, on compact domains, the
attention operation is Lipschitz continuous and provide an estimate of its
Lipschitz constant. Additionally, by focusing on a specific type of attention,
we extend these Lipschitz continuity results to non-compact domains. We also
discuss the effects regularity can have on NLP models, and applications to
invertible and infinitely-deep networks.","['stat.ML', 'cs.LG']",137,86
Apply Artificial Neural Network to Solving Manpower Scheduling Problem,"The manpower scheduling problem is a kind of critical combinational
optimization problem. Researching solutions to scheduling problems can improve
the efficiency of companies, hospitals, and other work units. This paper
proposes a new model combined with deep learning to solve the multi-shift
manpower scheduling problem based on the existing research. This model first
solves the objective function's optimized value according to the current
constraints to find the plan of employee arrangement initially. It will then
use the scheduling table generation algorithm to obtain the scheduling result
in a short time. Moreover, the most prominent feature we propose is that we
will use the neural network training method based on the time series to solve
long-term and long-period scheduling tasks and obtain manpower arrangement. The
selection criteria of the neural network and the training process are also
described in this paper. We demonstrate that our model can make a precise
forecast based on the improvement of neural networks. This paper also discusses
the challenges in the neural network training process and obtains enlightening
results after getting the arrangement plan. Our research shows that neural
networks and deep learning strategies have the potential to solve similar
problems effectively.",['cs.LG'],200,113
rlpyt: A Research Code Base for Deep Reinforcement Learning in PyTorch,"Since the recent advent of deep reinforcement learning for game play and
simulated robotic control, a multitude of new algorithms have flourished. Most
are model-free algorithms which can be categorized into three families: deep
Q-learning, policy gradients, and Q-value policy gradients. These have
developed along separate lines of research, such that few, if any, code bases
incorporate all three kinds. Yet these algorithms share a great depth of common
deep reinforcement learning machinery. We are pleased to share rlpyt, which
implements all three algorithm families on top of a shared, optimized
infrastructure, in a single repository. It contains modular implementations of
many common deep RL algorithms in Python using PyTorch, a leading deep learning
library. rlpyt is designed as a high-throughput code base for small- to
medium-scale research in deep RL. This white paper summarizes its features,
algorithms implemented, and relation to prior work, and concludes with detailed
implementation and usage notes. rlpyt is available at
https://github.com/astooke/rlpyt.","['cs.LG', 'cs.AI']",166,115
Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association,"Person re-identification is an important task that requires learning
discriminative visual features for distinguishing different person identities.
Diverse auxiliary information has been utilized to improve the visual feature
learning. In this paper, we propose to exploit natural language description as
additional training supervisions for effective visual features. Compared with
other auxiliary information, language can describe a specific person from more
compact and semantic visual aspects, thus is complementary to the pixel-level
image data. Our method not only learns better global visual feature with the
supervision of the overall description but also enforces semantic consistencies
between local visual and linguistic features, which is achieved by building
global and local image-language associations. The global image-language
association is established according to the identity labels, while the local
association is based upon the implicit correspondences between image regions
and noun phrases. Extensive experiments demonstrate the effectiveness of
employing language as training supervisions with the two association schemes.
Our method achieves state-of-the-art performance without utilizing any
auxiliary information during testing and shows better performance than other
joint embedding methods for the image-language association.",['cs.CV'],186,118
MC2G: An Efficient Algorithm for Matrix Completion with Social and Item Similarity Graphs,"In this paper, we design and analyze MC2G (Matrix Completion with 2 Graphs),
an algorithm that performs matrix completion in the presence of social and item
similarity graphs. MC2G runs in quasilinear time and is parameter free. It is
based on spectral clustering and local refinement steps. The expected number of
sampled entries required for MC2G to succeed (i.e., recover the clusters in the
graphs and complete the matrix) matches an information-theoretic lower bound up
to a constant factor for a wide range of parameters. We show via extensive
experiments on both synthetic and real datasets that MC2G outperforms other
state-of-the-art matrix completion algorithms that leverage graph side
information.","['cs.LG', 'cs.IR', 'cs.IT', 'eess.SP', 'math.IT']",114,84
Bayesian Belief Updating of Spatiotemporal Seizure Dynamics,"Epileptic seizure activity shows complicated dynamics in both space and time.
To understand the evolution and propagation of seizures spatially extended sets
of data need to be analysed. We have previously described an efficient
filtering scheme using variational Laplace that can be used in the Dynamic
Causal Modelling (DCM) framework [Friston, 2003] to estimate the temporal
dynamics of seizures recorded using either invasive or non-invasive electrical
recordings (EEG/ECoG). Spatiotemporal dynamics are modelled using a partial
differential equation -- in contrast to the ordinary differential equation used
in our previous work on temporal estimation of seizure dynamics [Cooray, 2016].
We provide the requisite theoretical background for the method and test the
ensuing scheme on simulated seizure activity data and empirical invasive ECoG
data. The method provides a framework to assimilate the spatial and temporal
dynamics of seizure activity, an aspect of great physiological and clinical
importance.","['stat.ML', 'cs.NE', 'stat.ME']",146,93
Accounting for Unobserved Confounding in Domain Generalization,"The ability to generalize from observed to new related environments is
central to any form of reliable machine learning, yet most methods fail when
moving beyond i.i.d data. This work argues that in some cases the reason lies
in a misapreciation of the causal structure in data; and in particular due to
the influence of unobserved confounders which void many of the invariances and
principles of minimum error between environments presently used for the problem
of domain generalization. This observation leads us to study generalization in
the context of a broader class of interventions in an underlying causal model
(including changes in observed, unobserved and target variable distributions)
and to connect this causal intuition with an explicit distributionally robust
optimization problem. From this analysis derives a new proposal for model
learning with explicit generalization guarantees that is based on the partial
equality of error derivatives with respect to model parameters. We demonstrate
the empirical performance of our approach on healthcare data from different
modalities, including image, speech and tabular data.","['stat.ML', 'cs.LG']",172,109
The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization,"Modern deep learning models employ considerably more parameters than required
to fit the training data. Whereas conventional statistical wisdom suggests such
models should drastically overfit, in practice these models generalize
remarkably well. An emerging paradigm for describing this unexpected behavior
is in terms of a \emph{double descent} curve, in which increasing a model's
capacity causes its test error to first decrease, then increase to a maximum
near the interpolation threshold, and then decrease again in the
overparameterized regime. Recent efforts to explain this phenomenon
theoretically have focused on simple settings, such as linear regression or
kernel regression with unstructured random features, which we argue are too
coarse to reveal important nuances of actual neural networks. We provide a
precise high-dimensional asymptotic analysis of generalization under kernel
regression with the Neural Tangent Kernel, which characterizes the behavior of
wide neural networks optimized with gradient descent. Our results reveal that
the test error has non-monotonic behavior deep in the overparameterized regime
and can even exhibit additional peaks and descents when the number of
parameters scales quadratically with the dataset size.","['stat.ML', 'cs.LG']",182,131
privGAN: Protecting GANs from membership inference attacks at low cost,"Generative Adversarial Networks (GANs) have made releasing of synthetic
images a viable approach to share data without releasing the original dataset.
It has been shown that such synthetic data can be used for a variety of
downstream tasks such as training classifiers that would otherwise require the
original dataset to be shared. However, recent work has shown that the GAN
models and their synthetically generated data can be used to infer the training
set membership by an adversary who has access to the entire dataset and some
auxiliary information. Current approaches to mitigate this problem (such as
DPGAN) lead to dramatically poorer generated sample quality than the original
non--private GANs. Here we develop a new GAN architecture (privGAN), where the
generator is trained not only to cheat the discriminator but also to defend
membership inference attacks. The new mechanism provides protection against
this mode of attack while leading to negligible loss in downstream
performances. In addition, our algorithm has been shown to explicitly prevent
overfitting to the training set, which explains why our protection is so
effective. The main contributions of this paper are: i) we propose a novel GAN
architecture that can generate synthetic data in a privacy preserving manner
without additional hyperparameter tuning and architecture selection, ii) we
provide a theoretical understanding of the optimal solution of the privGAN loss
function, iii) we demonstrate the effectiveness of our model against several
white and black--box attacks on several benchmark datasets, iv) we demonstrate
on three common benchmark datasets that synthetic images generated by privGAN
lead to negligible loss in downstream performance when compared against
non--private GANs.","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",270,158
Improving Action Quality Assessment using ResNets and Weighted Aggregation,"Action quality assessment (AQA) aims at automatically judging human action
based on a video of the said action and assigning a performance score to it.
The majority of works in the existing literature on AQA transform RGB videos to
higher-level representations using C3D networks. These higher-level
representations are used to perform action quality assessment. Due to the
relatively shallow nature of C3D, the quality of extracted features is lower
than what could be extracted using a deeper convolutional neural network. In
this paper, we experiment with deeper convolutional neural networks with
residual connections for learning representations for action quality
assessment. We assess the effects of the depth and the input clip size of the
convolutional neural network on the quality of action score predictions. We
also look at the effect of using (2+1)D convolutions instead of 3D convolutions
for feature extraction. We find that the current clip level feature
representation aggregation technique of averaging is insufficient to capture
the relative importance of features. To overcome this, we propose a
learning-based weighted-averaging technique that can perform better. We achieve
a new state-of-the-art Spearman's rank correlation of 0.9315 (an increase of
0.45%) on the MTL-AQA dataset using a 34 layer (2+1)D convolutional neural
network with the capability of processing 32 frame clips, using our proposed
aggregation technique.",['cs.CV'],230,128
High-Resolution Mammogram Synthesis using Progressive Generative Adversarial Networks,"The ability to generate synthetic medical images is useful for data
augmentation, domain transfer, and out-of-distribution detection. However,
generating realistic, high-resolution medical images is challenging,
particularly for Full Field Digital Mammograms (FFDM), due to the textural
heterogeneity, fine structural details and specific tissue properties. In this
paper, we explore the use of progressively trained generative adversarial
networks (GANs) to synthesize mammograms, overcoming the underlying
instabilities when training such adversarial models. This work is the first to
show that generation of realistic synthetic medical images is feasible at up to
1280x1024 pixels, the highest resolution achieved for medical image synthesis,
enabling visualizations within standard mammographic hanging protocols. We hope
this work can serve as a useful guide and facilitate further research on GANs
in the medical imaging domain.",['cs.CV'],130,97
A Robust Color Edge Detection Algorithm Based on Quaternion Hardy Filter,"This paper presents a robust filter called quaternion Hardy filter (QHF) for
color image edge detection. The QHF can be capable of color edge feature
enhancement and noise resistance. It is flexible to use QHF by selecting
suitable parameters to handle different levels of noise. In particular, the
quaternion analytic signal, which is an effective tool in color image
processing, can also be produced by quaternion Hardy filtering with specific
parameters. Based on the QHF and the improved Di Zenzo gradient operator, a
novel color edge detection algorithm is proposed. Importantly, it can be
efficiently implemented by using the fast discrete quaternion Fourier transform
technique. From the experimental results, we conclude that the minimum PSNR
improvement rate is 2.3% and minimum SSIM improvement rate is 30.2% on the
Dataset 3. The experiments demonstrate that the proposed algorithm outperforms
several widely used algorithms.","['cs.CV', 'eess.IV']",144,95
On the Adversarial Robustness of Visual Transformers,"Following the success in advancing natural language processing and
understanding, transformers are expected to bring revolutionary changes to
computer vision. This work provides the first and comprehensive study on the
robustness of vision transformers (ViTs) against adversarial perturbations.
Tested on various white-box and transfer attack settings, we find that ViTs
possess better adversarial robustness when compared with convolutional neural
networks (CNNs). We summarize the following main observations contributing to
the improved robustness of ViTs:
  1) Features learned by ViTs contain less low-level information and are more
generalizable, which contributes to superior robustness against adversarial
perturbations.
  2) Introducing convolutional or tokens-to-token blocks for learning low-level
features in ViTs can improve classification accuracy but at the cost of
adversarial robustness.
  3) Increasing the proportion of transformers in the model structure (when the
model consists of both transformer and CNN blocks) leads to better robustness.
But for a pure transformer model, simply increasing the size or adding layers
cannot guarantee a similar effect.
  4) Pre-training on larger datasets does not significantly improve adversarial
robustness though it is critical for training ViTs.
  5) Adversarial training is also applicable to ViT for training robust models.
  Furthermore, feature visualization and frequency analysis are conducted for
explanation. The results show that ViTs are less sensitive to high-frequency
perturbations than CNNs and there is a high correlation between how well the
model learns low-level features and its robustness against different
frequency-based perturbations.","['cs.CV', 'cs.AI', 'cs.LG']",243,151
Combining Parametric and Nonparametric Models for Off-Policy Evaluation,"We consider a model-based approach to perform batch off-policy evaluation in
reinforcement learning. Our method takes a mixture-of-experts approach to
combine parametric and non-parametric models of the environment such that the
final value estimate has the least expected error. We do so by first estimating
the local accuracy of each model and then using a planner to select which model
to use at every time step as to minimize the return error estimate along entire
trajectories. Across a variety of domains, our mixture-based approach
outperforms the individual models alone as well as state-of-the-art importance
sampling-based estimators.","['cs.LG', 'stat.ML']",106,74
Representation Learning with Weighted Inner Product for Universal Approximation of General Similarities,"We propose $\textit{weighted inner product similarity}$ (WIPS) for neural
network-based graph embedding. In addition to the parameters of neural
networks, we optimize the weights of the inner product by allowing positive and
negative values. Despite its simplicity, WIPS can approximate arbitrary general
similarities including positive definite, conditionally positive definite, and
indefinite kernels. WIPS is free from similarity model selection, since it can
learn any similarity models such as cosine similarity, negative Poincar\'e
distance and negative Wasserstein distance. Our experiments show that the
proposed method can learn high-quality distributed representations of nodes
from real datasets, leading to an accurate approximation of similarities as
well as high performance in inductive tasks.","['cs.LG', 'stat.ML']",113,82
Learning Belief Representations for Imitation Learning in POMDPs,"We consider the problem of imitation learning from expert demonstrations in
partially observable Markov decision processes (POMDPs). Belief
representations, which characterize the distribution over the latent states in
a POMDP, have been modeled using recurrent neural networks and probabilistic
latent variable models, and shown to be effective for reinforcement learning in
POMDPs. In this work, we investigate the belief representation learning problem
for generative adversarial imitation learning in POMDPs. Instead of training
the belief module and the policy separately as suggested in prior work, we
learn the belief module jointly with the policy, using a task-aware imitation
loss to ensure that the representation is more aligned with the policy's
objective. To improve robustness of representation, we introduce several
informative belief regularization techniques, including multi-step prediction
of dynamics and action-sequences. Evaluated on various partially observable
continuous-control locomotion tasks, our belief-module imitation learning
approach (BMIL) substantially outperforms several baselines, including the
original GAIL algorithm and the task-agnostic belief learning algorithm.
Extensive ablation analysis indicates the effectiveness of task-aware belief
learning and belief regularization.","['cs.LG', 'stat.ML']",179,111
ABD-Net: Attention Based Decomposition Network for 3D Point Cloud Decomposition,"In this paper, we propose Attention Based Decomposition Network (ABD-Net),
for point cloud decomposition into basic geometric shapes namely, plane,
sphere, cone and cylinder. We show improved performance of 3D object
classification using attention features based on primitive shapes in point
clouds. Point clouds, being the simple and compact representation of 3D objects
have gained increasing popularity. They demand robust methods for feature
extraction due to unorderness in point sets. In ABD-Net the proposed Local
Proximity Encapsulator captures the local geometric variations along with
spatial encoding around each point from the input point sets. The encapsulated
local features are further passed to proposed Attention Feature Encoder to
learn basic shapes in point cloud. Attention Feature Encoder models geometric
relationship between the neighborhoods of all the points resulting in capturing
global point cloud information. We demonstrate the results of our proposed
ABD-Net on ANSI mechanical component and ModelNet40 datasets. We also
demonstrate the effectiveness of ABD-Net over the acquired attention features
by improving the performance of 3D object classification on ModelNet40
benchmark dataset and compare them with state-of-the-art techniques.","['cs.CV', 'cs.AI']",185,116
Block Pruning For Faster Transformers,"Pre-training has improved model accuracy for both classification and
generation tasks at the cost of introducing much larger and slower models.
Pruning methods have proven to be an effective way of reducing model size,
whereas distillation methods are proven for speeding up inference. We introduce
a block pruning approach targeting both small and fast models. Our approach
extends structured methods by considering blocks of any size and integrates
this structure into the movement pruning paradigm for fine-tuning. We find that
this approach learns to prune out full components of the underlying model, such
as attention heads. Experiments consider classification and generation tasks,
yielding among other results a pruned model that is a 2.4x faster, 74% smaller
BERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models
in speed and pruned models in size.","['cs.LG', 'cs.CL', 'I.2.6; I.2.7']",141,99
"Deep Model-Based Reinforcement Learning for High-Dimensional Problems, a Survey","Deep reinforcement learning has shown remarkable success in the past few
years. Highly complex sequential decision making problems have been solved in
tasks such as game playing and robotics. Unfortunately, the sample complexity
of most deep reinforcement learning methods is high, precluding their use in
some important applications. Model-based reinforcement learning creates an
explicit model of the environment dynamics to reduce the need for environment
samples. Current deep learning methods use high-capacity networks to solve
high-dimensional problems. Unfortunately, high-capacity models typically
require many samples, negating the potential benefit of lower sample complexity
in model-based methods. A challenge for deep model-based methods is therefore
to achieve high predictive power while maintaining low sample complexity. In
recent years, many model-based methods have been introduced to address this
challenge. In this paper, we survey the contemporary model-based landscape.
First we discuss definitions and relations to other fields. We propose a
taxonomy based on three approaches: using explicit planning on given
transitions, using explicit planning on learned transitions, and end-to-end
learning of both planning and transitions. We use these approaches to organize
a comprehensive overview of important recent developments such as latent
models. We describe methods and benchmarks, and we suggest directions for
future work for each of the approaches. Among promising research directions are
curriculum learning, uncertainty modeling, and use of latent models for
transfer learning.","['cs.LG', 'cs.AI']",233,129
Stacked Wasserstein Autoencoder,"Approximating distributions over complicated manifolds, such as natural
images, are conceptually attractive. The deep latent variable model, trained
using variational autoencoders and generative adversarial networks, is now a
key technique for representation learning. However, it is difficult to unify
these two models for exact latent-variable inference and parallelize both
reconstruction and sampling, partly due to the regularization under the latent
variables, to match a simple explicit prior distribution. These approaches are
prone to be oversimplified, and can only characterize a few modes of the true
distribution. Based on the recently proposed Wasserstein autoencoder (WAE) with
a new regularization as an optimal transport. The paper proposes a stacked
Wasserstein autoencoder (SWAE) to learn a deep latent variable model. SWAE is a
hierarchical model, which relaxes the optimal transport constraints at two
stages. At the first stage, the SWAE flexibly learns a representation
distribution, i.e., the encoded prior; and at the second stage, the encoded
representation distribution is approximated with a latent variable model under
the regularization encouraging the latent distribution to match the explicit
prior. This model allows us to generate natural textual outputs as well as
perform manipulations in the latent space to induce changes in the output
space. Both quantitative and qualitative results demonstrate the superior
performance of SWAE compared with the state-of-the-art approaches in terms of
faithful reconstruction and generation quality.","['cs.CV', 'cs.LG']",228,132
Small Data Challenges in Big Data Era: A Survey of Recent Progress on Unsupervised and Semi-Supervised Methods,"Representation learning with small labeled data have emerged in many
problems, since the success of deep neural networks often relies on the
availability of a huge amount of labeled data that is expensive to collect. To
address it, many efforts have been made on training sophisticated models with
few labeled data in an unsupervised and semi-supervised fashion. In this paper,
we will review the recent progresses on these two major categories of methods.
A wide spectrum of models will be categorized in a big picture, where we will
show how they interplay with each other to motivate explorations of new ideas.
We will review the principles of learning the transformation equivariant,
disentangled, self-supervised and semi-supervised representations, all of which
underpin the foundation of recent progresses. Many implementations of
unsupervised and semi-supervised generative models have been developed on the
basis of these criteria, greatly expanding the territory of existing
autoencoders, generative adversarial nets (GANs) and other deep networks by
exploring the distribution of unlabeled data for more powerful representations.
We will discuss emerging topics by revealing the intrinsic connections between
unsupervised and semi-supervised learning, and propose in future directions to
bridge the algorithmic and theoretical gap between transformation equivariance
for unsupervised learning and supervised invariance for supervised learning,
and unify unsupervised pretraining and supervised finetuning. We will also
provide a broader outlook of future directions to unify transformation and
instance equivariances for representation learning, connect unsupervised and
semi-supervised augmentations, and explore the role of the self-supervised
regularization for many learning problems.",['cs.CV'],257,140
UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer,"Most recent semantic segmentation methods adopt a U-Net framework with an
encoder-decoder architecture. It is still challenging for U-Net with a simple
skip connection scheme to model the global multi-scale context: 1) Not each
skip connection setting is effective due to the issue of incompatible feature
sets of encoder and decoder stage, even some skip connection negatively
influence the segmentation performance; 2) The original U-Net is worse than the
one without any skip connection on some datasets. Based on our findings, we
propose a new segmentation framework, named UCTransNet (with a proposed CTrans
module in U-Net), from the channel perspective with attention mechanism.
Specifically, the CTrans module is an alternate of the U-Net skip connections,
which consists of a sub-module to conduct the multi-scale Channel Cross fusion
with Transformer (named CCT) and a sub-module Channel-wise Cross-Attention
(named CCA) to guide the fused multi-scale channel-wise information to
effectively connect to the decoder features for eliminating the ambiguity.
Hence, the proposed connection consisting of the CCT and CCA is able to replace
the original skip connection to solve the semantic gaps for an accurate
automatic medical image segmentation. The experimental results suggest that our
UCTransNet produces more precise segmentation performance and achieves
consistent improvements over the state-of-the-art for semantic segmentation
across different datasets and conventional architectures involving transformer
or U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet.","['cs.CV', 'cs.LG', 'eess.IV']",243,135
Indoor 3D Video Monitoring Using Multiple Kinect Depth-Cameras,"This article describes the design and development of a system for remote
indoor 3D monitoring using an undetermined number of Microsoft(R) Kinect
sensors. In the proposed client-server system, the Kinect cameras can be
connected to different computers, addressing this way the hardware limitation
of one sensor per USB controller. The reason behind this limitation is the high
bandwidth needed by the sensor, which becomes also an issue for the distributed
system TCP/IP communications. Since traffic volume is too high, 3D data has to
be compressed before it can be sent over the network. The solution consists in
selfcoding the Kinect data into RGB images and then using a standard multimedia
codec to compress color maps. Information from different sources is collected
into a central client computer, where point clouds are transformed to
reconstruct the scene in 3D. An algorithm is proposed to merge the skeletons
detected locally by each Kinect conveniently, so that monitoring of people is
robust to self and inter-user occlusions. Final skeletons are labeled and
trajectories of every joint can be saved for event reconstruction or further
analysis.",['cs.CV'],185,126
Disentangled Attention as Intrinsic Regularization for Bimanual Multi-Object Manipulation,"We address the problem of solving complex bimanual robot manipulation tasks
on multiple objects with sparse rewards. Such complex tasks can be decomposed
into sub-tasks that are accomplishable by different robots concurrently or
sequentially for better efficiency. While previous reinforcement learning
approaches primarily focus on modeling the compositionality of sub-tasks, two
fundamental issues are largely ignored particularly when learning cooperative
strategies for two robots: (i) domination, i.e., one robot may try to solve a
task by itself and leaves the other idle; (ii) conflict, i.e., one robot can
easily interrupt another's workspace when executing different sub-tasks
simultaneously. To tackle these two issues, we propose a novel technique called
disentangled attention, which provides an intrinsic regularization for two
robots to focus on separate sub-tasks and objects. We evaluate our method on
four bimanual manipulation tasks. Experimental results show that our proposed
intrinsic regularization successfully avoids domination and reduces conflicts
for the policies, which leads to significantly more effective cooperative
strategies than all the baselines. Our project page with videos is at
https://mehooz.github.io/bimanual-attention.","['cs.LG', 'cs.RO']",183,123
Synthesizing Diverse Lung Nodules Wherever Massively: 3D Multi-Conditional GAN-based CT Image Augmentation for Object Detection,"Accurate Computer-Assisted Diagnosis, relying on large-scale annotated
pathological images, can alleviate the risk of overlooking the diagnosis.
Unfortunately, in medical imaging, most available datasets are
small/fragmented. To tackle this, as a Data Augmentation (DA) method, 3D
conditional Generative Adversarial Networks (GANs) can synthesize desired
realistic/diverse 3D images as additional training data. However, no 3D
conditional GAN-based DA approach exists for general bounding box-based 3D
object detection, while it can locate disease areas with physicians' minimum
annotation cost, unlike rigorous 3D segmentation. Moreover, since lesions vary
in position/size/attenuation, further GAN-based DA performance requires
multiple conditions. Therefore, we propose 3D Multi-Conditional GAN (MCGAN) to
generate realistic/diverse 32 X 32 X 32 nodules placed naturally on lung
Computed Tomography images to boost sensitivity in 3D object detection. Our
MCGAN adopts two discriminators for conditioning: the context discriminator
learns to classify real vs synthetic nodule/surrounding pairs with noise
box-centered surroundings; the nodule discriminator attempts to classify real
vs synthetic nodules with size/attenuation conditions. The results show that 3D
Convolutional Neural Network-based detection can achieve higher sensitivity
under any nodule size/attenuation at fixed False Positive rates and overcome
the medical data paucity with the MCGAN-generated realistic nodules---even
expert physicians fail to distinguish them from the real ones in Visual Turing
Test.","['cs.CV', 'eess.IV']",224,154
Induction of Subgoal Automata for Reinforcement Learning,"In this work we present ISA, a novel approach for learning and exploiting
subgoals in reinforcement learning (RL). Our method relies on inducing an
automaton whose transitions are subgoals expressed as propositional formulas
over a set of observable events. A state-of-the-art inductive logic programming
system is used to learn the automaton from observation traces perceived by the
RL agent. The reinforcement learning and automaton learning processes are
interleaved: a new refined automaton is learned whenever the RL agent generates
a trace not recognized by the current automaton. We evaluate ISA in several
gridworld problems and show that it performs similarly to a method for which
automata are given in advance. We also show that the learned automata can be
exploited to speed up convergence through reward shaping and transfer learning
across multiple tasks. Finally, we analyze the running time and the number of
traces that ISA needs to learn an automata, and the impact that the number of
observable events has on the learner's performance.","['cs.LG', 'cs.AI', 'cs.LO', 'stat.ML']",169,105
Unsupervised Adversarial Attacks on Deep Feature-based Retrieval with GAN,"Studies show that Deep Neural Network (DNN)-based image classification models
are vulnerable to maliciously constructed adversarial examples. However, little
effort has been made to investigate how DNN-based image retrieval models are
affected by such attacks. In this paper, we introduce Unsupervised Adversarial
Attacks with Generative Adversarial Networks (UAA-GAN) to attack deep
feature-based image retrieval systems. UAA-GAN is an unsupervised learning
model that requires only a small amount of unlabeled data for training. Once
trained, it produces query-specific perturbations for query images to form
adversarial queries. The core idea is to ensure that the attached perturbation
is barely perceptible to human yet effective in pushing the query away from its
original position in the deep feature space. UAA-GAN works with various
application scenarios that are based on deep features, including image
retrieval, person Re-ID and face search. Empirical results show that UAA-GAN
cripples retrieval performance without significant visual changes in the query
images. UAA-GAN generated adversarial examples are less distinguishable because
they tend to incorporate subtle perturbations in textured or salient areas of
the images, such as key body parts of human, dominant structural
patterns/textures or edges, rather than in visually insignificant areas (e.g.,
background and sky). Such tendency indicates that the model indeed learned how
to toy with both image retrieval systems and human eyes.",['cs.CV'],227,151
Text to Image Generation with Semantic-Spatial Aware GAN,"A text to image generation (T2I) model aims to generate photo-realistic
images which are semantically consistent with the text descriptions. Built upon
the recent advances in generative adversarial networks (GANs), existing T2I
models have made great progress. However, a close inspection of their generated
images reveals two major limitations: (1) The condition batch normalization
methods are applied on the whole image feature maps equally, ignoring the local
semantics; (2) The text encoder is fixed during training, which should be
trained with the image generator jointly to learn better text representations
for image generation. To address these limitations, we propose a novel
framework Semantic-Spatial Aware GAN, which is trained in an end-to-end fashion
so that the text encoder can exploit better text information. Concretely, we
introduce a novel Semantic-Spatial Aware Convolution Network, which (1) learns
semantic-adaptive transformation conditioned on text to effectively fuse text
features and image features, and (2) learns a mask map in a weakly-supervised
way that depends on the current text-image fusion process in order to guide the
transformation spatially. Experiments on the challenging COCO and CUB bird
datasets demonstrate the advantage of our method over the recent
state-of-the-art approaches, regarding both visual fidelity and alignment with
input text description. Code is available at
https://github.com/wtliao/text2image.","['cs.CV', 'cs.LG']",222,148
Unified Generative Adversarial Networks for Controllable Image-to-Image Translation,"We propose a unified Generative Adversarial Network (GAN) for controllable
image-to-image translation, i.e., transferring an image from a source to a
target domain guided by controllable structures. In addition to conditioning on
a reference image, we show how the model can generate images conditioned on
controllable structures, e.g., class labels, object keypoints, human skeletons,
and scene semantic maps. The proposed model consists of a single generator and
a discriminator taking a conditional image and the target controllable
structure as input. In this way, the conditional image can provide appearance
information and the controllable structure can provide the structure
information for generating the target result. Moreover, our model learns the
image-to-image mapping through three novel losses, i.e., color loss,
controllable structure guided cycle-consistency loss, and controllable
structure guided self-content preserving loss. Also, we present the Fr\'echet
ResNet Distance (FRD) to evaluate the quality of the generated images.
Experiments on two challenging image translation tasks, i.e., hand
gesture-to-gesture translation and cross-view image translation, show that our
model generates convincing results, and significantly outperforms other
state-of-the-art methods on both tasks. Meanwhile, the proposed framework is a
unified solution, thus it can be applied to solving other controllable
structure guided image translation tasks such as landmark guided facial
expression translation and keypoint guided person image generation. To the best
of our knowledge, we are the first to make one GAN framework work on all such
controllable structure guided image translation tasks. Code is available at
https://github.com/Ha0Tang/GestureGAN.","['cs.CV', 'cs.LG', 'eess.IV']",263,145
Deep Bidirectional Transformers for Relation Extraction without Supervision,"We present a novel framework to deal with relation extraction tasks in cases
where there is complete lack of supervision, either in the form of gold
annotations, or relations from a knowledge base. Our approach leverages
syntactic parsing and pre-trained word embeddings to extract few but precise
relations,which are then used to annotate a larger cor-pus, in a manner
identical to distant supervision. The resulting data set is employed to fine
tune a pre-trained BERT model in order to perform relation extraction.
Empirical evaluation on four data sets from the biomedical domain shows that
our method significantly outperforms two simple baselines for unsupervised
relation extraction and, even if not using any supervision at all, achieves
slightly worse results than the state-of-the-art in three out of four data
sets. Importantly, we show that it is possible to successfully fine tune a
large pre-trained language model with noisy data, as op-posed to previous works
that rely on gold data for fine tuning.","['cs.LG', 'stat.ML']",169,116
Recurrent Existence Determination Through Policy Optimization,"Binary determination of the presence of objects is one of the problems where
humans perform extraordinarily better than computer vision systems, in terms of
both speed and preciseness. One of the possible reasons is that humans can skip
most of the clutter and attend only on salient regions. Recurrent attention
models (RAM) are the first computational models to imitate the way humans
process images via the REINFORCE algorithm. Despite that RAM is originally
designed for image recognition, we extend it and present recurrent existence
determination, an attention-based mechanism to solve the existence
determination. Our algorithm employs a novel $k$-maximum aggregation layer and
a new reward mechanism to address the issue of delayed rewards, which would
have caused the instability of the training process. The experimental analysis
demonstrates significant efficiency and accuracy improvement over existing
approaches, on both synthetic and real-world datasets.","['cs.LG', 'cs.AI', 'stat.ML']",144,103
Combining Deep Learning and Mathematical Morphology for Historical Map Segmentation,"The digitization of historical maps enables the study of ancient, fragile,
unique, and hardly accessible information sources. Main map features can be
retrieved and tracked through the time for subsequent thematic analysis. The
goal of this work is the vectorization step, i.e., the extraction of vector
shapes of the objects of interest from raster images of maps. We are
particularly interested in closed shape detection such as buildings, building
blocks, gardens, rivers, etc. in order to monitor their temporal evolution.
Historical map images present significant pattern recognition challenges. The
extraction of closed shapes by using traditional Mathematical Morphology (MM)
is highly challenging due to the overlapping of multiple map features and
texts. Moreover, state-of-the-art Convolutional Neural Networks (CNN) are
perfectly designed for content image filtering but provide no guarantee about
closed shape detection. Also, the lack of textural and color information of
historical maps makes it hard for CNN to detect shapes that are represented by
only their boundaries. Our contribution is a pipeline that combines the
strengths of CNN (efficient edge detection and filtering) and MM (guaranteed
extraction of closed shapes) in order to achieve such a task. The evaluation of
our approach on a public dataset shows its effectiveness for extracting the
closed boundaries of objects in historical maps.","['cs.CV', 'eess.IV']",215,135
Real-Time Human Motion Capture with Multiple Depth Cameras,"Commonly used human motion capture systems require intrusive attachment of
markers that are visually tracked with multiple cameras. In this work we
present an efficient and inexpensive solution to markerless motion capture
using only a few Kinect sensors. Unlike the previous work on 3d pose estimation
using a single depth camera, we relax constraints on the camera location and do
not assume a co-operative user. We apply recent image segmentation techniques
to depth images and use curriculum learning to train our system on purely
synthetic data. Our method accurately localizes body parts without requiring an
explicit shape model. The body joint locations are then recovered by combining
evidence from multiple views in real-time. We also introduce a dataset of ~6
million synthetic depth frames for pose estimation from multiple cameras and
exceed state-of-the-art results on the Berkeley MHAD dataset.",['cs.CV'],144,108
The Divergence of Reinforcement Learning Algorithms with Value-Iteration and Function Approximation,"This paper gives specific divergence examples of value-iteration for several
major Reinforcement Learning and Adaptive Dynamic Programming algorithms, when
using a function approximator for the value function. These divergence examples
differ from previous divergence examples in the literature, in that they are
applicable for a greedy policy, i.e. in a ""value iteration"" scenario. Perhaps
surprisingly, with a greedy policy, it is also possible to get divergence for
the algorithms TD(1) and Sarsa(1). In addition to these divergences, we also
achieve divergence for the Adaptive Dynamic Programming algorithms HDP, DHP and
GDHP.",['cs.LG'],95,61
Deep Generative Adversarial Networks for Compressed Sensing Automates MRI,"Magnetic resonance image (MRI) reconstruction is a severely ill-posed linear
inverse task demanding time and resource intensive computations that can
substantially trade off {\it accuracy} for {\it speed} in real-time imaging. In
addition, state-of-the-art compressed sensing (CS) analytics are not cognizant
of the image {\it diagnostic quality}. To cope with these challenges we put
forth a novel CS framework that permeates benefits from generative adversarial
networks (GAN) to train a (low-dimensional) manifold of diagnostic-quality MR
images from historical patients. Leveraging a mixture of least-squares (LS)
GANs and pixel-wise $\ell_1$ cost, a deep residual network with skip
connections is trained as the generator that learns to remove the {\it
aliasing} artifacts by projecting onto the manifold. LSGAN learns the texture
details, while $\ell_1$ controls the high-frequency noise. A multilayer
convolutional neural network is then jointly trained based on diagnostic
quality images to discriminate the projection quality. The test phase performs
feed-forward propagation over the generator network that demands a very low
computational overhead. Extensive evaluations are performed on a large
contrast-enhanced MR dataset of pediatric patients. In particular, images rated
based on expert radiologists corroborate that GANCS retrieves high contrast
images with detailed texture relative to conventional CS, and pixel-wise
schemes. In addition, it offers reconstruction under a few milliseconds, two
orders of magnitude faster than state-of-the-art CS-MRI schemes.","['cs.CV', 'cs.LG', 'stat.ML']",235,155
Stein Variational Inference for Discrete Distributions,"Gradient-based approximate inference methods, such as Stein variational
gradient descent (SVGD), provide simple and general-purpose inference engines
for differentiable continuous distributions. However, existing forms of SVGD
cannot be directly applied to discrete distributions. In this work, we fill
this gap by proposing a simple yet general framework that transforms discrete
distributions to equivalent piecewise continuous distributions, on which the
gradient-free SVGD is applied to perform efficient approximate inference. The
empirical results show that our method outperforms traditional algorithms such
as Gibbs sampling and discontinuous Hamiltonian Monte Carlo on various
challenging benchmarks of discrete graphical models. We demonstrate that our
method provides a promising tool for learning ensembles of binarized neural
network (BNN), outperforming other widely used ensemble methods on learning
binarized AlexNet on CIFAR-10 dataset. In addition, such transform can be
straightforwardly employed in gradient-free kernelized Stein discrepancy to
perform goodness-of-fit (GOF) test on discrete distributions. Our proposed
method outperforms existing GOF test methods for intractable discrete
distributions.","['cs.LG', 'cs.AI', 'stat.ML']",165,109
GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation,"Molecular graph generation is a fundamental problem for drug discovery and
has been attracting growing attention. The problem is challenging since it
requires not only generating chemically valid molecular structures but also
optimizing their chemical properties in the meantime. Inspired by the recent
progress in deep generative models, in this paper we propose a flow-based
autoregressive model for graph generation called GraphAF. GraphAF combines the
advantages of both autoregressive and flow-based approaches and enjoys: (1)
high model flexibility for data density estimation; (2) efficient parallel
computation for training; (3) an iterative sampling process, which allows
leveraging chemical domain knowledge for valency checking. Experimental results
show that GraphAF is able to generate 68% chemically valid molecules even
without chemical knowledge rules and 100% valid molecules with chemical rules.
The training process of GraphAF is two times faster than the existing
state-of-the-art approach GCPN. After fine-tuning the model for goal-directed
property optimization with reinforcement learning, GraphAF achieves
state-of-the-art performance on both chemical property optimization and
constrained property optimization.","['cs.LG', 'stat.ML']",176,119
Higher-Order Explanations of Graph Neural Networks via Relevant Walks,"Graph Neural Networks (GNNs) are a popular approach for predicting graph
structured data. As GNNs tightly entangle the input graph into the neural
network structure, common explainable AI approaches are not applicable. To a
large extent, GNNs have remained black-boxes for the user so far. In this
paper, we show that GNNs can in fact be naturally explained using higher-order
expansions, i.e. by identifying groups of edges that jointly contribute to the
prediction. Practically, we find that such explanations can be extracted using
a nested attribution scheme, where existing techniques such as layer-wise
relevance propagation (LRP) can be applied at each step. The output is a
collection of walks into the input graph that are relevant for the prediction.
Our novel explanation method, which we denote by GNN-LRP, is applicable to a
broad range of graph neural networks and lets us extract practically relevant
insights on sentiment analysis of text data, structure-property relationships
in quantum chemistry, and image classification.","['cs.LG', 'cs.AI', 'stat.ML']",165,118
Equivariant Maps for Hierarchical Structures,"While using invariant and equivariant maps, it is possible to apply deep
learning to a range of primitive data structures, a formalism for dealing with
hierarchy is lacking. This is a significant issue because many practical
structures are hierarchies of simple building blocks; some examples include
sequences of sets, graphs of graphs, or multiresolution images. Observing that
the symmetry of a hierarchical structure is the ""wreath product"" of symmetries
of the building blocks, we express the equivariant map for the hierarchy using
an intuitive combination of the equivariant linear layers of the building
blocks. More generally, we show that any equivariant map for the hierarchy has
this form. To demonstrate the effectiveness of this approach to model design,
we consider its application in the semantic segmentation of point-cloud data.
By voxelizing the point cloud, we impose a hierarchy of translation and
permutation symmetries on the data and report state-of-the-art on Semantic3D,
S3DIS, and vKITTI, that include some of the largest real-world point-cloud
benchmarks.","['cs.LG', 'cs.CV', 'math.GR', 'stat.ML']",169,99
Understanding Neural Architecture Search Techniques,"Automatic methods for generating state-of-the-art neural network
architectures without human experts have generated significant attention
recently. This is because of the potential to remove human experts from the
design loop which can reduce costs and decrease time to model deployment.
Neural architecture search (NAS) techniques have improved significantly in
their computational efficiency since the original NAS was proposed. This
reduction in computation is enabled via weight sharing such as in Efficient
Neural Architecture Search (ENAS). However, recently a body of work confirms
our discovery that ENAS does not do significantly better than random search
with weight sharing, contradicting the initial claims of the authors. We
provide an explanation for this phenomenon by investigating the
interpretability of the ENAS controller's hidden state. We find models sampled
from identical controller hidden states have no correlation with various graph
similarity metrics, so no notion of structural similarity is learned. This
failure mode implies the RNN controller does not condition on past architecture
choices. Lastly, we propose a solution to this failure mode by forcing the
controller's hidden state to encode pasts decisions by training it with a
memory buffer of previously sampled architectures. Doing this improves hidden
state interpretability by increasing the correlation between controller hidden
states and graph similarity metrics.","['cs.LG', 'stat.ML']",213,135
Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time Series Forecasting,"Autonomous driving gained huge traction in recent years, due to its potential
to change the way we commute. Much effort has been put into trying to estimate
the state of a vehicle. Meanwhile, learning to forecast the state of a vehicle
ahead introduces new capabilities, such as predicting dangerous situations.
Moreover, forecasting brings new supervision opportunities by learning to
predict richer a context, expressed by multiple horizons. Intuitively, a video
stream originated from a front-facing camera is necessary because it encodes
information about the upcoming road. Besides, historical traces of the
vehicle's states give more context. In this paper, we tackle multi-horizon
forecasting of vehicle states by fusing the two modalities. We design and
experiment with 3 end-to-end architectures that exploit 3D convolutions for
visual features extraction and 1D convolutions for features extraction from
speed and steering angle traces. To demonstrate the effectiveness of our
method, we perform extensive experiments on two publicly available real-world
datasets, Comma2k19 and the Udacity challenge. We show that we are able to
forecast a vehicle's state to various horizons, while outperforming the current
state-of-the-art results on the related task of driving state estimation. We
examine the contribution of vision features, and find that a model fed with
vision features achieves an error that is 56.6% and 66.9% of the error of a
model that doesn't use those features, on the Udacity and Comma2k19 datasets
respectively.","['cs.CV', 'cs.LG']",244,153
Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention,"Satellite image time series, bolstered by their growing availability, are at
the forefront of an extensive effort towards automated Earth monitoring by
international institutions. In particular, large-scale control of agricultural
parcels is an issue of major political and economic importance. In this regard,
hybrid convolutional-recurrent neural architectures have shown promising
results for the automated classification of satellite image time series.We
propose an alternative approach in which the convolutional layers are
advantageously replaced with encoders operating on unordered sets of pixels to
exploit the typically coarse resolution of publicly available satellite images.
We also propose to extract temporal features using a bespoke neural
architecture based on self-attention instead of recurrent networks. We
demonstrate experimentally that our method not only outperforms previous
state-of-the-art approaches in terms of precision, but also significantly
decreases processing time and memory requirements. Lastly, we release a large
open-access annotated dataset as a benchmark for future work on satellite image
time series.",['cs.CV'],162,119
Transductive image segmentation: Self-training and effect of uncertainty estimation,"Semi-supervised learning (SSL) uses unlabeled data during training to learn
better models. Previous studies on SSL for medical image segmentation focused
mostly on improving model generalization to unseen data. In some applications,
however, our primary interest is not generalization but to obtain optimal
predictions on a specific unlabeled database that is fully available during
model development. Examples include population studies for extracting imaging
phenotypes. This work investigates an often overlooked aspect of SSL,
transduction. It focuses on the quality of predictions made on the unlabeled
data of interest when they are included for optimization during training,
rather than improving generalization. We focus on the self-training framework
and explore its potential for transduction. We analyze it through the lens of
Information Gain and reveal that learning benefits from the use of calibrated
or under-confident models. Our extensive experiments on a large MRI database
for multi-class segmentation of traumatic brain lesions shows promising results
when comparing transductive with inductive predictions. We believe this study
will inspire further research on transductive learning, a well-suited paradigm
for medical image analysis.",['cs.CV'],181,123
ConCAD: Contrastive Learning-based Cross Attention for Sleep Apnea Detection,"With recent advancements in deep learning methods, automatically learning
deep features from the original data is becoming an effective and widespread
approach. However, the hand-crafted expert knowledge-based features are still
insightful. These expert-curated features can increase the model's
generalization and remind the model of some data characteristics, such as the
time interval between two patterns. It is particularly advantageous in tasks
with the clinically-relevant data, where the data are usually limited and
complex. To keep both implicit deep features and expert-curated explicit
features together, an effective fusion strategy is becoming indispensable. In
this work, we focus on a specific clinical application, i.e., sleep apnea
detection. In this context, we propose a contrastive learning-based cross
attention framework for sleep apnea detection (named ConCAD). The cross
attention mechanism can fuse the deep and expert features by automatically
assigning attention weights based on their importance. Contrastive learning can
learn better representations by keeping the instances of each class closer and
pushing away instances from different classes in the embedding space
concurrently. Furthermore, a new hybrid loss is designed to simultaneously
conduct contrastive learning and classification by integrating a supervised
contrastive loss with a cross-entropy loss. Our proposed framework can be
easily integrated into standard deep learning models to utilize expert
knowledge and contrastive learning to boost performance. As demonstrated on two
public ECG dataset with sleep apnea annotation, ConCAD significantly improves
the detection performance and outperforms state-of-art benchmark methods.",['cs.LG'],247,150
Learning to Track Objects from Unlabeled Videos,"In this paper, we propose to learn an Unsupervised Single Object Tracker
(USOT) from scratch. We identify that three major challenges, i.e., moving
object discovery, rich temporal variation exploitation, and online update, are
the central causes of the performance bottleneck of existing unsupervised
trackers. To narrow the gap between unsupervised trackers and supervised
counterparts, we propose an effective unsupervised learning approach composed
of three stages. First, we sample sequentially moving objects with unsupervised
optical flow and dynamic programming, instead of random cropping. Second, we
train a naive Siamese tracker from scratch using single-frame pairs. Third, we
continue training the tracker with a novel cycle memory learning scheme, which
is conducted in longer temporal spans and also enables our tracker to update
online. Extensive experiments show that the proposed USOT learned from
unlabeled videos performs well over the state-of-the-art unsupervised trackers
by large margins, and on par with recent supervised deep trackers. Code is
available at https://github.com/VISION-SJTU/USOT.",['cs.CV'],166,119
Attentive Group Equivariant Convolutional Networks,"Although group convolutional networks are able to learn powerful
representations based on symmetry patterns, they lack explicit means to learn
meaningful relationships among them (e.g., relative positions and poses). In
this paper, we present attentive group equivariant convolutions, a
generalization of the group convolution, in which attention is applied during
the course of convolution to accentuate meaningful symmetry combinations and
suppress non-plausible, misleading ones. We indicate that prior work on visual
attention can be described as special cases of our proposed framework and show
empirically that our attentive group equivariant convolutional networks
consistently outperform conventional group convolutional networks on benchmark
image datasets. Simultaneously, we provide interpretability to the learned
concepts through the visualization of equivariant attention maps.","['cs.CV', 'cs.LG', 'stat.ML']",119,86
Scalable Surface Reconstruction with Delaunay-Graph Neural Networks,"We introduce a novel learning-based, visibility-aware, surface reconstruction
method for large-scale, defect-laden point clouds. Our approach can cope with
the scale and variety of point cloud defects encountered in real-life
Multi-View Stereo (MVS) acquisitions. Our method relies on a 3D Delaunay
tetrahedralization whose cells are classified as inside or outside the surface
by a graph neural network and an energy model solvable with a graph cut. Our
model, making use of both local geometric attributes and line-of-sight
visibility information, is able to learn a visibility model from a small amount
of synthetic training data and generalizes to real-life acquisitions. Combining
the efficiency of deep learning methods and the scalability of energy based
models, our approach outperforms both learning and non learning-based
reconstruction algorithms on two publicly available reconstruction benchmarks.","['cs.CV', 'cs.CG']",139,94
Benchmark for Generic Product Detection: A Low Data Baseline for Dense Object Detection,"Object detection in densely packed scenes is a new area where standard object
detectors fail to train well. Dense object detectors like RetinaNet trained on
large and dense datasets show great performance. We train a standard object
detector on a small, normally packed dataset with data augmentation techniques.
This dataset is 265 times smaller than the standard dataset, in terms of number
of annotations. This low data baseline achieves satisfactory results (mAP=0.56)
at standard IoU of 0.5. We also create a varied benchmark for generic SKU
product detection by providing full annotations for multiple public datasets.
It can be accessed at
https://github.com/ParallelDots/generic-sku-detection-benchmark. We hope that
this benchmark helps in building robust detectors that perform reliably across
different settings in the wild.",['cs.CV'],131,94
Class balanced underwater object detection dataset generated by class-wise style augmentation,"Underwater object detection technique is of great significance for various
applications in underwater the scenes. However, class imbalance issue is still
an unsolved bottleneck for current underwater object detection algorithms. It
leads to large precision discrepancies among different classes that the
dominant classes with more training data achieve higher detection precisions
while the minority classes with fewer training data achieves much lower
detection precisions. In this paper, we propose a novel class-wise style
augmentation (CWSA) algorithm to generate a class-balanced underwater dataset
Balance18 from the public contest underwater dataset URPC2018. CWSA is a new
kind of data augmentation technique which augments the training data for the
minority classes by generating various colors, textures and contrasts for the
minority classes. Compare with previous data augmentation algorithms such
flipping, cropping and rotations, CWSA is able to generate a class balanced
underwater dataset with diverse color distortions and haze-effects.",['cs.CV'],149,91
When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations,"Vision Transformers (ViTs) and MLPs signal further efforts on replacing
hand-wired features or inductive biases with general-purpose neural
architectures. Existing works empower the models by massive data, such as
large-scale pretraining and/or repeated strong data augmentations, and still
report optimization-related problems (e.g., sensitivity to initialization and
learning rate). Hence, this paper investigates ViTs and MLP-Mixers from the
lens of loss geometry, intending to improve the models' data efficiency at
training and generalization at inference. Visualization and Hessian reveal
extremely sharp local minima of converged models. By promoting smoothness with
a recently proposed sharpness-aware optimizer, we substantially improve the
accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning
supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\% and
+11.0\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively,
with the simple Inception-style preprocessing). We show that the improved
smoothness attributes to sparser active neurons in the first few layers. The
resultant ViTs outperform ResNets of similar size and throughput when trained
from scratch on ImageNet without large-scale pretraining or strong data
augmentations. They also possess more perceptive attention maps.","['cs.CV', 'cs.LG']",198,142
A Personalized Affective Memory Neural Model for Improving Emotion Recognition,"Recent models of emotion recognition strongly rely on supervised deep
learning solutions for the distinction of general emotion expressions. However,
they are not reliable when recognizing online and personalized facial
expressions, e.g., for person-specific affective understanding. In this paper,
we present a neural model based on a conditional adversarial autoencoder to
learn how to represent and edit general emotion expressions. We then propose
Grow-When-Required networks as personalized affective memories to learn
individualized aspects of emotion expressions. Our model achieves
state-of-the-art performance on emotion recognition when evaluated on
\textit{in-the-wild} datasets. Furthermore, our experiments include ablation
studies and neural visualizations in order to explain the behavior of our
model.","['cs.CV', 'cs.AI']",117,82
Neural Sequence Model Training via $$-divergence Minimization,"We propose a new neural sequence model training method in which the objective
function is defined by $\alpha$-divergence. We demonstrate that the objective
function generalizes the maximum-likelihood (ML)-based and reinforcement
learning (RL)-based objective functions as special cases (i.e., ML corresponds
to $\alpha \to 0$ and RL to $\alpha \to1$). We also show that the gradient of
the objective function can be considered a mixture of ML- and RL-based
objective gradients. The experimental results of a machine translation task
show that minimizing the objective function with $\alpha > 0$ outperforms
$\alpha \to 0$, which corresponds to ML-based methods.","['stat.ML', 'cs.LG']",103,59
Video Depth Estimation by Fusing Flow-to-Depth Proposals,"Depth from a monocular video can enable billions of devices and robots with a
single camera to see the world in 3D. In this paper, we present an approach
with a differentiable flow-to-depth layer for video depth estimation. The model
consists of a flow-to-depth layer, a camera pose refinement module, and a depth
fusion network. Given optical flow and camera pose, our flow-to-depth layer
generates depth proposals and the corresponding confidence maps by explicitly
solving an epipolar geometry optimization problem. Our flow-to-depth layer is
differentiable, and thus we can refine camera poses by maximizing the
aggregated confidence in the camera pose refinement module. Our depth fusion
network can utilize depth proposals and their confidence maps inferred from
different adjacent frames to produce the final depth map. Furthermore, the
depth fusion network can additionally take the depth proposals generated by
other methods to improve the results further. The experiments on three public
datasets show that our approach outperforms state-of-the-art depth estimation
methods, and has reasonable cross dataset generalization capability: our model
trained on KITTI still performs well on the unseen Waymo dataset.",['cs.CV'],192,105
COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning,"Many real-world video-text tasks involve different levels of granularity,
such as frames and words, clip and sentences or videos and paragraphs, each
with distinct semantics. In this paper, we propose a Cooperative hierarchical
Transformer (COOT) to leverage this hierarchy information and model the
interactions between different levels of granularity and different modalities.
The method consists of three major components: an attention-aware feature
aggregation layer, which leverages the local temporal context (intra-level,
e.g., within a clip), a contextual transformer to learn the interactions
between low-level and high-level semantics (inter-level, e.g. clip-video,
sentence-paragraph), and a cross-modal cycle-consistency loss to connect video
and text. The resulting method compares favorably to the state of the art on
several benchmarks while having few parameters. All code is available
open-source at https://github.com/gingsi/coot-videotext","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG', 'I.2.7; I.2.10']",145,105
Weakly Supervised Object Detection with Pointwise Mutual Information,"In this work a novel approach for weakly supervised object detection that
incorporates pointwise mutual information is presented. A fully convolutional
neural network architecture is applied in which the network learns one filter
per object class. The resulting feature map indicates the location of objects
in an image, yielding an intuitive representation of a class activation map.
While traditionally such networks are learned by a softmax or binary logistic
regression (sigmoid cross-entropy loss), a learning approach based on a cosine
loss is introduced. A pointwise mutual information layer is incorporated in the
network in order to project predictions and ground truth presence labels in a
non-categorical embedding space. Thus, the cosine loss can be employed in this
non-categorical representation. Besides integrating image level annotations, it
is shown how to integrate point-wise annotations using a Spatial Pyramid
Pooling layer. The approach is evaluated on the VOC2012 dataset for
classification, point localization and weakly supervised bounding box
localization. It is shown that the combination of pointwise mutual information
and a cosine loss eases the learning process and thus improves the accuracy.
The integration of coarse point-wise localizations further improves the results
at minimal annotation costs.",['cs.CV'],198,124
Efficient Active Learning for Image Classification and Segmentation using a Sample Selection and Conditional Generative Adversarial Network,"Training robust deep learning (DL) systems for medical image classification
or segmentation is challenging due to limited images covering different disease
types and severity. We propose an active learning (AL) framework to select most
informative samples and add to the training data. We use conditional generative
adversarial networks (cGANs) to generate realistic chest xray images with
different disease characteristics by conditioning its generation on a real
image sample. Informative samples to add to the training set are identified
using a Bayesian neural network. Experiments show our proposed AL framework is
able to achieve state of the art performance by using about 35% of the full
dataset, thus saving significant time and effort over conventional methods.",['cs.CV'],115,88
EAGER: Embedding-Assisted Entity Resolution for Knowledge Graphs,"Entity Resolution (ER) is a constitutional part for integrating different
knowledge graphs in order to identify entities referring to the same real-world
object. A promising approach is the use of graph embeddings for ER in order to
determine the similarity of entities based on the similarity of their graph
neighborhood. The similarity computations for such embeddings translates to
calculating the distance between them in the embedding space which is
comparatively simple. However, previous work has shown that the use of graph
embeddings alone is not sufficient to achieve high ER quality. We therefore
propose a more comprehensive ER approach for knowledge graphs called EAGER
(Embedding-Assisted Knowledge Graph Entity Resolution) to flexibly utilize both
the similarity of graph embeddings and attribute values within a supervised
machine learning approach. We evaluate our approach on 23 benchmark datasets
with differently sized and structured knowledge graphs and use hypothesis tests
to ensure statistical significance of our results. Furthermore we compare our
approach with state-of-the-art ER solutions, where our approach yields
competitive results for table-oriented ER problems and shallow knowledge graphs
but much better results for deeper knowledge graphs.","['cs.LG', 'cs.DB', 'I.2.4; I.2.6']",190,114
Beyond Brightness Constancy: Learning Noise Models for Optical Flow,"Optical flow is typically estimated by minimizing a ""data cost"" and an
optional regularizer. While there has been much work on different regularizers
many modern algorithms still use a data cost that is not very different from
the ones used over 30 years ago: a robust version of brightness constancy or
gradient constancy. In this paper we leverage the recent availability of
ground-truth optical flow databases in order to learn a data cost. Specifically
we take a generative approach in which the data cost models the distribution of
noise after warping an image according to the flow and we measure the
""goodness"" of a data cost by how well it matches the true distribution of flow
warp error. Consistent with current practice, we find that robust versions of
gradient constancy are better models than simple brightness constancy but a
learned GMM that models the density of patches of warp error gives a much
better fit than any existing assumption of constancy. This significant
advantage of the GMM is due to an explicit modeling of the spatial structure of
warp errors, a feature which is missing from almost all existing data costs in
optical flow. Finally, we show how a good density model of warp error patches
can be used for optical flow estimation on whole images. We replace the data
cost by the expected patch log-likelihood (EPLL), and show how this cost can be
optimized iteratively using an additional step of denoising the warp error
image. The results of our experiments are promising and show that patch models
with higher likelihood lead to better optical flow estimation.",['cs.CV'],269,146
Feature Importance-aware Transferable Adversarial Attacks,"Transferability of adversarial examples is of central importance for
attacking an unknown model, which facilitates adversarial attacks in more
practical scenarios, e.g., black-box attacks. Existing transferable attacks
tend to craft adversarial examples by indiscriminately distorting features to
degrade prediction accuracy in a source model without aware of intrinsic
features of objects in the images. We argue that such brute-force degradation
would introduce model-specific local optimum into adversarial examples, thus
limiting the transferability. By contrast, we propose the Feature
Importance-aware Attack (FIA), which disrupts important object-aware features
that dominate model decisions consistently. More specifically, we obtain
feature importance by introducing the aggregate gradient, which averages the
gradients with respect to feature maps of the source model, computed on a batch
of random transforms of the original clean image. The gradients will be highly
correlated to objects of interest, and such correlation presents invariance
across different models. Besides, the random transforms will preserve intrinsic
features of objects and suppress model-specific information. Finally, the
feature importance guides to search for adversarial examples towards disrupting
critical features, achieving stronger transferability. Extensive experimental
evaluation demonstrates the effectiveness and superior performance of the
proposed FIA, i.e., improving the success rate by 9.5% against normally trained
models and 12.8% against defense models as compared to the state-of-the-art
transferable attacks. Code is available at: https://github.com/hcguoO0/FIA",['cs.CV'],234,149
Unsupervised Domain Adaptation in Semantic Segmentation via Orthogonal and Clustered Embeddings,"Deep learning frameworks allowed for a remarkable advancement in semantic
segmentation, but the data hungry nature of convolutional networks has rapidly
raised the demand for adaptation techniques able to transfer learned knowledge
from label-abundant domains to unlabeled ones. In this paper we propose an
effective Unsupervised Domain Adaptation (UDA) strategy, based on a feature
clustering method that captures the different semantic modes of the feature
distribution and groups features of the same class into tight and
well-separated clusters. Furthermore, we introduce two novel learning
objectives to enhance the discriminative clustering performance: an
orthogonality loss forces spaced out individual representations to be
orthogonal, while a sparsity loss reduces class-wise the number of active
feature channels. The joint effect of these modules is to regularize the
structure of the feature space. Extensive evaluations in the synthetic-to-real
scenario show that we achieve state-of-the-art performance.",['cs.CV'],149,110
GarNet: A Two-Stream Network for Fast and Accurate 3D Cloth Draping,"While Physics-Based Simulation (PBS) can accurately drape a 3D garment on a
3D body, it remains too costly for real-time applications, such as virtual
try-on. By contrast, inference in a deep network, requiring a single forward
pass, is much faster. Taking advantage of this, we propose a novel architecture
to fit a 3D garment template to a 3D body. Specifically, we build upon the
recent progress in 3D point cloud processing with deep networks to extract
garment features at varying levels of detail, including point-wise, patch-wise
and global features. We fuse these features with those extracted in parallel
from the 3D body, so as to model the cloth-body interactions. The resulting
two-stream architecture, which we call as GarNet, is trained using a loss
function inspired by physics-based modeling, and delivers visually plausible
garment shapes whose 3D points are, on average, less than 1 cm away from those
of a PBS method, while running 100 times faster. Moreover, the proposed method
can model various garment types with different cutting patterns when parameters
of those patterns are given as input to the network.",['cs.CV'],189,127
Score-Guided Generative Adversarial Networks,"We propose a Generative Adversarial Network (GAN) that introduces an
evaluator module using pre-trained networks. The proposed model, called
score-guided GAN (ScoreGAN), is trained with an evaluation metric for GANs,
i.e., the Inception score, as a rough guide for the training of the generator.
By using another pre-trained network instead of the Inception network, ScoreGAN
circumvents the overfitting of the Inception network in order that generated
samples do not correspond to adversarial examples of the Inception network.
Also, to prevent the overfitting, the evaluation metrics are employed only as
an auxiliary role, while the conventional target of GANs is mainly used.
Evaluated with the CIFAR-10 dataset, ScoreGAN demonstrated an Inception score
of 10.36$\pm$0.15, which corresponds to state-of-the-art performance.
Furthermore, to generalize the effectiveness of ScoreGAN, the model was further
evaluated with another dataset, i.e., the CIFAR-100; as a result, ScoreGAN
outperformed the other existing methods, where the Fr\'echet Inception Distance
(FID) was 13.98.","['cs.LG', 'cs.CV', 'eess.IV']",169,101
Underestimation Bias and Underfitting in Machine Learning,"Often, what is termed algorithmic bias in machine learning will be due to
historic bias in the training data. But sometimes the bias may be introduced
(or at least exacerbated) by the algorithm itself. The ways in which algorithms
can actually accentuate bias has not received a lot of attention with
researchers focusing directly on methods to eliminate bias - no matter the
source. In this paper we report on initial research to understand the factors
that contribute to bias in classification algorithms. We believe this is
important because underestimation bias is inextricably tied to regularization,
i.e. measures to address overfitting can accentuate bias.","['cs.LG', 'stat.ML']",104,77
Learning Topology from Synthetic Data for Unsupervised Depth Completion,"We present a method for inferring dense depth maps from images and sparse
depth measurements by leveraging synthetic data to learn the association of
sparse point clouds with dense natural shapes, and using the image as evidence
to validate the predicted depth map. Our learned prior for natural shapes uses
only sparse depth as input, not images, so the method is not affected by the
covariate shift when attempting to transfer learned models from synthetic data
to real ones. This allows us to use abundant synthetic data with ground truth
to learn the most difficult component of the reconstruction process, which is
topology estimation, and use the image to refine the prediction based on
photometric evidence. Our approach uses fewer parameters than previous methods,
yet, achieves the state of the art on both indoor and outdoor benchmark
datasets. Code available at:
https://github.com/alexklwong/learning-topology-synthetic-data.","['cs.CV', 'cs.LG', 'cs.RO']",149,96
Learning Functionally Decomposed Hierarchies for Continuous Control Tasks with Path Planning,"We present HiDe, a novel hierarchical reinforcement learning architecture
that successfully solves long horizon control tasks and generalizes to unseen
test scenarios. Functional decomposition between planning and low-level control
is achieved by explicitly separating the state-action spaces across the
hierarchy, which allows the integration of task-relevant knowledge per layer.
We propose an RL-based planner to efficiently leverage the information in the
planning layer of the hierarchy, while the control layer learns a
goal-conditioned control policy. The hierarchy is trained jointly but allows
for the composition of different policies such as transferring layers across
multiple agents. We experimentally show that our method generalizes across
unseen test environments and can scale to tasks well beyond 3x horizon length
compared to both learning and non-learning based approaches. We evaluate on
complex continuous control tasks with sparse rewards, including navigation and
robot manipulation.","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",145,102
Answerer in Questioner's Mind: Information Theoretic Approach to Goal-Oriented Visual Dialog,"Goal-oriented dialog has been given attention due to its numerous
applications in artificial intelligence. Goal-oriented dialogue tasks occur
when a questioner asks an action-oriented question and an answerer responds
with the intent of letting the questioner know a correct action to take. To ask
the adequate question, deep learning and reinforcement learning have been
recently applied. However, these approaches struggle to find a competent
recurrent neural questioner, owing to the complexity of learning a series of
sentences. Motivated by theory of mind, we propose ""Answerer in Questioner's
Mind"" (AQM), a novel information theoretic algorithm for goal-oriented dialog.
With AQM, a questioner asks and infers based on an approximated probabilistic
model of the answerer. The questioner figures out the answerer's intention via
selecting a plausible question by explicitly calculating the information gain
of the candidate intentions and possible answers to each question. We test our
framework on two goal-oriented visual dialog tasks: ""MNIST Counting Dialog"" and
""GuessWhat?!"". In our experiments, AQM outperforms comparative algorithms by a
large margin.","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']",174,113
Depth Estimation on Underwater Omni-directional Images Using a Deep Neural Network,"In this work, we exploit a depth estimation Fully Convolutional Residual
Neural Network (FCRN) for in-air perspective images to estimate the depth of
underwater perspective and omni-directional images. We train one conventional
and one spherical FCRN for underwater perspective and omni-directional images,
respectively. The spherical FCRN is derived from the perspective FCRN via a
spherical longitude-latitude mapping. For that, the omni-directional camera is
modeled as a sphere, while images captured by it are displayed in the
longitude-latitude form. Due to the lack of underwater datasets, we synthesize
images in both data-driven and theoretical ways, which are used in training and
testing. Finally, experiments are conducted on these synthetic images and
results are displayed in both qualitative and quantitative way. The comparison
between ground truth and the estimated depth map indicates the effectiveness of
our method.",['cs.CV'],142,87
When are Deep Networks really better than Random Forests at small sample sizes?,"Random forests (RF) and deep networks (DN) are two of the most popular
machine learning methods in the current scientific literature and yield
differing levels of performance on different data modalities. We wish to
further explore and establish the conditions and domains in which each approach
excels, particularly in the context of sample size and feature dimension. To
address these issues, we tested the performance of these approaches across
tabular, image, and audio settings using varying model parameters and
architectures. Our focus is on datasets with at most 10,000 samples, which
represent a large fraction of scientific and biomedical datasets. In general,
we found RF to excel at tabular and structured data (image and audio) with
small sample sizes, whereas DN performed better on structured data with larger
sample sizes. Although we plan to continue updating this technical report in
the coming months, we believe the current preliminary results may be of
interest to others.","['cs.LG', 'cs.AI']",156,104
Face Identification by SIFT-based Complete Graph Topology,"This paper presents a new face identification system based on Graph Matching
Technique on SIFT features extracted from face images. Although SIFT features
have been successfully used for general object detection and recognition, only
recently they were applied to face recognition. This paper further investigates
the performance of identification techniques based on Graph matching topology
drawn on SIFT features which are invariant to rotation, scaling and
translation. Face projections on images, represented by a graph, can be matched
onto new images by maximizing a similarity function taking into account spatial
distortions and the similarities of the local features. Two graph based
matching techniques have been investigated to deal with false pair assignment
and reducing the number of features to find the optimal feature set between
database and query face SIFT features. The experimental results, performed on
the BANCA database, demonstrate the effectiveness of the proposed system for
automatic face identification.","['cs.CV', 'cs.AI', 'D.2.2; I.2.10']",150,94
Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits,"We derive an algorithm that achieves the optimal (within constants)
pseudo-regret in both adversarial and stochastic multi-armed bandits without
prior knowledge of the regime and time horizon. The algorithm is based on
online mirror descent (OMD) with Tsallis entropy regularization with power
$\alpha=1/2$ and reduced-variance loss estimators. More generally, we define an
adversarial regime with a self-bounding constraint, which includes stochastic
regime, stochastically constrained adversarial regime (Wei and Luo), and
stochastic regime with adversarial corruptions (Lykouris et al.) as special
cases, and show that the algorithm achieves logarithmic regret guarantee in
this regime and all of its special cases simultaneously with the adversarial
regret guarantee.} The algorithm also achieves adversarial and stochastic
optimality in the utility-based dueling bandit setting. We provide empirical
evaluation of the algorithm demonstrating that it significantly outperforms
UCB1 and EXP3 in stochastic environments. We also provide examples of
adversarial environments, where UCB1 and Thompson Sampling exhibit almost
linear regret, whereas our algorithm suffers only logarithmic regret. To the
best of our knowledge, this is the first example demonstrating vulnerability of
Thompson Sampling in adversarial environments. Last, but not least, we present
a general stochastic analysis and a general adversarial analysis of OMD
algorithms with Tsallis entropy regularization for $\alpha\in[0,1]$ and explain
the reason why $\alpha=1/2$ works best.","['cs.LG', 'stat.ML']",223,121
Multi-Instance Learning by Utilizing Structural Relationship among Instances,"Multi-Instance Learning(MIL) aims to learn the mapping between a bag of
instances and the bag-level label. Therefore, the relationships among instances
are very important for learning the mapping. In this paper, we propose an MIL
algorithm based on a graph built by structural relationship among instances
within a bag. Then, Graph Convolutional Network(GCN) and the graph-attention
mechanism are used to learn bag-embedding. In the task of medical image
classification, our GCN-based MIL algorithm makes full use of the structural
relationships among patches(instances) in an original image space domain, and
experimental results verify that our method is more suitable for handling
medical high-resolution images. We also verify experimentally that the proposed
method achieves better results than previous methods on five bechmark MIL
datasets and four medical image datasets.","['cs.LG', 'cs.CV']",136,87
Dialog-based Interactive Image Retrieval,"Existing methods for interactive image retrieval have demonstrated the merit
of integrating user feedback, improving retrieval results. However, most
current systems rely on restricted forms of user feedback, such as binary
relevance responses, or feedback based on a fixed set of relative attributes,
which limits their impact. In this paper, we introduce a new approach to
interactive image search that enables users to provide feedback via natural
language, allowing for more natural and effective interaction. We formulate the
task of dialog-based interactive image retrieval as a reinforcement learning
problem, and reward the dialog system for improving the rank of the target
image during each dialog turn. To mitigate the cumbersome and costly process of
collecting human-machine conversations as the dialog system learns, we train
our system with a user simulator, which is itself trained to describe the
differences between target and candidate images. The efficacy of our approach
is demonstrated in a footwear retrieval application. Experiments on both
simulated and real-world data show that 1) our proposed learning framework
achieves better accuracy than other supervised and reinforcement learning
baselines and 2) user feedback based on natural language rather than
pre-specified attributes leads to more effective retrieval results, and a more
natural and expressive communication interface.","['cs.CV', 'cs.AI']",209,127
Event-LSTM: An Unsupervised and Asynchronous Learning-based Representation for Event-based Data,"Event cameras are activity-driven bio-inspired vision sensors, thereby
resulting in advantages such as sparsity,high temporal resolution, low latency,
and power consumption. Given the different sensing modality of event camera and
high quality of conventional vision paradigm, event processing is predominantly
solved by transforming the sparse and asynchronous events into 2D grid and
subsequently applying standard vision pipelines. Despite the promising results
displayed by supervised learning approaches in 2D grid generation, these
approaches treat the task in supervised manner. Labeled task specific ground
truth event data is challenging to acquire. To overcome this limitation, we
propose Event-LSTM, an unsupervised Auto-Encoder architecture made up of LSTM
layers as a promising alternative to learn 2D grid representation from event
sequence. Compared to competing supervised approaches, ours is a task-agnostic
approach ideally suited for the event domain, where task specific labeled data
is scarce. We also tailor the proposed solution to exploit asynchronous nature
of event stream, which gives it desirable charateristics such as speed
invariant and energy-efficient 2D grid generation. Besides, we also push
state-of-the-art event de-noising forward by introducing memory into the
de-noising process. Evaluations on activity recognition and gesture recognition
demonstrate that our approach yields improvement over state-of-the-art
approaches, while providing the flexibilty to learn from unlabelled data.",['cs.CV'],222,145
Depth-Aware Multi-Grid Deep Homography Estimation with Contextual Correlation,"Homography estimation is an important task in computer vision, such as image
stitching, video stabilization, and camera calibration. Traditional homography
estimation methods heavily depend on the quantity and distribution of feature
points, leading to poor robustness in textureless scenes. The learning
solutions, on the contrary, try to learn robust deep features but demonstrate
unsatisfying performance in the scenes of low overlap rates. In this paper, we
address the two problems simultaneously, by designing a contextual correlation
layer, which can capture the long-range correlation on feature maps and
flexibly be bridged in a learning framework. In addition, considering that a
single homography can not represent the complex spatial transformation in
depth-varying images with parallax, we propose to predict multi-grid homography
from global to local. Moreover, we equip our network with depth perception
capability, by introducing a novel depth-aware shape-preserved loss. Extensive
experiments demonstrate the superiority of our method over other
state-of-the-art solutions in the synthetic benchmark dataset and real-world
dataset. The codes and models will be available at
https://github.com/nie-lang/Multi-Grid-Deep-Homogarphy.","['cs.CV', 'cs.AI']",185,136
Creating Lightweight Object Detectors with Model Compression for Deployment on Edge Devices,"To achieve lightweight object detectors for deployment on the edge devices,
an effective model compression pipeline is proposed in this paper. The
compression pipeline consists of automatic channel pruning for the backbone,
fixed channel deletion for the branch layers and knowledge distillation for the
guidance learning. As results, the Resnet50-v1d is auto-pruned and fine-tuned
on ImageNet to attain a compact base model as the backbone of object detector.
Then, lightweight object detectors are implemented with proposed compression
pipeline. For instance, the SSD-300 with model size=16.3MB, FLOPS=2.31G, and
mAP=71.2 is created, revealing a better result than SSD-300-MobileNet.","['cs.CV', 'eess.IV']",108,75
Approximate Newton-based statistical inference using only stochastic gradients,"We present a novel statistical inference framework for convex empirical risk
minimization, using approximate stochastic Newton steps. The proposed algorithm
is based on the notion of finite differences and allows the approximation of a
Hessian-vector product from first-order information. In theory, our method
efficiently computes the statistical error covariance in $M$-estimation, both
for unregularized convex learning problems and high-dimensional LASSO
regression, without using exact second order information, or resampling the
entire data set. We also present a stochastic gradient sampling scheme for
statistical inference in non-i.i.d. time series analysis, where we sample
contiguous blocks of indices. In practice, we demonstrate the effectiveness of
our framework on large-scale machine learning problems, that go even beyond
convexity: as a highlight, our work can be used to detect certain adversarial
attacks on neural networks.","['cs.LG', 'cs.CV', 'math.OC', 'math.ST', 'stat.ML', 'stat.TH']",139,105
Graph Learning: A Survey,"Graphs are widely used as a popular representation of the network structure
of connected data. Graph data can be found in a broad spectrum of application
domains such as social systems, ecosystems, biological networks, knowledge
graphs, and information systems. With the continuous penetration of artificial
intelligence technologies, graph learning (i.e., machine learning on graphs) is
gaining attention from both researchers and practitioners. Graph learning
proves effective for many tasks, such as classification, link prediction, and
matching. Generally, graph learning methods extract relevant features of graphs
by taking advantage of machine learning algorithms. In this survey, we present
a comprehensive overview on the state-of-the-art of graph learning. Special
attention is paid to four categories of existing graph learning methods,
including graph signal processing, matrix factorization, random walk, and deep
learning. Major models and algorithms under these categories are reviewed
respectively. We examine graph learning applications in areas such as text,
images, science, knowledge graphs, and combinatorial optimization. In addition,
we discuss several promising research directions in this field.","['cs.LG', 'cs.AI', 'cs.SI', '68T07', 'I.2.6']",171,115
Multi-modal Ensemble Classification for Generalized Zero Shot Learning,"Generalized zero shot learning (GZSL) is defined by a training process
containing a set of visual samples from seen classes and a set of semantic
samples from seen and unseen classes, while the testing process consists of the
classification of visual samples from seen and unseen classes. Current
approaches are based on testing processes that focus on only one of the
modalities (visual or semantic), even when the training uses both modalities
(mostly for regularizing the training process). This under-utilization of
modalities, particularly during testing, can hinder the classification accuracy
of the method. In addition, we note a scarce attention to the development of
learning methods that explicitly optimize a balanced performance of seen and
unseen classes. Such issue is one of the reasons behind the vastly superior
classification accuracy of seen classes in GZSL methods. In this paper, we
mitigate these issues by proposing a new GZSL method based on multi-modal
training and testing processes, where the optimization explicitly promotes a
balanced classification accuracy between seen and unseen classes. Furthermore,
we explore Bayesian inference for the visual and semantic classifiers, which is
another novelty of our work in the GZSL framework. Experiments show that our
method holds the state of the art (SOTA) results in terms of harmonic mean
(H-mean) classification between seen and unseen classes and area under the seen
and unseen curve (AUSUC) on several public GZSL benchmarks.",['cs.CV'],234,116
An Unsupervised Approach to Solving Inverse Problems using Generative Adversarial Networks,"Solving inverse problems continues to be a challenge in a wide array of
applications ranging from deblurring, image inpainting, source separation etc.
Most existing techniques solve such inverse problems by either explicitly or
implicitly finding the inverse of the model. The former class of techniques
require explicit knowledge of the measurement process which can be unrealistic,
and rely on strong analytical regularizers to constrain the solution space,
which often do not generalize well. The latter approaches have had remarkable
success in part due to deep learning, but require a large collection of
source-observation pairs, which can be prohibitively expensive. In this paper,
we propose an unsupervised technique to solve inverse problems with generative
adversarial networks (GANs). Using a pre-trained GAN in the space of source
signals, we show that one can reliably recover solutions to under determined
problems in a `blind' fashion, i.e., without knowledge of the measurement
process. We solve this by making successive estimates on the model and the
solution in an iterative fashion. We show promising results in three
challenging applications -- blind source separation, image deblurring, and
recovering an image from its edge map, and perform better than several
baselines.","['cs.CV', 'stat.ML']",196,125
Ground Metric Learning on Graphs,"Optimal transport (OT) distances between probability distributions are
parameterized by the ground metric they use between observations. Their
relevance for real-life applications strongly hinges on whether that ground
metric parameter is suitably chosen. Selecting it adaptively and
algorithmically from prior knowledge, the so-called ground metric learning GML)
problem, has therefore appeared in various settings. We consider it in this
paper when the learned metric is constrained to be a geodesic distance on a
graph that supports the measures of interest. This imposes a rich structure for
candidate metrics, but also enables far more efficient learning procedures when
compared to a direct optimization over the space of all metric matrices. We use
this setting to tackle an inverse problem stemming from the observation of a
density evolving with time: we seek a graph ground metric such that the OT
interpolation between the starting and ending densities that result from that
ground metric agrees with the observed evolution. This OT dynamic framework is
relevant to model natural phenomena exhibiting displacements of mass, such as
for instance the evolution of the color palette induced by the modification of
lighting and materials.","['stat.ML', 'cs.GR', 'cs.LG', 'math.OC']",190,125
Contraband Materials Detection Within Volumetric 3D Computed Tomography Baggage Security Screening Imagery,"Automatic prohibited object detection within 2D/3D X-ray Computed Tomography
(CT) has been studied in literature to enhance the aviation security screening
at checkpoints. Deep Convolutional Neural Networks (CNN) have demonstrated
superior performance in 2D X-ray imagery. However, there exists very limited
proof of how deep neural networks perform in materials detection within
volumetric 3D CT baggage screening imagery. We attempt to close this gap by
applying Deep Neural Networks in 3D contraband substance detection based on
their material signatures. Specifically, we formulate it as a 3D semantic
segmentation problem to identify material types for all voxels based on which
contraband materials can be detected. To this end, we firstly investigate 3D
CNN based semantic segmentation algorithms such as 3D U-Net and its variants.
In contrast to the original dense representation form of volumetric 3D CT data,
we propose to convert the CT volumes into sparse point clouds which allows the
use of point cloud processing approaches such as PointNet++ towards more
efficient processing. Experimental results on a publicly available dataset (NEU
ATR) demonstrate the effectiveness of both 3D U-Net and PointNet++ in materials
detection in 3D CT imagery for baggage security screening.","['cs.CV', 'cs.LG', 'eess.IV']",197,126
Detecting Overfitting of Deep Generative Networks via Latent Recovery,"State of the art deep generative networks are capable of producing images
with such incredible realism that they can be suspected of memorizing training
images. It is why it is not uncommon to include visualizations of training set
nearest neighbors, to suggest generated images are not simply memorized. We
demonstrate this is not sufficient and motivates the need to study
memorization/overfitting of deep generators with more scrutiny. This paper
addresses this question by i) showing how simple losses are highly effective at
reconstructing images for deep generators ii) analyzing the statistics of
reconstruction errors when reconstructing training and validation images, which
is the standard way to analyze overfitting in machine learning. Using this
methodology, this paper shows that overfitting is not detectable in the pure
GAN models proposed in the literature, in contrast with those using hybrid
adversarial losses, which are amongst the most widely applied generative
methods. The paper also shows that standard GAN evaluation metrics fail to
capture memorization for some deep generators. Finally, the paper also shows
how off-the-shelf GAN generators can be successfully applied to face inpainting
and face super-resolution using the proposed reconstruction method, without
hybrid adversarial losses.","['cs.LG', 'stat.ML']",197,116
SaccadeCam: Adaptive Visual Attention for Monocular Depth Sensing,"Most monocular depth sensing methods use conventionally captured images that
are created without considering scene content. In contrast, animal eyes have
fast mechanical motions, called saccades, that control how the scene is imaged
by the fovea, where resolution is highest. In this paper, we present the
SaccadeCam framework for adaptively distributing resolution onto regions of
interest in the scene. Our algorithm for adaptive resolution is a
self-supervised network and we demonstrate results for end-to-end learning for
monocular depth estimation. We also show preliminary results with a real
SaccadeCam hardware prototype.",['cs.CV'],93,72
A Hierarchical Approach for Visual Storytelling Using Image Description,"One of the primary challenges of visual storytelling is developing techniques
that can maintain the context of the story over long event sequences to
generate human-like stories. In this paper, we propose a hierarchical deep
learning architecture based on encoder-decoder networks to address this
problem. To better help our network maintain this context while also generating
long and diverse sentences, we incorporate natural language image descriptions
along with the images themselves to generate each story sentence. We evaluate
our system on the Visual Storytelling (VIST) dataset and show that our method
outperforms state-of-the-art techniques on a suite of different automatic
evaluation metrics. The empirical results from this evaluation demonstrate the
necessities of different components of our proposed architecture and shows the
effectiveness of the architecture for visual storytelling.","['cs.CV', 'cs.CL', 'cs.LG', 'stat.ML']",133,91
Bag of Color Features For Color Constancy,"In this paper, we propose a novel color constancy approach, called Bag of
Color Features (BoCF), building upon Bag-of-Features pooling. The proposed
method substantially reduces the number of parameters needed for illumination
estimation. At the same time, the proposed method is consistent with the color
constancy assumption stating that global spatial information is not relevant
for illumination estimation and local information ( edges, etc.) is sufficient.
Furthermore, BoCF is consistent with color constancy statistical approaches and
can be interpreted as a learning-based generalization of many statistical
approaches. To further improve the illumination estimation accuracy, we propose
a novel attention mechanism for the BoCF model with two variants based on
self-attention. BoCF approach and its variants achieve competitive, compared to
the state of the art, results while requiring much fewer parameters on three
benchmark datasets: ColorChecker RECommended, INTEL-TUT version 2, and NUS8.",['cs.CV'],145,94
A low discrepancy sequence on graphs,"Many applications such as election forecasting, environmental monitoring,
health policy, and graph based machine learning require taking expectation of
functions defined on the vertices of a graph. We describe a construction of a
sampling scheme analogous to the so called Leja points in complex potential
theory that can be proved to give low discrepancy estimates for the
approximation of the expected value by the impirical expected value based on
these points. In contrast to classical potential theory where the kernel is
fixed and the equilibrium distribution depends upon the kernel, we fix a
probability distribution and construct a kernel (which represents the graph
structure) for which the equilibrium distribution is the given probability
distribution. Our estimates do not depend upon the size of the graph.","['cs.LG', 'math.PR']",125,79
Hierarchical Graph Capsule Network,"Graph Neural Networks (GNNs) draw their strength from explicitly modeling the
topological information of structured data. However, existing GNNs suffer from
limited capability in capturing the hierarchical graph representation which
plays an important role in graph classification. In this paper, we innovatively
propose hierarchical graph capsule network (HGCN) that can jointly learn node
embeddings and extract graph hierarchies. Specifically, disentangled graph
capsules are established by identifying heterogeneous factors underlying each
node, such that their instantiation parameters represent different properties
of the same entity. To learn the hierarchical representation, HGCN
characterizes the part-whole relationship between lower-level capsules (part)
and higher-level capsules (whole) by explicitly considering the structure
information among the parts. Experimental studies demonstrate the effectiveness
of HGCN and the contribution of each component.","['cs.LG', 'cs.SI']",126,88
Learning from Irregularly-Sampled Time Series: A Missing Data Perspective,"Irregularly-sampled time series occur in many domains including healthcare.
They can be challenging to model because they do not naturally yield a
fixed-dimensional representation as required by many standard machine learning
models. In this paper, we consider irregular sampling from the perspective of
missing data. We model observed irregularly-sampled time series data as a
sequence of index-value pairs sampled from a continuous but unobserved
function. We introduce an encoder-decoder framework for learning from such
generic indexed sequences. We propose learning methods for this framework based
on variational autoencoders and generative adversarial networks. For continuous
irregularly-sampled time series, we introduce continuous convolutional layers
that can efficiently interface with existing neural network architectures.
Experiments show that our models are able to achieve competitive or better
classification results on irregularly-sampled multivariate time series compared
to recent RNN models while offering significantly faster training times.","['cs.LG', 'stat.ML']",148,109
StackGAN: Facial Image Generation Optimizations,"Current state-of-the-art photorealistic generators are computationally
expensive, involve unstable training processes, and have real and synthetic
distributions that are dissimilar in higher-dimensional spaces. To solve these
issues, we propose a variant of the StackGAN architecture. The new architecture
incorporates conditional generators to construct an image in many stages. In
our model, we generate grayscale facial images in two different stages: noise
to edges (stage one) and edges to grayscale (stage two). Our model is trained
with the CelebA facial image dataset and achieved a Fr\'echet Inception
Distance (FID) score of 73 for edge images and a score of 59 for grayscale
images generated using the synthetic edge images. Although our model achieved
subpar results in relation to state-of-the-art models, dropout layers could
reduce the overfitting in our conditional mapping. Additionally, since most
images can be broken down into important features, improvements to our model
can generalize to other datasets. Therefore, our model can potentially serve as
a superior alternative to traditional means of generating photorealistic
images.","['cs.CV', 'cs.LG', 'eess.IV']",174,111
An Efficient Quantitative Approach for Optimizing Convolutional Neural Networks,"With the increasing popularity of deep learning, Convolutional Neural
Networks (CNNs) have been widely applied in various domains, such as image
classification and object detection, and achieve stunning success in terms of
their high accuracy over the traditional statistical methods. To exploit the
potential of CNN models, a huge amount of research and industry efforts have
been devoted to optimizing CNNs. Among these endeavors, CNN architecture design
has attracted tremendous attention because of its great potential of improving
model accuracy or reducing model complexity. However, existing work either
introduces repeated training overhead in the search process or lacks an
interpretable metric to guide the design. To clear these hurdles, we propose
3D-Receptive Field (3DRF), an explainable and easy-to-compute metric, to
estimate the quality of a CNN architecture and guide the search process of
designs. To validate the effectiveness of 3DRF, we build a static optimizer to
improve the CNN architectures at both the stage level and the kernel level. Our
optimizer not only provides a clear and reproducible procedure but also
mitigates unnecessary training efforts in the architecture search process.
Extensive experiments and studies show that the models generated by our
optimizer can achieve up to 5.47% accuracy improvement and up to 65.38%
parameters deduction, compared with state-of-the-art CNN structures like
MobileNet and ResNet.","['cs.CV', 'cs.LG', 'eess.IV']",222,143
OctNet: Learning Deep 3D Representations at High Resolutions,"We present OctNet, a representation for deep learning with sparse 3D data. In
contrast to existing models, our representation enables 3D convolutional
networks which are both deep and high resolution. Towards this goal, we exploit
the sparsity in the input data to hierarchically partition the space using a
set of unbalanced octrees where each leaf node stores a pooled feature
representation. This allows to focus memory allocation and computation to the
relevant dense regions and enables deeper networks without compromising
resolution. We demonstrate the utility of our OctNet representation by
analyzing the impact of resolution on several 3D tasks including 3D object
classification, orientation estimation and point cloud labeling.",['cs.CV'],109,79
Dense Optical Flow based Change Detection Network Robust to Difference of Camera Viewpoints,"This paper presents a novel method for detecting scene changes from a pair of
images with a difference of camera viewpoints using a dense optical flow based
change detection network. In the case that camera poses of input images are
fixed or known, such as with surveillance and satellite cameras, the pixel
correspondence between the images captured at different times can be known.
Hence, it is possible to comparatively accurately detect scene changes between
the images by modeling the appearance of the scene. On the other hand, in case
of cameras mounted on a moving object, such as ground and aerial vehicles, we
must consider the spatial correspondence between the images captured at
different times. However, it can be difficult to accurately estimate the camera
pose or 3D model of a scene, owing to the scene changes or lack of imagery. To
solve this problem, we propose a change detection convolutional neural network
utilizing dense optical flow between input images to improve the robustness to
the difference between camera viewpoints. Our evaluation based on the panoramic
change detection dataset shows that the proposed method outperforms
state-of-the-art change detection algorithms.",['cs.CV'],192,106
Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series,"Integrating deep learning with latent state space models has the potential to
yield temporal models that are powerful, yet tractable and interpretable.
Unfortunately, current models are not designed to handle missing data or
multiple data modalities, which are both prevalent in real-world data. In this
work, we introduce a factorized inference method for Multimodal Deep Markov
Models (MDMMs), allowing us to filter and smooth in the presence of missing
data, while also performing uncertainty-aware multimodal fusion. We derive this
method by factorizing the posterior p(z|x) for non-linear state space models,
and develop a variational backward-forward algorithm for inference. Because our
method handles incompleteness over both time and modalities, it is capable of
interpolation, extrapolation, conditional generation, label prediction, and
weakly supervised learning of multimodal time series. We demonstrate these
capabilities on both synthetic and real-world multimodal data under high levels
of data deletion. Our method performs well even with more than 50% missing
data, and outperforms existing deep approaches to inference in latent time
series.","['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",172,117
Neural Subgraph Isomorphism Counting,"In this paper, we study a new graph learning problem: learning to count
subgraph isomorphisms. Different from other traditional graph learning problems
such as node classification and link prediction, subgraph isomorphism counting
is NP-complete and requires more global inference to oversee the whole graph.
To make it scalable for large-scale graphs and patterns, we propose a learning
framework which augments different representation learning architectures and
iteratively attends pattern and target data graphs to memorize subgraph
isomorphisms for the global counting. We develop both small graphs (<= 1,024
subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms
in each) sets to evaluate different models. A mutagenic compound dataset,
MUTAG, is also used to evaluate neural models and demonstrate the success of
transfer learning. While the learning based approach is inexact, we are able to
generalize to count large patterns and data graphs in linear time compared to
the exponential time of the original NP-complete problem. Experimental results
show that learning based subgraph isomorphism counting can speed up the
traditional algorithm, VF2, 10-1,000 times with acceptable errors. Domain
adaptation based on fine-tuning also shows the usefulness of our approach in
real-world applications.","['cs.LG', 'stat.ML']",199,123
Few-shot Object Detection with Self-adaptive Attention Network for Remote Sensing Images,"In remote sensing field, there are many applications of object detection in
recent years, which demands a great number of labeled data. However, we may be
faced with some cases where only limited data are available. In this paper, we
proposed a few-shot object detector which is designed for detecting novel
objects provided with only a few examples. Particularly, in order to fit the
object detection settings, our proposed few-shot detector concentrates on the
relations that lie in the level of objects instead of the full image with the
assistance of Self-Adaptive Attention Network (SAAN). The SAAN can fully
leverage the object-level relations through a relation GRU unit and
simultaneously attach attention on object features in a self-adaptive way
according to the object-level relations to avoid some situations where the
additional attention is useless or even detrimental. Eventually, the detection
results are produced from the features that are added with attention and thus
are able to be detected simply. The experiments demonstrate the effectiveness
of the proposed method in few-shot scenes.",['cs.CV'],178,107
"Random Forests, Decision Trees, and Categorical Predictors: The ""Absent Levels"" Problem","One advantage of decision tree based methods like random forests is their
ability to natively handle categorical predictors without having to first
transform them (e.g., by using feature engineering techniques). However, in
this paper, we show how this capability can lead to an inherent ""absent levels""
problem for decision tree based methods that has never been thoroughly
discussed, and whose consequences have never been carefully explored. This
problem occurs whenever there is an indeterminacy over how to handle an
observation that has reached a categorical split which was determined when the
observation in question's level was absent during training. Although these
incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's
random forests FORTRAN code and the randomForest R package (Liaw and Wiener,
2002) as motivating case studies, we examine how overlooking the absent levels
problem can systematically bias a model. Furthermore, by using three real data
examples, we illustrate how absent levels can dramatically alter a model's
performance in practice, and we empirically demonstrate how some simple
heuristics can be used to help mitigate the effects of the absent levels
problem until a more robust theoretical solution is found.","['stat.ML', 'cs.LG']",197,131
A Simple Fine-tuning Is All You Need: Towards Robust Deep Learning Via Adversarial Fine-tuning,"Adversarial Training (AT) with Projected Gradient Descent (PGD) is an
effective approach for improving the robustness of the deep neural networks.
However, PGD AT has been shown to suffer from two main limitations: i) high
computational cost, and ii) extreme overfitting during training that leads to
reduction in model generalization. While the effect of factors such as model
capacity and scale of training data on adversarial robustness have been
extensively studied, little attention has been paid to the effect of a very
important parameter in every network optimization on adversarial robustness:
the learning rate. In particular, we hypothesize that effective learning rate
scheduling during adversarial training can significantly reduce the overfitting
issue, to a degree where one does not even need to adversarially train a model
from scratch but can instead simply adversarially fine-tune a pre-trained
model. Motivated by this hypothesis, we propose a simple yet very effective
adversarial fine-tuning approach based on a $\textit{slow start, fast decay}$
learning rate scheduling strategy which not only significantly decreases
computational cost required, but also greatly improves the accuracy and
robustness of a deep neural network. Experimental results show that the
proposed adversarial fine-tuning approach outperforms the state-of-the-art
methods on CIFAR-10, CIFAR-100 and ImageNet datasets in both test accuracy and
the robustness, while reducing the computational cost by 8-10$\times$.
Furthermore, a very important benefit of the proposed adversarial fine-tuning
approach is that it enables the ability to improve the robustness of any
pre-trained deep neural network without needing to train the model from
scratch, which to the best of the authors' knowledge has not been previously
demonstrated in research literature.","['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE']",281,155
Refine Myself by Teaching Myself: Feature Refinement via Self-Knowledge Distillation,"Knowledge distillation is a method of transferring the knowledge from a
pretrained complex teacher model to a student model, so a smaller network can
replace a large teacher network at the deployment stage. To reduce the
necessity of training a large teacher model, the recent literatures introduced
a self-knowledge distillation, which trains a student network progressively to
distill its own knowledge without a pretrained teacher network. While
Self-knowledge distillation is largely divided into a data augmentation based
approach and an auxiliary network based approach, the data augmentation
approach looses its local information in the augmentation process, which
hinders its applicability to diverse vision tasks, such as semantic
segmentation. Moreover, these knowledge distillation approaches do not receive
the refined feature maps, which are prevalent in the object detection and
semantic segmentation community. This paper proposes a novel self-knowledge
distillation method, Feature Refinement via Self-Knowledge Distillation
(FRSKD), which utilizes an auxiliary self-teacher network to transfer a refined
knowledge for the classifier network. Our proposed method, FRSKD, can utilize
both soft label and feature-map distillations for the self-knowledge
distillation. Therefore, FRSKD can be applied to classification, and semantic
segmentation, which emphasize preserving the local information. We demonstrate
the effectiveness of FRSKD by enumerating its performance improvements in
diverse tasks and benchmark datasets. The implemented code is available at
https://github.com/MingiJi/FRSKD.",['cs.CV'],227,124
The Impact of Explanations on AI Competency Prediction in VQA,"Explainability is one of the key elements for building trust in AI systems.
Among numerous attempts to make AI explainable, quantifying the effect of
explanations remains a challenge in conducting human-AI collaborative tasks.
Aside from the ability to predict the overall behavior of AI, in many
applications, users need to understand an AI agent's competency in different
aspects of the task domain. In this paper, we evaluate the impact of
explanations on the user's mental model of AI agent competency within the task
of visual question answering (VQA). We quantify users' understanding of
competency, based on the correlation between the actual system performance and
user rankings. We introduce an explainable VQA system that uses spatial and
object features and is powered by the BERT language model. Each group of users
sees only one kind of explanation to rank the competencies of the VQA model.
The proposed model is evaluated through between-subject experiments to probe
explanations' impact on the user's perception of competency. The comparison
between two VQA models shows BERT based explanations and the use of object
features improve the user's prediction of the model's competencies.","['cs.CV', 'cs.AI', 'cs.HC']",193,107
Event-Based Structured Light for Depth Reconstruction using Frequency Tagged Light Patterns,"This paper presents a new method for 3D depth estimation using the output of
an asynchronous time driven image sensor. In association with a high speed
Digital Light Processing projection system, our method achieves real-time
reconstruction of 3D points cloud, up to several hundreds of hertz. Unlike
state of the art methodology, we introduce a method that relies on the use of
frequency tagged light pattern that make use of the high temporal resolution of
event based sensors. This approch eases matching as each pattern unique
frequency allow for any easy matching between displayed patterns and the event
based sensor. Results are show on real scenes.",['cs.CV'],107,78
SOLQ: Segmenting Objects by Learning Queries,"In this paper, we propose an end-to-end framework for instance segmentation.
Based on the recently introduced DETR [1], our method, termed SOLQ, segments
objects by learning unified queries. In SOLQ, each query represents one object
and has multiple representations: class, location and mask. The object queries
learned perform classification, box regression and mask encoding simultaneously
in an unified vector form. During training phase, the mask vectors encoded are
supervised by the compression coding of raw spatial masks. In inference time,
mask vectors produced can be directly transformed to spatial masks by the
inverse process of compression coding. Experimental results show that SOLQ can
achieve state-of-the-art performance, surpassing most of existing approaches.
Moreover, the joint learning of unified query representation can greatly
improve the detection performance of original DETR. We hope our SOLQ can serve
as a strong baseline for the Transformer-based instance segmentation. Code is
available at https://github.com/megvii-research/SOLQ.",['cs.CV'],159,111
Training Object Detectors on Synthetic Images Containing Reflecting Materials,"One of the grand challenges of deep learning is the requirement to obtain
large labeled training data sets. While synthesized data sets can be used to
overcome this challenge, it is important that these data sets close the reality
gap, i.e., a model trained on synthetic image data is able to generalize to
real images. Whereas, the reality gap can be considered bridged in several
application scenarios, training on synthesized images containing reflecting
materials requires further research. Since the appearance of objects with
reflecting materials is dominated by the surrounding environment, this
interaction needs to be considered during training data generation. Therefore,
within this paper we examine the effect of reflecting materials in the context
of synthetic image generation for training object detectors. We investigate the
influence of rendering approach used for image synthesis, the effect of domain
randomization, as well as the amount of used training data. To be able to
compare our results to the state-of-the-art, we focus on indoor scenes as they
have been investigated extensively. Within this scenario, bathroom furniture is
a natural choice for objects with reflecting materials, for which we report our
findings on real and synthetic testing data.","['cs.CV', 'cs.GR', 'stat.ML']",199,114
Temporally Abstract Partial Models,"Humans and animals have the ability to reason and make predictions about
different courses of action at many time scales. In reinforcement learning,
option models (Sutton, Precup \& Singh, 1999; Precup, 2000) provide the
framework for this kind of temporally abstract prediction and reasoning.
Natural intelligent agents are also able to focus their attention on courses of
action that are relevant or feasible in a given situation, sometimes termed
affordable actions. In this paper, we define a notion of affordances for
options, and develop temporally abstract partial option models, that take into
account the fact that an option might be affordable only in certain situations.
We analyze the trade-offs between estimation and approximation error in
planning and learning when using such models, and identify some interesting
special cases. Additionally, we demonstrate empirically the potential impact of
partial option models on the efficiency of planning.","['cs.LG', 'cs.AI', 'stat.ML']",144,101
Multistage Model for Robust Face Alignment Using Deep Neural Networks,"An ability to generalize unconstrained conditions such as severe occlusions
and large pose variations remains a challenging goal to achieve in face
alignment. In this paper, a multistage model based on deep neural networks is
proposed which takes advantage of spatial transformer networks, hourglass
networks and exemplar-based shape constraints. First, a spatial transformer -
generative adversarial network which consists of convolutional layers and
residual units is utilized to solve the initialization issues caused by face
detectors, such as rotation and scale variations, to obtain improved face
bounding boxes for face alignment. Then, stacked hourglass network is employed
to obtain preliminary locations of landmarks as well as their corresponding
scores. In addition, an exemplar-based shape dictionary is designed to
determine landmarks with low scores based on those with high scores. By
incorporating face shape constraints, misaligned landmarks caused by occlusions
or cluttered backgrounds can be considerably improved. Extensive experiments
based on challenging benchmark datasets are performed to demonstrate the
superior performance of the proposed method over other state-of-the-art
methods.","['cs.CV', 'eess.IV']",172,111
Modelling the Scene Dependent Imaging in Cameras with a Deep Neural Network,"We present a novel deep learning framework that models the scene dependent
image processing inside cameras. Often called as the radiometric calibration,
the process of recovering RAW images from processed images (JPEG format in the
sRGB color space) is essential for many computer vision tasks that rely on
physically accurate radiance values. All previous works rely on the
deterministic imaging model where the color transformation stays the same
regardless of the scene and thus they can only be applied for images taken
under the manual mode. In this paper, we propose a data-driven approach to
learn the scene dependent and locally varying image processing inside cameras
under the automode. Our method incorporates both the global and the local scene
context into pixel-wise features via multi-scale pyramid of learnable histogram
layers. The results show that we can model the imaging pipeline of different
cameras that operate under the automode accurately in both directions (from RAW
to sRGB, from sRGB to RAW) and we show how we can apply our method to improve
the performance of image deblurring.",['cs.CV'],179,114
A Review of Relational Machine Learning for Knowledge Graphs,"Relational machine learning studies methods for the statistical analysis of
relational, or graph-structured, data. In this paper, we provide a review of
how such statistical models can be ""trained"" on large knowledge graphs, and
then used to predict new facts about the world (which is equivalent to
predicting new edges in the graph). In particular, we discuss two fundamentally
different kinds of statistical relational models, both of which can scale to
massive datasets. The first is based on latent feature models such as tensor
factorization and multiway neural networks. The second is based on mining
observable patterns in the graph. We also show how to combine these latent and
observable models to get improved modeling power at decreased computational
cost. Finally, we discuss how such statistical models of graphs can be combined
with text-based information extraction methods for automatically constructing
knowledge graphs from the Web. To this end, we also discuss Google's Knowledge
Vault project as an example of such combination.","['stat.ML', 'cs.LG']",164,105
High dimensional Bayesian Optimization Algorithm for Complex System in Time Series,"At present, high-dimensional global optimization problems with time-series
models have received much attention from engineering fields. Since it was
proposed, Bayesian optimization has quickly become a popular and promising
approach for solving global optimization problems. However, the standard
Bayesian optimization algorithm is insufficient to solving the global optimal
solution when the model is high-dimensional. Hence, this paper presents a novel
high dimensional Bayesian optimization algorithm by considering dimension
reduction and different dimension fill-in strategies. Most existing literature
about Bayesian optimization algorithms did not discuss the sampling strategies
to optimize the acquisition function. This study proposed a new sampling method
based on both the multi-armed bandit and random search methods while optimizing
the acquisition function. Besides, based on the time-dependent or
dimension-dependent characteristics of the model, the proposed algorithm can
reduce the dimension evenly. Then, five different dimension fill-in strategies
were discussed and compared in this study. Finally, to increase the final
accuracy of the optimal solution, the proposed algorithm adds a local search
based on a series of Adam-based steps at the final stage. Our computational
experiments demonstrated that the proposed Bayesian optimization algorithm
could achieve reasonable solutions with excellent performances for high
dimensional global optimization problems with a time-series optimal control
model.","['cs.LG', 'math.OC', 'stat.AP', 'stat.ME']",213,119
A Graph-based Interactive Reasoning for Human-Object Interaction Detection,"Human-Object Interaction (HOI) detection devotes to learn how humans interact
with surrounding objects via inferring triplets of < human, verb, object >.
However, recent HOI detection methods mostly rely on additional annotations
(e.g., human pose) and neglect powerful interactive reasoning beyond
convolutions. In this paper, we present a novel graph-based interactive
reasoning model called Interactive Graph (abbr. in-Graph) to infer HOIs, in
which interactive semantics implied among visual targets are efficiently
exploited. The proposed model consists of a project function that maps related
targets from convolution space to a graph-based semantic space, a message
passing process propagating semantics among all nodes and an update function
transforming the reasoned nodes back to convolution space. Furthermore, we
construct a new framework to assemble in-Graph models for detecting HOIs,
namely in-GraphNet. Beyond inferring HOIs using instance features respectively,
the framework dynamically parses pairwise interactive semantics among visual
targets by integrating two-level in-Graphs, i.e., scene-wide and instance-wide
in-Graphs. Our framework is end-to-end trainable and free from costly
annotations like human pose. Extensive experiments show that our proposed
framework outperforms existing HOI detection methods on both V-COCO and
HICO-DET benchmarks and improves the baseline about 9.4% and 15% relatively,
validating its efficacy in detecting HOIs.",['cs.CV'],216,144
Learning Nonsymmetric Determinantal Point Processes,"Determinantal point processes (DPPs) have attracted substantial attention as
an elegant probabilistic model that captures the balance between quality and
diversity within sets. DPPs are conventionally parameterized by a positive
semi-definite kernel matrix, and this symmetric kernel encodes only repulsive
interactions between items. These so-called symmetric DPPs have significant
expressive power, and have been successfully applied to a variety of machine
learning tasks, including recommendation systems, information retrieval, and
automatic summarization, among many others. Efficient algorithms for learning
symmetric DPPs and sampling from these models have been reasonably well
studied. However, relatively little attention has been given to nonsymmetric
DPPs, which relax the symmetric constraint on the kernel. Nonsymmetric DPPs
allow for both repulsive and attractive item interactions, which can
significantly improve modeling power, resulting in a model that may better fit
for some applications. We present a method that enables a tractable algorithm,
based on maximum likelihood estimation, for learning nonsymmetric DPPs from
data composed of observed subsets. Our method imposes a particular
decomposition of the nonsymmetric kernel that enables such tractable learning
algorithms, which we analyze both theoretically and experimentally. We evaluate
our model on synthetic and real-world datasets, demonstrating improved
predictive performance compared to symmetric DPPs, which have previously shown
strong performance on modeling tasks associated with these datasets.","['cs.LG', 'stat.ML']",215,140
Fingerprint Policy Optimisation for Robust Reinforcement Learning,"Policy gradient methods ignore the potential value of adjusting environment
variables: unobservable state features that are randomly determined by the
environment in a physical setting, but are controllable in a simulator. This
can lead to slow learning, or convergence to suboptimal policies, if the
environment variable has a large impact on the transition dynamics. In this
paper, we present fingerprint policy optimisation (FPO), which finds a policy
that is optimal in expectation across the distribution of environment
variables. The central idea is to use Bayesian optimisation (BO) to actively
select the distribution of the environment variable that maximises the
improvement generated by each iteration of the policy gradient method. To make
this BO practical, we contribute two easy-to-compute low-dimensional
fingerprints of the current policy. Our experiments show that FPO can
efficiently learn policies that are robust to significant rare events, which
are unlikely to be observable under random sampling, but are key to learning
good policies.","['cs.LG', 'cs.AI', 'stat.ML']",159,102
Joint Object Detection and Multi-Object Tracking with Graph Neural Networks,"Object detection and data association are critical components in multi-object
tracking (MOT) systems. Despite the fact that the two components are dependent
on each other, prior works often design detection and data association modules
separately which are trained with separate objectives. As a result, one cannot
back-propagate the gradients and optimize the entire MOT system, which leads to
sub-optimal performance. To address this issue, recent works simultaneously
optimize detection and data association modules under a joint MOT framework,
which has shown improved performance in both modules. In this work, we propose
a new instance of joint MOT approach based on Graph Neural Networks (GNNs). The
key idea is that GNNs can model relations between variable-sized objects in
both the spatial and temporal domains, which is essential for learning
discriminative features for detection and data association. Through extensive
experiments on the MOT15/16/17/20 datasets, we demonstrate the effectiveness of
our GNN-based joint MOT approach and show state-of-the-art performance for both
detection and MOT tasks. Our code is available at:
https://github.com/yongxinw/GSDT","['cs.CV', 'cs.LG', 'cs.MA', 'cs.RO']",183,120
Pre-trained Models for Sonar Images,"Machine learning and neural networks are now ubiquitous in sonar perception,
but it lags behind the computer vision field due to the lack of data and
pre-trained models specifically for sonar images. In this paper we present the
Marine Debris Turntable dataset and produce pre-trained neural networks trained
on this dataset, meant to fill the gap of missing pre-trained models for sonar
images. We train Resnet 20, MobileNets, DenseNet121, SqueezeNet, MiniXception,
and an Autoencoder, over several input image sizes, from 32 x 32 to 96 x 96, on
the Marine Debris turntable dataset. We evaluate these models using transfer
learning for low-shot classification in the Marine Debris Watertank and another
dataset captured using a Gemini 720i sonar. Our results show that in both
datasets the pre-trained models produce good features that allow good
classification accuracy with low samples (10-30 samples per class). The Gemini
dataset validates that the features transfer to other kinds of sonar sensors.
We expect that the community benefits from the public release of our
pre-trained models and the turntable dataset.","['cs.CV', 'cs.LG', 'cs.RO']",181,105
On the Importance of Attention in Meta-Learning for Few-Shot Text Classification,"Current deep learning based text classification methods are limited by their
ability to achieve fast learning and generalization when the data is scarce. We
address this problem by integrating a meta-learning procedure that uses the
knowledge learned across many tasks as an inductive bias towards better natural
language understanding. Based on the Model-Agnostic Meta-Learning framework
(MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML)
algorithm for text classification. The essential difference between MAML and
ATAML is in the separation of task-agnostic representation learning and
task-specific attentive adaptation. The proposed ATAML is designed to encourage
task-agnostic representation learning by way of task-agnostic parameterization
and facilitate task-specific adaptation via attention mechanisms. We provide
evidence to show that the attention mechanism in ATAML has a synergistic effect
on learning performance. In comparisons with models trained from random
initialization, pretrained models and meta trained MAML, our proposed ATAML
method generalizes better on single-label and multi-label classification tasks
in miniRCV1 and miniReuters-21578 datasets.","['cs.LG', 'cs.AI', 'stat.ML']",170,110
Distribution-Specific Agnostic Boosting,"We consider the problem of boosting the accuracy of weak learning algorithms
in the agnostic learning framework of Haussler (1992) and Kearns et al. (1992).
Known algorithms for this problem (Ben-David et al., 2001; Gavinsky, 2002;
Kalai et al., 2008) follow the same strategy as boosting algorithms in the PAC
model: the weak learner is executed on the same target function but over
different distributions on the domain. We demonstrate boosting algorithms for
the agnostic learning framework that only modify the distribution on the labels
of the points (or, equivalently, modify the target function). This allows
boosting a distribution-specific weak agnostic learner to a strong agnostic
learner with respect to the same distribution.
  When applied to the weak agnostic parity learning algorithm of Goldreich and
Levin (1989) our algorithm yields a simple PAC learning algorithm for DNF and
an agnostic learning algorithm for decision trees over the uniform distribution
using membership queries. These results substantially simplify Jackson's famous
DNF learning algorithm (1994) and the recent result of Gopalan et al. (2008).
  We also strengthen the connection to hard-core set constructions discovered
by Klivans and Servedio (1999) by demonstrating that hard-core set
constructions that achieve the optimal hard-core set size (given by Holenstein
(2005) and Barak et al. (2009)) imply distribution-specific agnostic boosting
algorithms. Conversely, our boosting algorithm gives a simple hard-core set
construction with an (almost) optimal hard-core set size.","['cs.LG', 'cs.CC']",239,118
Text to Image Synthesis Using Generative Adversarial Networks,"Generating images from natural language is one of the primary applications of
recent conditional generative models. Besides testing our ability to model
conditional, highly dimensional distributions, text to image synthesis has many
exciting and practical applications such as photo editing or computer-aided
content creation. Recent progress has been made using Generative Adversarial
Networks (GANs). This material starts with a gentle introduction to these
topics and discusses the existent state of the art models. Moreover, I propose
Wasserstein GAN-CLS, a new model for conditional image generation based on the
Wasserstein distance which offers guarantees of stability. Then, I show how the
novel loss function of Wasserstein GAN-CLS can be used in a Conditional
Progressive Growing GAN. In combination with the proposed loss, the model
boosts by 7.07% the best Inception Score (on the Caltech birds dataset) of the
models which use only the sentence-level visual semantics. The only model which
performs better than the Conditional Wasserstein Progressive Growing GAN is the
recently proposed AttnGAN which uses word-level visual semantics as well.","['cs.CV', 'cs.CL']",176,120
Broaden Your Views for Self-Supervised Video Learning,"Most successful self-supervised learning methods are trained to align the
representations of two independent views from the data. State-of-the-art
methods in video are inspired by image techniques, where these two views are
similarly extracted by cropping and augmenting the resulting crop. However,
these methods miss a crucial element in the video domain: time. We introduce
BraVe, a self-supervised learning framework for video. In BraVe, one of the
views has access to a narrow temporal window of the video while the other view
has a broad access to the video content. Our models learn to generalise from
the narrow view to the general content of the video. Furthermore, BraVe
processes the views with different backbones, enabling the use of alternative
augmentations or modalities into the broad view such as optical flow, randomly
convolved RGB frames, audio or their combinations. We demonstrate that BraVe
achieves state-of-the-art results in self-supervised representation learning on
standard video and audio classification benchmarks including UCF101, HMDB51,
Kinetics, ESC-50 and AudioSet.",['cs.CV'],173,104
A maximum principle argument for the uniform convergence of graph Laplacian regressors,"This paper investigates the use of methods from partial differential
equations and the Calculus of variations to study learning problems that are
regularized using graph Laplacians. Graph Laplacians are a powerful, flexible
method for capturing local and global geometry in many classes of learning
problems, and the techniques developed in this paper help to broaden the
methodology of studying such problems. In particular, we develop the use of
maximum principle arguments to establish asymptotic consistency guarantees
within the context of noise corrupted, non-parametric regression with samples
living on an unknown manifold embedded in $\mathbb{R}^d$. The maximum principle
arguments provide a new technical tool which informs parameter selection by
giving concrete error estimates in terms of various regularization parameters.
A review of learning algorithms which utilize graph Laplacians, as well as
previous developments in the use of differential equation and variational
techniques to study those algorithms, is given. In addition, new connections
are drawn between Laplacian methods and other machine learning techniques, such
as kernel regression and k-nearest neighbor methods.","['stat.ML', 'cs.LG', 'math.AP', 'math.ST', 'stat.TH']",173,116
Domain Adaptation for Visual Applications: A Comprehensive Survey,"The aim of this paper is to give an overview of domain adaptation and
transfer learning with a specific view on visual applications. After a general
motivation, we first position domain adaptation in the larger transfer learning
problem. Second, we try to address and analyze briefly the state-of-the-art
methods for different types of scenarios, first describing the historical
shallow methods, addressing both the homogeneous and the heterogeneous domain
adaptation methods. Third, we discuss the effect of the success of deep
convolutional architectures which led to new type of domain adaptation methods
that integrate the adaptation within the deep architecture. Fourth, we overview
the methods that go beyond image categorization, such as object detection or
image segmentation, video analyses or learning visual attributes. Finally, we
conclude the paper with a section where we relate domain adaptation to other
machine learning solutions.",['cs.CV'],143,88
Robust Image Captioning,"Automated captioning of photos is a mission that incorporates the
difficulties of photo analysis and text generation. One essential feature of
captioning is the concept of attention: how to determine what to specify and in
which sequence. In this study, we leverage the Object Relation using
adversarial robust cut algorithm, that grows upon this method by specifically
embedding knowledge about the spatial association between input data through
graph representation. Our experimental study represent the promising
performance of our proposed method for image captioning.",['cs.CV'],83,66
"A local geometry of hyperedges in hypergraphs, and its applications to social networks","In many real world datasets arising from social networks, there are hidden
higher order relations among data points which cannot be captured using graph
modeling. It is natural to use a more general notion of hypergraphs to model
such social networks. In this paper, we introduce a new local geometry of
hyperdges in hypergraphs which allows to capture higher order relations among
data points. Furthermore based on this new geometry, we also introduce new
methodology--the nearest neighbors method in hypergraphs--for analyzing
datasets arising from sociology.","['cs.LG', 'cs.SI', 'stat.AP']",87,61
End-to-end Lane Shape Prediction with Transformers,"Lane detection, the process of identifying lane markings as approximated
curves, is widely used for lane departure warning and adaptive cruise control
in autonomous vehicles. The popular pipeline that solves it in two steps --
feature extraction plus post-processing, while useful, is too inefficient and
flawed in learning the global context and lanes' long and thin structures. To
tackle these issues, we propose an end-to-end method that directly outputs
parameters of a lane shape model, using a network built with a transformer to
learn richer structures and context. The lane shape model is formulated based
on road structures and camera pose, providing physical interpretation for
parameters of network output. The transformer models non-local interactions
with a self-attention mechanism to capture slender structures and global
context. The proposed method is validated on the TuSimple benchmark and shows
state-of-the-art accuracy with the most lightweight model size and fastest
speed. Additionally, our method shows excellent adaptability to a challenging
self-collected lane detection dataset, showing its powerful deployment
potential in real applications. Codes are available at
https://github.com/liuruijin17/LSTR.","['cs.CV', 'cs.AI']",185,127
Primer: Searching for Efficient Transformers for Language Modeling,"Large Transformer models have been central to recent advances in natural
language processing. The training and inference costs of these models, however,
have grown rapidly and become prohibitively expensive. Here we aim to reduce
the costs of Transformers by searching for a more efficient variant. Compared
to previous approaches, our search is performed at a lower level, over the
primitives that define a Transformer TensorFlow program. We identify an
architecture, named Primer, that has a smaller training cost than the original
Transformer and other variants for auto-regressive language modeling. Primer's
improvements can be mostly attributed to two simple modifications: squaring
ReLU activations and adding a depthwise convolution layer after each Q, K, and
V projection in self-attention.
  Experiments show Primer's gains over Transformer increase as compute scale
grows and follow a power law with respect to quality at optimal model sizes. We
also verify empirically that Primer can be dropped into different codebases to
significantly speed up training without additional tuning. For example, at a
500M parameter size, Primer improves the original T5 architecture on C4
auto-regressive language modeling, reducing the training cost by 4X.
Furthermore, the reduced training cost means Primer needs much less compute to
reach a target one-shot performance. For instance, in a 1.9B parameter
configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to
achieve the same one-shot performance as Transformer. We open source our models
and several comparisons in T5 to help with reproducibility.","['cs.LG', 'cs.AI', 'cs.CL', 'cs.NE']",252,164
Mitigate Bias in Face Recognition using Skewness-Aware Reinforcement Learning,"Racial equality is an important theme of international human rights law, but
it has been largely obscured when the overall face recognition accuracy is
pursued blindly. More facts indicate racial bias indeed degrades the fairness
of recognition system and the error rates on non-Caucasians are usually much
higher than Caucasians. To encourage fairness, we introduce the idea of
adaptive margin to learn balanced performance for different races based on
large margin losses. A reinforcement learning based race balance network
(RL-RBN) is proposed. We formulate the process of finding the optimal margins
for non-Caucasians as a Markov decision process and employ deep Q-learning to
learn policies for an agent to select appropriate margin by approximating the
Q-value function. Guided by the agent, the skewness of feature scatter between
races can be reduced. Besides, we provide two ethnicity aware training
datasets, called BUPT-Globalface and BUPT-Balancedface dataset, which can be
utilized to study racial bias from both data and algorithm aspects. Extensive
experiments on RFW database show that RL-RBN successfully mitigates racial bias
and learns more balanced performance for different races.",['cs.CV'],186,130
Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction,"Time series prediction has been studied in a variety of domains. However, it
is still challenging to predict future series given historical observations and
past exogenous data. Existing methods either fail to consider the interactions
among different components of exogenous variables which may affect the
prediction accuracy, or cannot model the correlations between exogenous data
and target data. Besides, the inherent temporal dynamics of exogenous data are
also related to the target series prediction, and thus should be considered as
well. To address these issues, we propose an end-to-end deep learning model,
i.e., Hierarchical attention-based Recurrent Highway Network (HRHN), which
incorporates spatio-temporal feature extraction of exogenous variables and
temporal dynamics modeling of target variables into a single framework.
Moreover, by introducing the hierarchical attention mechanism, HRHN can
adaptively select the relevant exogenous features in different semantic levels.
We carry out comprehensive empirical evaluations with various methods over
several datasets, and show that HRHN outperforms the state of the arts in time
series prediction, especially in capturing sudden changes and sudden
oscillations of time series.","['cs.LG', 'cs.CV', 'stat.ML']",179,121
JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D Point Clouds,"Semantic segmentation and semantic edge detection can be seen as two dual
problems with close relationships in computer vision. Despite the fast
evolution of learning-based 3D semantic segmentation methods, little attention
has been drawn to the learning of 3D semantic edge detectors, even less to a
joint learning method for the two tasks. In this paper, we tackle the 3D
semantic edge detection task for the first time and present a new two-stream
fully-convolutional network that jointly performs the two tasks. In particular,
we design a joint refinement module that explicitly wires region information
and edge information to improve the performances of both tasks. Further, we
propose a novel loss function that encourages the network to produce semantic
segmentation results with better boundaries. Extensive evaluations on S3DIS and
ScanNet datasets show that our method achieves on par or better performance
than the state-of-the-art methods for semantic segmentation and outperforms the
baseline methods for semantic edge detection. Code release:
https://github.com/hzykent/JSENet",['cs.CV'],169,104
You Only Need Adversarial Supervision for Semantic Image Synthesis,"Despite their recent successes, GAN models for semantic image synthesis still
suffer from poor image quality when trained with only adversarial supervision.
Historically, additionally employing the VGG-based perceptual loss has helped
to overcome this issue, significantly improving the synthesis quality, but at
the same time limiting the progress of GAN models for semantic image synthesis.
In this work, we propose a novel, simplified GAN model, which needs only
adversarial supervision to achieve high quality results. We re-design the
discriminator as a semantic segmentation network, directly using the given
semantic label maps as the ground truth for training. By providing stronger
supervision to the discriminator as well as to the generator through spatially-
and semantically-aware discriminator feedback, we are able to synthesize images
of higher fidelity with better alignment to their input label maps, making the
use of the perceptual loss superfluous. Moreover, we enable high-quality
multi-modal image synthesis through global and local sampling of a 3D noise
tensor injected into the generator, which allows complete or partial image
change. We show that images synthesized by our model are more diverse and
follow the color and texture distributions of real images more closely. We
achieve an average improvement of $6$ FID and $5$ mIoU points over the state of
the art across different datasets using only adversarial supervision.","['cs.CV', 'cs.LG', 'eess.IV']",222,139
Fast and Accurate 3D Medical Image Segmentation with Data-swapping Method,"Deep neural network models used for medical image segmentation are large
because they are trained with high-resolution three-dimensional (3D) images.
Graphics processing units (GPUs) are widely used to accelerate the trainings.
However, the memory on a GPU is not large enough to train the models. A popular
approach to tackling this problem is patch-based method, which divides a large
image into small patches and trains the models with these small patches.
However, this method would degrade the segmentation quality if a target object
spans multiple patches. In this paper, we propose a novel approach for 3D
medical image segmentation that utilizes the data-swapping, which swaps out
intermediate data from GPU memory to CPU memory to enlarge the effective GPU
memory size, for training high-resolution 3D medical images without patching.
We carefully tuned parameters in the data-swapping method to obtain the best
training performance for 3D U-Net, a widely used deep neural network model for
medical image segmentation. We applied our tuning to train 3D U-Net with
full-size images of 192 x 192 x 192 voxels in brain tumor dataset. As a result,
communication overhead, which is the most important issue, was reduced by
17.1%. Compared with the patch-based method for patches of 128 x 128 x 128
voxels, our training for full-size images achieved improvement on the mean Dice
score by 4.48% and 5.32 % for detecting whole tumor sub-region and tumor core
sub-region, respectively. The total training time was reduced from 164 hours to
47 hours, resulting in 3.53 times of acceleration.","['cs.LG', 'cs.CV', 'cs.PF', 'stat.ML', 'C.4; I.2.6; I.2.10; I.4.6; I.4.9; J.4']",269,147
Accelerating GMM-based patch priors for image restoration: Three ingredients for a 100$\times$ speed-up,"Image restoration methods aim to recover the underlying clean image from
corrupted observations. The Expected Patch Log-likelihood (EPLL) algorithm is a
powerful image restoration method that uses a Gaussian mixture model (GMM)
prior on the patches of natural images. Although it is very effective for
restoring images, its high runtime complexity makes EPLL ill-suited for most
practical applications. In this paper, we propose three approximations to the
original EPLL algorithm. The resulting algorithm, which we call the fast-EPLL
(FEPLL), attains a dramatic speed-up of two orders of magnitude over EPLL while
incurring a negligible drop in the restored image quality (less than 0.5 dB).
We demonstrate the efficacy and versatility of our algorithm on a number of
inverse problems such as denoising, deblurring, super-resolution, inpainting
and devignetting. To the best of our knowledge, FEPLL is the first algorithm
that can competitively restore a 512x512 pixel image in under 0.5s for all the
degradations mentioned above without specialized code optimizations such as CPU
parallelization or GPU implementation.",['cs.CV'],173,126
Bayesian Attention Belief Networks,"Attention-based neural networks have achieved state-of-the-art results on a
wide range of tasks. Most such models use deterministic attention while
stochastic attention is less explored due to the optimization difficulties or
complicated model design. This paper introduces Bayesian attention belief
networks, which construct a decoder network by modeling unnormalized attention
weights with a hierarchy of gamma distributions, and an encoder network by
stacking Weibull distributions with a deterministic-upward-stochastic-downward
structure to approximate the posterior. The resulting auto-encoding networks
can be optimized in a differentiable way with a variational lower bound. It is
simple to convert any models with deterministic attention, including pretrained
ones, to the proposed Bayesian attention belief networks. On a variety of
language understanding tasks, we show that our method outperforms deterministic
attention and state-of-the-art stochastic attention in accuracy, uncertainty
estimation, generalization across domains, and robustness to adversarial
attacks. We further demonstrate the general applicability of our method on
neural machine translation and visual question answering, showing great
potential of incorporating our method into various attention-related tasks.","['cs.LG', 'cs.CL', 'stat.ML']",180,119
Structured Multi-Hashing for Model Compression,"Despite the success of deep neural networks (DNNs), state-of-the-art models
are too large to deploy on low-resource devices or common server configurations
in which multiple models are held in memory. Model compression methods address
this limitation by reducing the memory footprint, latency, or energy
consumption of a model with minimal impact on accuracy. We focus on the task of
reducing the number of learnable variables in the model. In this work we
combine ideas from weight hashing and dimensionality reductions resulting in a
simple and powerful structured multi-hashing method based on matrix products
that allows direct control of model size of any deep network and is trained
end-to-end. We demonstrate the strength of our approach by compressing models
from the ResNet, EfficientNet, and MobileNet architecture families. Our method
allows us to drastically decrease the number of variables while maintaining
high accuracy. For instance, by applying our approach to EfficentNet-B4 (16M
parameters) we reduce it to to the size of B0 (5M parameters), while gaining
over 3% in accuracy over B0 baseline. On the commonly used benchmark CIFAR10 we
reduce the ResNet32 model by 75% with no loss in quality, and are able to do a
10x compression while still achieving above 90% accuracy.","['cs.LG', 'cs.CV', 'stat.ML']",211,132
Transfer Learning of Graph Neural Networks with Ego-graph Information Maximization,"Graph neural networks (GNNs) have been shown with superior performance in
various applications, but training dedicated GNNs can be costly for large-scale
graphs. Some recent work started to study the pre-training of GNNs. However,
none of them provide theoretical insights into the design of their frameworks,
or clear requirements and guarantees towards the transferability of GNNs. In
this work, we establish a theoretically grounded and practically useful
framework for the transfer learning of GNNs. Firstly, we propose a novel view
towards the essential graph information and advocate the capturing of it as the
goal of transferable GNN training, which motivates the design of Ours, a novel
GNN framework based on ego-graph information maximization to analytically
achieve this goal. Secondly, we specify the requirement of structure-respecting
node features as the GNN input, and derive a rigorous bound of GNN
transferability based on the difference between the local graph Laplacians of
the source and target graphs. Finally, we conduct controlled synthetic
experiments to directly justify our theoretical conclusions. Extensive
experiments on real-world networks towards role identification show consistent
results in the rigorously analyzed setting of direct-transfering, while those
towards large-scale relation prediction show promising results in the more
generalized and practical setting of transfering with fine-tuning.","['cs.LG', 'stat.ML']",212,133
DSP-SLAM: Object Oriented SLAM with Deep Shape Priors,"We propose DSP-SLAM, an object-oriented SLAM system that builds a rich and
accurate joint map of dense 3D models for foreground objects, and sparse
landmark points to represent the background. DSP-SLAM takes as input the 3D
point cloud reconstructed by a feature-based SLAM system and equips it with the
ability to enhance its sparse map with dense reconstructions of detected
objects. Objects are detected via semantic instance segmentation, and their
shape and pose is estimated using category-specific deep shape embeddings as
priors, via a novel second order optimization. Our object-aware bundle
adjustment builds a pose-graph to jointly optimize camera poses, object
locations and feature points. DSP-SLAM can operate at 10 frames per second on 3
different input modalities: monocular, stereo, or stereo+LiDAR. We demonstrate
DSP-SLAM operating at almost frame rate on monocular-RGB sequences from the
Friburg and Redwood-OS datasets, and on stereo+LiDAR sequences on the KITTI
odometry dataset showing that it achieves high-quality full object
reconstructions, even from partial observations, while maintaining a consistent
global map. Our evaluation shows improvements in object pose and shape
reconstruction with respect to recent deep prior-based reconstruction methods
and reductions in camera tracking drift on the KITTI dataset.","['cs.CV', 'cs.RO']",209,131
Fully Convolutional Multi-scale Residual DenseNets for Cardiac Segmentation and Automated Cardiac Diagnosis using Ensemble of Classifiers,"Deep fully convolutional neural network (FCN) based architectures have shown
great potential in medical image segmentation. However, such architectures
usually have millions of parameters and inadequate number of training samples
leading to over-fitting and poor generalization. In this paper, we present a
novel highly parameter and memory efficient FCN based architecture for medical
image analysis. We propose a novel up-sampling path which incorporates long
skip and short-cut connections to overcome the feature map explosion in FCN
like architectures. In order to processes the input images at multiple scales
and view points simultaneously, we propose to incorporate Inception module's
parallel structures. We also propose a novel dual loss function whose weighting
scheme allows to combine advantages of cross-entropy and dice loss. We have
validated our proposed network architecture on two publicly available datasets,
namely: (i) Automated Cardiac Disease Diagnosis Challenge (ACDC-2017), (ii)
Left Ventricular Segmentation Challenge (LV-2011). Our approach in ACDC-2017
challenge stands second place for segmentation and first place in automated
cardiac disease diagnosis tasks with an accuracy of 100%. In the LV-2011
challenge our approach attained 0.74 Jaccard index, which is so far the highest
published result in fully automated algorithms. From the segmentation we
extracted clinically relevant cardiac parameters and hand-crafted features
which reflected the clinical diagnostic analysis to train an ensemble system
for cardiac disease classification. Our approach combined both cardiac
segmentation and disease diagnosis into a fully automated framework which is
computational efficient and hence has the potential to be incorporated in
computer-aided diagnosis (CAD) tools for clinical application.",['cs.CV'],265,173
Monocular Camera Localization in Prior LiDAR Maps with 2D-3D Line Correspondences,"Light-weight camera localization in existing maps is essential for
vision-based navigation. Currently, visual and visual-inertial odometry
(VO\&VIO) techniques are well-developed for state estimation but with
inevitable accumulated drifts and pose jumps upon loop closure. To overcome
these problems, we propose an efficient monocular camera localization method in
prior LiDAR maps using direct 2D-3D line correspondences. To handle the
appearance differences and modality gaps between LiDAR point clouds and images,
geometric 3D lines are extracted offline from LiDAR maps while robust 2D lines
are extracted online from video sequences. With the pose prediction from VIO,
we can efficiently obtain coarse 2D-3D line correspondences. Then the camera
poses and 2D-3D correspondences are iteratively optimized by minimizing the
projection error of correspondences and rejecting outliers. Experimental
results on the EurocMav dataset and our collected dataset demonstrate that the
proposed method can efficiently estimate camera poses without accumulated
drifts or pose jumps in structured environments.",['cs.CV'],159,106
Combining Supervised and Un-supervised Learning for Automatic Citrus Segmentation,"Citrus segmentation is a key step of automatic citrus picking. While most
current image segmentation approaches achieve good segmentation results by
pixel-wise segmentation, these supervised learning-based methods require a
large amount of annotated data, and do not consider the continuous temporal
changes of citrus position in real-world applications. In this paper, we first
train a simple CNN with a small number of labelled citrus images in a
supervised manner, which can roughly predict the citrus location from each
frame. Then, we extend a state-of-the-art unsupervised learning approach to
pre-learn the citrus's potential movements between frames from unlabelled
citrus's videos. To take advantages of both networks, we employ the multimodal
transformer to combine supervised learned static information and unsupervised
learned movement information. The experimental results show that combing both
network allows the prediction accuracy reached at 88.3$\%$ IOU and 93.6$\%$
precision, outperforming the original supervised baseline 1.2$\%$ and 2.4$\%$.
Compared with most of the existing citrus segmentation methods, our method uses
a small amount of supervised data and a large number of unsupervised data,
while learning the pixel level location information and the temporal
information of citrus changes to enhance the segmentation effect.",['cs.CV'],205,127
Convolutional Neural Network (CNN) vs Visual Transformer (ViT) for Digital Holography,"In Digital Holography (DH), it is crucial to extract the object distance from
a hologram in order to reconstruct its amplitude and phase. This step is called
auto-focusing and it is conventionally solved by first reconstructing a stack
of images and then by sharpening each reconstructed image using a focus metric
such as entropy or variance. The distance corresponding to the sharpest image
is considered the focal position. This approach, while effective, is
computationally demanding and time-consuming. In this paper, the determination
of the distance is performed by Deep Learning (DL). Two deep learning (DL)
architectures are compared: Convolutional Neural Network (CNN)and Visual
transformer (ViT). ViT and CNN are used to cope with the problem of
auto-focusing as a classification problem. Compared to a first attempt [11] in
which the distance between two consecutive classes was 100{\mu}m, our proposal
allows us to drastically reduce this distance to 1{\mu}m. Moreover, ViT reaches
similar accuracy and is more robust than CNN.","['cs.CV', 'eess.IV']",167,112
Interpretable and Generalizable Person Re-Identification with Query-Adaptive Convolution and Temporal Lifting,"For person re-identification, existing deep networks often focus on
representation learning. However, without transfer learning, the learned model
is fixed as is, which is not adaptable for handling various unseen scenarios.
In this paper, beyond representation learning, we consider how to formulate
person image matching directly in deep feature maps. We treat image matching as
finding local correspondences in feature maps, and construct query-adaptive
convolution kernels on the fly to achieve local matching. In this way, the
matching process and results are interpretable, and this explicit matching is
more generalizable than representation features to unseen scenarios, such as
unknown misalignments, pose or viewpoint changes. To facilitate end-to-end
training of this architecture, we further build a class memory module to cache
feature maps of the most recent samples of each class, so as to compute image
matching losses for metric learning. Through direct cross-dataset evaluation,
the proposed Query-Adaptive Convolution (QAConv) method gains large
improvements over popular learning methods (about 10%+ mAP), and achieves
comparable results to many transfer learning methods. Besides, a model-free
temporal cooccurrence based score weighting method called TLift is proposed,
which improves the performance to a further extent, achieving state-of-the-art
results in cross-dataset person re-identification. Code is available at
https://github.com/ShengcaiLiao/QAConv.",['cs.CV'],218,140
Planning in Dynamic Environments with Conditional Autoregressive Models,"We demonstrate the use of conditional autoregressive generative models (van
den Oord et al., 2016a) over a discrete latent space (van den Oord et al.,
2017b) for forward planning with MCTS. In order to test this method, we
introduce a new environment featuring varying difficulty levels, along with
moving goals and obstacles. The combination of high-quality frame generation
and classical planning approaches nearly matches true environment performance
for our task, demonstrating the usefulness of this method for model-based
planning in dynamic environments.","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",84,66
Learning Motion Patterns in Videos,"The problem of determining whether an object is in motion, irrespective of
camera motion, is far from being solved. We address this challenging task by
learning motion patterns in videos. The core of our approach is a fully
convolutional network, which is learned entirely from synthetic video
sequences, and their ground-truth optical flow and motion segmentation. This
encoder-decoder style architecture first learns a coarse representation of the
optical flow field features, and then refines it iteratively to produce motion
labels at the original high-resolution. We further improve this labeling with
an objectness map and a conditional random field, to account for errors in
optical flow, and also to focus on moving ""things"" rather than ""stuff"". The
output label of each pixel denotes whether it has undergone independent motion,
i.e., irrespective of camera motion. We demonstrate the benefits of this
learning framework on the moving object segmentation task, where the goal is to
segment all objects in motion. Our approach outperforms the top method on the
recently released DAVIS benchmark dataset, comprising real-world sequences, by
5.6%. We also evaluate on the Berkeley motion segmentation database, achieving
state-of-the-art results.",['cs.CV'],196,126
Graph Pruning for Model Compression,"Previous AutoML pruning works utilized individual layer features to
automatically prune filters. We analyze the correlation for two layers from
different blocks which have a short-cut structure. It is found that, in one
block, the deeper layer has many redundant filters which can be represented by
filters in the former layer so that it is necessary to take information from
other layers into consideration in pruning. In this paper, a graph pruning
approach is proposed, which views any deep model as a topology graph. Graph
PruningNet based on the graph convolution network is designed to automatically
extract neighboring information for each node. To extract features from various
topologies, Graph PruningNet is connected with Pruned Network by an individual
fully connection layer for each node and jointly trained on a training dataset
from scratch. Thus, we can obtain reasonable weights for any size of
sub-network. We then search the best configuration of the Pruned Network by
reinforcement learning. Different from previous work, we take the node features
from well-trained Graph PruningNet, instead of the hand-craft features, as the
states in reinforcement learning. Compared with other AutoML pruning works, our
method has achieved the state-of-the-art under same conditions on
ImageNet-2012. The code will be released on GitHub.",['cs.CV'],213,129
Saliency guided deep network for weakly-supervised image segmentation,"Weakly-supervised image segmentation is an important task in computer vision.
A key problem is how to obtain high quality objects location from image-level
category. Classification activation mapping is a common method which can be
used to generate high-precise object location cues. However these location cues
are generally very sparse and small such that they can not provide effective
information for image segmentation. In this paper, we propose a saliency guided
image segmentation network to resolve this problem. We employ a self-attention
saliency method to generate subtle saliency maps, and render the location cues
grow as seeds by seeded region growing method to expand pixel-level labels
extent. In the process of seeds growing, we use the saliency values to weight
the similarity between pixels to control the growing. Therefore saliency
information could help generate discriminative object regions, and the effects
of wrong salient pixels can be suppressed efficiently. Experimental results on
a common segmentation dataset PASCAL VOC2012 demonstrate the effectiveness of
our method.",['cs.CV'],167,110
Probability Link Models with Symmetric Information Divergence,"This paper introduces link functions for transforming one probability
distribution to another such that the Kullback-Leibler and R\'enyi divergences
between the two distributions are symmetric. Two general classes of link models
are proposed. The first model links two survival functions and is applicable to
models such as the proportional odds and change point, which are used in
survival analysis and reliability modeling. A prototype application involving
the proportional odds model demonstrates advantages of symmetric divergence
measures over asymmetric measures for assessing the efficacy of features and
for model averaging purposes. The advantages include providing unique ranks for
models and unique information weights for model averaging with one-half as much
computation requirement of asymmetric divergences. The second model links two
cumulative probability distribution functions. This model produces a
generalized location model which are continuous counterparts of the binary
probability models such as probit and logit models. Examples include the
generalized probit and logit models which have appeared in the survival
analysis literature, and a generalized Laplace model and a generalized
Student-$t$ model, which are survival time models corresponding to the
respective binary probability models. Lastly, extensions to symmetric
divergence between survival functions and conditions for copula dependence
information are presented.","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']",203,103
Analog Circuit Design with Dyna-Style Reinforcement Learning,"In this work, we present a learning based approach to analog circuit design,
where the goal is to optimize circuit performance subject to certain design
constraints. One of the aspects that makes this problem challenging to
optimize, is that measuring the performance of candidate configurations with
simulation can be computationally expensive, particularly in the post-layout
design. Additionally, the large number of design constraints and the
interaction between the relevant quantities makes the problem complex.
Therefore, to better facilitate supporting the human designers, it is desirable
to gain knowledge about the whole space of feasible solutions. In order to
tackle these challenges, we take inspiration from model-based reinforcement
learning and propose a method with two key properties. First, it learns a
reward model, i.e., surrogate model of the performance approximated by neural
networks, to reduce the required number of simulation. Second, it uses a
stochastic policy generator to explore the diverse solution space satisfying
constraints. Together we combine these in a Dyna-style optimization framework,
which we call DynaOpt, and empirically evaluate the performance on a circuit
benchmark of a two-stage operational amplifier. The results show that, compared
to the model-free method applied with 20,000 circuit simulations to train the
policy, DynaOpt achieves even much better performance by learning from scratch
with only 500 simulations.",['cs.LG'],220,133
Gradient Play in Multi-Agent Markov Stochastic Games: Stationary Points and Convergence,"We study the performance of the gradient play algorithm for multi-agent
tabular Markov decision processes (MDPs), which are also known as stochastic
games (SGs), where each agent tries to maximize its own total discounted reward
by making decisions independently based on current state information which is
shared between agents. Policies are directly parameterized by the probability
of choosing a certain action at a given state. We show that Nash equilibria
(NEs) and first order stationary policies are equivalent in this setting, and
give a non-asymptotic global convergence rate analysis to an $\epsilon$-NE for
a subclass of multi-agent MDPs called Markov potential games, which includes
the cooperative setting with identical rewards among agents as an important
special case. Our result shows that the number of iterations to reach an
$\epsilon$-NE scales linearly, instead of exponentially, with the number of
agents. Local geometry and local stability are also considered. For Markov
potential games, we prove that strict NEs are local maxima of the total
potential function and fully-mixed NEs are saddle points. We also give a local
convergence rate around strict NEs for more general settings.","['cs.LG', 'cs.GT', 'cs.MA', 'math.OC']",190,122
Queue-Learning: A Reinforcement Learning Approach for Providing Quality of Service,"End-to-end delay is a critical attribute of quality of service (QoS) in
application domains such as cloud computing and computer networks. This metric
is particularly important in tandem service systems, where the end-to-end
service is provided through a chain of services. Service-rate control is a
common mechanism for providing QoS guarantees in service systems. In this
paper, we introduce a reinforcement learning-based (RL-based) service-rate
controller that provides probabilistic upper-bounds on the end-to-end delay of
the system, while preventing the overuse of service resources. In order to have
a general framework, we use queueing theory to model the service systems.
However, we adopt an RL-based approach to avoid the limitations of
queueing-theoretic methods. In particular, we use Deep Deterministic Policy
Gradient (DDPG) to learn the service rates (action) as a function of the queue
lengths (state) in tandem service systems. In contrast to existing RL-based
methods that quantify their performance by the achieved overall reward, which
could be hard to interpret or even misleading, our proposed controller provides
explicit probabilistic guarantees on the end-to-end delay of the system. The
evaluations are presented for a tandem queueing system with non-exponential
inter-arrival and service times, the results of which validate our controller's
capability in meeting QoS constraints.","['cs.LG', 'cs.AI', 'cs.NI', 'cs.PF']",223,128
Focus on defocus: bridging the synthetic to real domain gap for depth estimation,"Data-driven depth estimation methods struggle with the generalization outside
their training scenes due to the immense variability of the real-world scenes.
This problem can be partially addressed by utilising synthetically generated
images, but closing the synthetic-real domain gap is far from trivial. In this
paper, we tackle this issue by using domain invariant defocus blur as direct
supervision. We leverage defocus cues by using a permutation invariant
convolutional neural network that encourages the network to learn from the
differences between images with a different point of focus. Our proposed
network uses the defocus map as an intermediate supervisory signal. We are able
to train our model completely on synthetic data and directly apply it to a wide
range of real-world images. We evaluate our model on synthetic and real
datasets, showing compelling generalization results and state-of-the-art depth
prediction.",['cs.CV'],145,99
Deep Quality-Value (DQV) Learning,"We introduce a novel Deep Reinforcement Learning (DRL) algorithm called Deep
Quality-Value (DQV) Learning. DQV uses temporal-difference learning to train a
Value neural network and uses this network for training a second Quality-value
network that learns to estimate state-action values. We first test DQV's update
rules with Multilayer Perceptrons as function approximators on two classic RL
problems, and then extend DQV with the use of Deep Convolutional Neural
Networks, `Experience Replay' and `Target Neural Networks' for tackling four
games of the Atari Arcade Learning environment. Our results show that DQV
learns significantly faster and better than Deep Q-Learning and Double Deep
Q-Learning, suggesting that our algorithm can potentially be a better
performing synchronous temporal difference algorithm than what is currently
present in DRL.","['stat.ML', 'cs.LG']",130,87
Multitask learning and benchmarking with clinical time series data,"Health care is one of the most exciting frontiers in data mining and machine
learning. Successful adoption of electronic health records (EHRs) created an
explosion in digital clinical data available for analysis, but progress in
machine learning for healthcare research has been difficult to measure because
of the absence of publicly available benchmark data sets. To address this
problem, we propose four clinical prediction benchmarks using data derived from
the publicly available Medical Information Mart for Intensive Care (MIMIC-III)
database. These tasks cover a range of clinical problems including modeling
risk of mortality, forecasting length of stay, detecting physiologic decline,
and phenotype classification. We propose strong linear and neural baselines for
all four tasks and evaluate the effect of deep supervision, multitask training
and data-specific architectural modifications on the performance of neural
models.","['stat.ML', 'cs.LG']",135,99
Self-Attentional Credit Assignment for Transfer in Reinforcement Learning,"The ability to transfer knowledge to novel environments and tasks is a
sensible desiderata for general learning agents. Despite the apparent promises,
transfer in RL is still an open and little exploited research area. In this
paper, we take a brand-new perspective about transfer: we suggest that the
ability to assign credit unveils structural invariants in the tasks that can be
transferred to make RL more sample-efficient. Our main contribution is SECRET,
a novel approach to transfer learning for RL that uses a backward-view credit
assignment mechanism based on a self-attentive architecture. Two aspects are
key to its generality: it learns to assign credit as a separate offline
supervised process and exclusively modifies the reward function. Consequently,
it can be supplemented by transfer methods that do not modify the reward
function and it can be plugged on top of any RL algorithm.","['cs.LG', 'cs.AI']",146,97
Rethinking the Discount Factor in Reinforcement Learning: A Decision Theoretic Approach,"Reinforcement learning (RL) agents have traditionally been tasked with
maximizing the value function of a Markov decision process (MDP), either in
continuous settings, with fixed discount factor $\gamma < 1$, or in episodic
settings, with $\gamma = 1$. While this has proven effective for specific tasks
with well-defined objectives (e.g., games), it has never been established that
fixed discounting is suitable for general purpose use (e.g., as a model of
human preferences). This paper characterizes rationality in sequential decision
making using a set of seven axioms and arrives at a form of discounting that
generalizes traditional fixed discounting. In particular, our framework admits
a state-action dependent ""discount"" factor that is not constrained to be less
than 1, so long as there is eventual long run discounting. Although this
broadens the range of possible preference structures in continuous settings, we
show that there exists a unique ""optimizing MDP"" with fixed $\gamma < 1$ whose
optimal value function matches the true utility of the optimal policy, and we
quantify the difference between value and utility for suboptimal policies. Our
work can be seen as providing a normative justification for (a slight
generalization of) Martha White's RL task formalism (2017) and other recent
departures from the traditional RL, and is relevant to task specification in
RL, inverse RL and preference-based RL.","['cs.LG', 'cs.AI']",221,138
Recursive-NeRF: An Efficient and Dynamically Growing NeRF,"View synthesis methods using implicit continuous shape representations
learned from a set of images, such as the Neural Radiance Field (NeRF) method,
have gained increasing attention due to their high quality imagery and
scalability to high resolution. However, the heavy computation required by its
volumetric approach prevents NeRF from being useful in practice; minutes are
taken to render a single image of a few megapixels. Now, an image of a scene
can be rendered in a level-of-detail manner, so we posit that a complicated
region of the scene should be represented by a large neural network while a
small neural network is capable of encoding a simple region, enabling a balance
between efficiency and quality. Recursive-NeRF is our embodiment of this idea,
providing an efficient and adaptive rendering and training approach for NeRF.
The core of Recursive-NeRF learns uncertainties for query coordinates,
representing the quality of the predicted color and volumetric intensity at
each level. Only query coordinates with high uncertainties are forwarded to the
next level to a bigger neural network with a more powerful representational
capability. The final rendered image is a composition of results from neural
networks of all levels. Our evaluation on three public datasets shows that
Recursive-NeRF is more efficient than NeRF while providing state-of-the-art
quality. The code will be available at https://github.com/Gword/Recursive-NeRF.",['cs.CV'],231,141
Deep Reinforcement Learning for Autonomous Driving,"Reinforcement learning has steadily improved and outperform human in lots of
traditional games since the resurgence of deep neural network. However, these
success is not easy to be copied to autonomous driving because the state spaces
in real world are extreme complex and action spaces are continuous and fine
control is required. Moreover, the autonomous driving vehicles must also keep
functional safety under the complex environments. To deal with these
challenges, we first adopt the deep deterministic policy gradient (DDPG)
algorithm, which has the capacity to handle complex state and action spaces in
continuous domain. We then choose The Open Racing Car Simulator (TORCS) as our
environment to avoid physical damage. Meanwhile, we select a set of appropriate
sensor information from TORCS and design our own rewarder. In order to fit DDPG
algorithm to TORCS, we design our network architecture for both actor and
critic inside DDPG paradigm. To demonstrate the effectiveness of our model, We
evaluate on different modes in TORCS and show both quantitative and qualitative
results.","['cs.CV', 'cs.LG', 'cs.RO']",169,115
TOAD-GAN: Coherent Style Level Generation from a Single Example,"In this work, we present TOAD-GAN (Token-based One-shot Arbitrary Dimension
Generative Adversarial Network), a novel Procedural Content Generation (PCG)
algorithm that generates token-based video game levels. TOAD-GAN follows the
SinGAN architecture and can be trained using only one example. We demonstrate
its application for Super Mario Bros. levels and are able to generate new
levels of similar style in arbitrary sizes. We achieve state-of-the-art results
in modeling the patterns of the training level and provide a comparison with
different baselines under several metrics. Additionally, we present an
extension of the method that allows the user to control the generation process
of certain token structures to ensure a coherent global level layout. We
provide this tool to the community to spur further research by publishing our
source code.","['cs.LG', 'cs.NE', 'stat.ML']",135,101
Distributional Gaussian Process Layers for Outlier Detection in Image Segmentation,"We propose a parameter efficient Bayesian layer for hierarchical
convolutional Gaussian Processes that incorporates Gaussian Processes operating
in Wasserstein-2 space to reliably propagate uncertainty. This directly
replaces convolving Gaussian Processes with a distance-preserving affine
operator on distributions. Our experiments on brain tissue-segmentation show
that the resulting architecture approaches the performance of well-established
deterministic segmentation algorithms (U-Net), which has never been achieved
with previous hierarchical Gaussian Processes. Moreover, by applying the same
segmentation model to out-of-distribution data (i.e., images with pathology
such as brain tumors), we show that our uncertainty estimates result in
out-of-distribution detection that outperforms the capabilities of previous
Bayesian networks and reconstruction-based approaches that learn normative
distributions.","['stat.ML', 'cs.LG']",120,86
Stacked Denoising Autoencoders and Transfer Learning for Immunogold Particles Detection and Recognition,"In this paper we present a system for the detection of immunogold particles
and a Transfer Learning (TL) framework for the recognition of these immunogold
particles. Immunogold particles are part of a high-magnification method for the
selective localization of biological molecules at the subcellular level only
visible through Electron Microscopy. The number of immunogold particles in the
cell walls allows the assessment of the differences in their compositions
providing a tool to analise the quality of different plants. For its
quantization one requires a laborious manual labeling (or annotation) of images
containing hundreds of particles. The system that is proposed in this paper can
leverage significantly the burden of this manual task.
  For particle detection we use a LoG filter coupled with a SDA. In order to
improve the recognition, we also study the applicability of TL settings for
immunogold recognition. TL reuses the learning model of a source problem on
other datasets (target problems) containing particles of different sizes. The
proposed system was developed to solve a particular problem on maize cells,
namely to determine the composition of cell wall ingrowths in endosperm
transfer cells. This novel dataset as well as the code for reproducing our
experiments is made publicly available.
  We determined that the LoG detector alone attained more than 84\% of accuracy
with the F-measure. Developing immunogold recognition with TL also provided
superior performance when compared with the baseline models augmenting the
accuracy rates by 10\%.",['cs.CV'],241,148
Sampling Strategies for GAN Synthetic Data,"Generative Adversarial Networks (GANs) have been used widely to generate
large volumes of synthetic data. This data is being utilized for augmenting
with real examples in order to train deep Convolutional Neural Networks (CNNs).
Studies have shown that the generated examples lack sufficient realism to train
deep CNNs and are poor in diversity. Unlike previous studies of randomly
augmenting the synthetic data with real data, we present our simple, effective
and easy to implement synthetic data sampling methods to train deep CNNs more
efficiently and accurately. To this end, we propose to maximally utilize the
parameters learned during training of the GAN itself. These include
discriminator's realism confidence score and the confidence on the target label
of the synthetic data. In addition to this, we explore reinforcement learning
(RL) to automatically search a subset of meaningful synthetic examples from a
large pool of GAN synthetic data. We evaluate our method on two challenging
face attribute classification data sets viz. AffectNet and CelebA. Our
extensive experiments clearly demonstrate the need of sampling synthetic data
before augmentation, which also improves the performance of one of the
state-of-the-art deep CNNs in vitro.",['cs.CV'],193,121
GraphMix: Improved Training of GNNs for Semi-Supervised Learning,"We present GraphMix, a regularization method for Graph Neural Network based
semi-supervised object classification, whereby we propose to train a
fully-connected network jointly with the graph neural network via parameter
sharing and interpolation-based regularization. Further, we provide a
theoretical analysis of how GraphMix improves the generalization bounds of the
underlying graph neural network, without making any assumptions about the
""aggregation"" layer or the depth of the graph neural networks. We
experimentally validate this analysis by applying GraphMix to various
architectures such as Graph Convolutional Networks, Graph Attention Networks
and Graph-U-Net. Despite its simplicity, we demonstrate that GraphMix can
consistently improve or closely match state-of-the-art performance using even
simpler architectures such as Graph Convolutional Networks, across three
established graph benchmarks: Cora, Citeseer and Pubmed citation network
datasets, as well as three newly proposed datasets: Cora-Full, Co-author-CS and
Co-author-Physics.","['cs.LG', 'stat.ML']",150,100
SSRNet: Scalable 3D Surface Reconstruction Network,"Existing learning-based surface reconstruction methods from point clouds are
still facing challenges in terms of scalability and preservation of details on
large-scale point clouds. In this paper, we propose the SSRNet, a novel
scalable learning-based method for surface reconstruction. The proposed SSRNet
constructs local geometry-aware features for octree vertices and designs a
scalable reconstruction pipeline, which not only greatly enhances the
predication accuracy of the relative position between the vertices and the
implicit surface facilitating the surface reconstruction quality, but also
allows dividing the point cloud and octree vertices and processing different
parts in parallel for superior scalability on large-scale point clouds with
millions of points. Moreover, SSRNet demonstrates outstanding generalization
capability and only needs several surface data for training, much less than
other learning-based reconstruction methods, which can effectively avoid
overfitting. The trained model of SSRNet on one dataset can be directly used on
other datasets with superior performance. Finally, the time consumption with
SSRNet on a large-scale point cloud is acceptable and competitive. To our
knowledge, the proposed SSRNet is the first to really bring a convincing
solution to the scalability issue of the learning-based surface reconstruction
methods, and is an important step to make learning-based methods competitive
with respect to geometry processing methods on real-world and challenging data.
Experiments show that our method achieves a breakthrough in scalability and
quality compared with state-of-the-art learning-based methods.",['cs.CV'],242,130
Towards Out-Of-Distribution Generalization: A Survey,"Classic machine learning methods are built on the $i.i.d.$ assumption that
training and testing data are independent and identically distributed. However,
in real scenarios, the $i.i.d.$ assumption can hardly be satisfied, rendering
the sharp drop of classic machine learning algorithms' performances under
distributional shifts, which indicates the significance of investigating the
Out-of-Distribution generalization problem. Out-of-Distribution (OOD)
generalization problem addresses the challenging setting where the testing
distribution is unknown and different from the training. This paper serves as
the first effort to systematically and comprehensively discuss the OOD
generalization problem, from the definition, methodology, evaluation to the
implications and future directions. Firstly, we provide the formal definition
of the OOD generalization problem. Secondly, existing methods are categorized
into three parts based on their positions in the whole learning pipeline,
namely unsupervised representation learning, supervised model learning and
optimization, and typical methods for each category are discussed in detail. We
then demonstrate the theoretical connections of different categories, and
introduce the commonly used datasets and evaluation metrics. Finally, we
summarize the whole literature and raise some future directions for OOD
generalization problem. The summary of OOD generalization methods reviewed in
this survey can be found at http://out-of-distribution-generalization.com.",['cs.LG'],208,124
Neighbor-Vote: Improving Monocular 3D Object Detection through Neighbor Distance Voting,"As cameras are increasingly deployed in new application domains such as
autonomous driving, performing 3D object detection on monocular images becomes
an important task for visual scene understanding. Recent advances on monocular
3D object detection mainly rely on the ``pseudo-LiDAR'' generation, which
performs monocular depth estimation and lifts the 2D pixels to pseudo 3D
points. However, depth estimation from monocular images, due to its poor
accuracy, leads to inevitable position shift of pseudo-LiDAR points within the
object. Therefore, the predicted bounding boxes may suffer from inaccurate
location and deformed shape. In this paper, we present a novel neighbor-voting
method that incorporates neighbor predictions to ameliorate object detection
from severely deformed pseudo-LiDAR point clouds. Specifically, each feature
point around the object forms their own predictions, and then the ``consensus''
is achieved through voting. In this way, we can effectively combine the
neighbors' predictions with local prediction and achieve more accurate 3D
detection. To further enlarge the difference between the foreground region of
interest (ROI) pseudo-LiDAR points and the background points, we also encode
the ROI prediction scores of 2D foreground pixels into the corresponding
pseudo-LiDAR points. We conduct extensive experiments on the KITTI benchmark to
validate the merits of our proposed method. Our results on the bird's eye view
detection outperform the state-of-the-art performance by a large margin,
especially for the ``hard'' level detection.",['cs.CV'],233,149
Distributed Deep Transfer Learning by Basic Probability Assignment,"Transfer learning is a popular practice in deep neural networks, but
fine-tuning of large number of parameters is a hard task due to the complex
wiring of neurons between splitting layers and imbalance distributions of data
in pretrained and transferred domains. The reconstruction of the original
wiring for the target domain is a heavy burden due to the size of
interconnections across neurons. We propose a distributed scheme that tunes the
convolutional filters individually while backpropagates them jointly by means
of basic probability assignment. Some of the most recent advances in evidence
theory show that in a vast variety of the imbalanced regimes, optimizing of
some proper objective functions derived from contingency matrices prevents
biases towards high-prior class distributions. Therefore, the original filters
get gradually transferred based on individual contributions to overall
performance of the target domain. This largely reduces the expected complexity
of transfer learning whilst highly improves precision. Our experiments on
standard benchmarks and scenarios confirm the consistent improvement of our
distributed deep transfer learning strategy.","['cs.LG', 'stat.ML']",170,119
Sparse Coding with Fast Image Alignment via Large Displacement Optical Flow,"Sparse representation-based classifiers have shown outstanding accuracy and
robustness in image classification tasks even with the presence of intense
noise and occlusion. However, it has been discovered that the performance
degrades significantly either when test image is not aligned with the
dictionary atoms or the dictionary atoms themselves are not aligned with each
other, in which cases the sparse linear representation assumption fails. In
this paper, having both training and test images misaligned, we introduce a
novel sparse coding framework that is able to efficiently adapt the dictionary
atoms to the test image via large displacement optical flow. In the proposed
algorithm, every dictionary atom is automatically aligned with the input image
and the sparse code is then recovered using the adapted dictionary atoms. A
corresponding supervised dictionary learning algorithm is also developed for
the proposed framework. Experimental results on digit datasets recognition
verify the efficacy and robustness of the proposed algorithm.",['cs.CV'],153,99
Context-Aware Group Captioning via Self-Attention and Contrastive Features,"While image captioning has progressed rapidly, existing works focus mainly on
describing single images. In this paper, we introduce a new task, context-aware
group captioning, which aims to describe a group of target images in the
context of another group of related reference images. Context-aware group
captioning requires not only summarizing information from both the target and
reference image group but also contrasting between them. To solve this problem,
we propose a framework combining self-attention mechanism with contrastive
feature construction to effectively summarize common information from each
image group while capturing discriminative information between them. To build
the dataset for this task, we propose to group the images and generate the
group captions based on single image captions using scene graphs matching. Our
datasets are constructed on top of the public Conceptual Captions dataset and
our new Stock Captions dataset. Experiments on the two datasets show the
effectiveness of our method on this new task. Related Datasets and code are
released at https://lizw14.github.io/project/groupcap .",['cs.CV'],171,106
SGD Learns One-Layer Networks in WGANs,"Generative adversarial networks (GANs) are a widely used framework for
learning generative models. Wasserstein GANs (WGANs), one of the most
successful variants of GANs, require solving a minmax optimization problem to
global optimality, but are in practice successfully trained using stochastic
gradient descent-ascent. In this paper, we show that, when the generator is a
one-layer network, stochastic gradient descent-ascent converges to a global
solution with polynomial time and sample complexity.","['cs.LG', 'stat.ML']",73,58
Crowd Flow Segmentation in Compressed Domain using CRF,"Crowd flow segmentation is an important step in many video surveillance
tasks. In this work, we propose an algorithm for segmenting flows in H.264
compressed videos in a completely unsupervised manner. Our algorithm works on
motion vectors which can be obtained by partially decoding the compressed video
without extracting any additional features. Our approach is based on modelling
the motion vector field as a Conditional Random Field (CRF) and obtaining
oriented motion segments by finding the optimal labelling which minimises the
global energy of CRF. These oriented motion segments are recursively merged
based on gradient across their boundaries to obtain the final flow segments.
This work in compressed domain can be easily extended to pixel domain by
substituting motion vectors with motion based features like optical flow. The
proposed algorithm is experimentally evaluated on a standard crowd flow dataset
and its superior performance in both accuracy and computational time are
demonstrated through quantitative results.",['cs.CV'],155,106
Multiple shooting with neural differential equations,"Neural differential equations have recently emerged as a flexible
data-driven/hybrid approach to model time-series data. This work experimentally
demonstrates that if the data contains oscillations, then standard fitting of a
neural differential equation may give flattened out trajectory that fails to
describe the data. We then introduce the multiple shooting method and present
successful demonstrations of this method for the fitting of a neural
differential equation to two datasets (synthetic and experimental) that the
standard approach fails to fit. Constraints introduced by multiple shooting can
be satisfied using a penalty or augmented Lagrangian method.",['cs.LG'],97,66
Learning Global and Local Consistent Representations for Unsupervised Image Retrieval via Deep Graph Diffusion Networks,"Diffusion has shown great success in improving accuracy of unsupervised image
retrieval systems by utilizing high-order structures of image manifold.
However, existing diffusion methods suffer from three major limitations: 1)
they usually rely on local structures without considering global manifold
information; 2) they focus on improving pair-wise similarities within existing
images input output transductively while lacking flexibility to learn
representations for novel unseen instances inductively; 3) they fail to scale
to large datasets due to prohibitive memory consumption and computational
burden due to intrinsic high-order operations on the whole graph. In this
paper, to address these limitations, we propose a novel method, Graph Diffusion
Networks (GRAD-Net), that adopts graph neural networks (GNNs), a novel variant
of deep learning algorithms on irregular graphs. GRAD-Net learns semantic
representations by exploiting both local and global structures of image
manifold in an unsupervised fashion. By utilizing sparse coding techniques,
GRAD-Net not only preserves global information on the image manifold, but also
enables scalable training and efficient querying. Experiments on several large
benchmark datasets demonstrate effectiveness of our method over
state-of-the-art diffusion algorithms for unsupervised image retrieval.",['cs.CV'],191,129
Offline Multi-Action Policy Learning: Generalization and Optimization,"In many settings, a decision-maker wishes to learn a rule, or policy, that
maps from observable characteristics of an individual to an action. Examples
include selecting offers, prices, advertisements, or emails to send to
consumers, as well as the problem of determining which medication to prescribe
to a patient. While there is a growing body of literature devoted to this
problem, most existing results are focused on the case where data comes from a
randomized experiment, and further, there are only two possible actions, such
as giving a drug to a patient or not. In this paper, we study the offline
multi-action policy learning problem with observational data and where the
policy may need to respect budget constraints or belong to a restricted policy
class such as decision trees. We build on the theory of efficient
semi-parametric inference in order to propose and implement a policy learning
algorithm that achieves asymptotically minimax-optimal regret. To the best of
our knowledge, this is the first result of this type in the multi-action setup,
and it provides a substantial performance improvement over the existing
learning algorithms. We then consider additional computational challenges that
arise in implementing our method for the case where the policy is restricted to
take the form of a decision tree. We propose two different approaches, one
using a mixed integer program formulation and the other using a tree-search
based algorithm.","['stat.ML', 'cs.LG', 'econ.EM']",237,141
A3T-GCN: Attention Temporal Graph Convolutional Network for Traffic Forecasting,"Accurate real-time traffic forecasting is a core technological problem
against the implementation of the intelligent transportation system. However,
it remains challenging considering the complex spatial and temporal
dependencies among traffic flows. In the spatial dimension, due to the
connectivity of the road network, the traffic flows between linked roads are
closely related. In terms of the temporal factor, although there exists a
tendency among adjacent time points in general, the importance of distant past
points is not necessarily smaller than that of recent past points since traffic
flows are also affected by external factors. In this study, an attention
temporal graph convolutional network (A3T-GCN) traffic forecasting method was
proposed to simultaneously capture global temporal dynamics and spatial
correlations. The A3T-GCN model learns the short-time trend in time series by
using the gated recurrent units and learns the spatial dependence based on the
topology of the road network through the graph convolutional network. Moreover,
the attention mechanism was introduced to adjust the importance of different
time points and assemble global temporal information to improve prediction
accuracy. Experimental results in real-world datasets demonstrate the
effectiveness and robustness of proposed A3T-GCN. The source code can be
visited at https://github.com/lehaifeng/T-GCN/A3T.","['cs.LG', 'stat.ML']",208,126
Deep learning for source camera identification on mobile devices,"In the present paper, we propose a source camera identification method for
mobile devices based on deep learning. Recently, convolutional neural networks
(CNNs) have shown a remarkable performance on several tasks such as image
recognition, video analysis or natural language processing. A CNN consists on a
set of layers where each layer is composed by a set of high pass filters which
are applied all over the input image. This convolution process provides the
unique ability to extract features automatically from data and to learn from
those features. Our proposal describes a CNN architecture which is able to
infer the noise pattern of mobile camera sensors (also known as camera
fingerprint) with the aim at detecting and identifying not only the mobile
device used to capture an image (with a 98\% of accuracy), but also from which
embedded camera the image was captured. More specifically, we provide an
extensive analysis on the proposed architecture considering different
configurations. The experiment has been carried out using the images captured
from different mobile devices cameras (MICHE-I Dataset was used) and the
obtained results have proved the robustness of the proposed method.",['cs.CV'],189,128
Extended Vertical Lists for Temporal Pattern Mining from Multivariate Time Series,"Temporal Pattern Mining (TPM) is the problem of mining predictive complex
temporal patterns from multivariate time series in a supervised setting. We
develop a new method called the Fast Temporal Pattern Mining with Extended
Vertical Lists. This method utilizes an extension of the Apriori property which
requires a more complex pattern to appear within records only at places where
all of its subpatterns are detected as well. The approach is based on a novel
data structure called the Extended Vertical List that tracks positions of the
first state of the pattern inside records. Extensive computational results
indicate that the new method performs significantly faster than the previous
version of the algorithm for TMP. However, the speed-up comes at the expense of
memory usage.","['cs.LG', 'stat.ML']",124,90
Signal Transformer: Complex-valued Attention and Meta-Learning for Signal Recognition,"Deep neural networks have been shown as a class of useful tools for
addressing signal recognition issues in recent years, especially for
identifying the nonlinear feature structures of signals. However, this power of
most deep learning techniques heavily relies on an abundant amount of training
data, so the performance of classic neural nets decreases sharply when the
number of training data samples is small or unseen data are presented in the
testing phase. This calls for an advanced strategy, i.e., model-agnostic
meta-learning (MAML), which is able to capture the invariant representation of
the data samples or signals. In this paper, inspired by the special structure
of the signal, i.e., real and imaginary parts consisted in practical
time-series signals, we propose a Complex-valued Attentional MEta Learner
(CAMEL) for the problem of few-shot signal recognition by leveraging attention
and meta-learning in the complex domain. To the best of our knowledge, this is
also the first complex-valued MAML that can find the first-order stationary
points of general nonconvex problems with theoretical convergence guarantees.
Extensive experiments results showcase the superiority of the proposed CAMEL
compared with the state-of-the-art methods.","['cs.LG', 'cs.AI', 'eess.SP']",198,133
Icon: An Interactive Approach to Train Deep Neural Networks for Segmentation of Neuronal Structures,"We present an interactive approach to train a deep neural network pixel
classifier for the segmentation of neuronal structures. An interactive training
scheme reduces the extremely tedious manual annotation task that is typically
required for deep networks to perform well on image segmentation problems. Our
proposed method employs a feedback loop that captures sparse annotations using
a graphical user interface, trains a deep neural network based on recent and
past annotations, and displays the prediction output to users in almost
real-time. Our implementation of the algorithm also allows multiple users to
provide annotations in parallel and receive feedback from the same classifier.
Quick feedback on classifier performance in an interactive setting enables
users to identify and label examples that are more important than others for
segmentation purposes. Our experiments show that an interactively-trained pixel
classifier produces better region segmentation results on Electron Microscopy
(EM) images than those generated by a network of the same architecture trained
offline on exhaustive ground-truth labels.",['cs.CV'],164,108
Pareto Self-Supervised Training for Few-Shot Learning,"While few-shot learning (FSL) aims for rapid generalization to new concepts
with little supervision, self-supervised learning (SSL) constructs supervisory
signals directly computed from unlabeled data. Exploiting the complementarity
of these two manners, few-shot auxiliary learning has recently drawn much
attention to deal with few labeled data. Previous works benefit from sharing
inductive bias between the main task (FSL) and auxiliary tasks (SSL), where the
shared parameters of tasks are optimized by minimizing a linear combination of
task losses. However, it is challenging to select a proper weight to balance
tasks and reduce task conflict. To handle the problem as a whole, we propose a
novel approach named as Pareto self-supervised training (PSST) for FSL. PSST
explicitly decomposes the few-shot auxiliary problem into multiple constrained
multi-objective subproblems with different trade-off preferences, and here a
preference region in which the main task achieves the best performance is
identified. Then, an effective preferred Pareto exploration is proposed to find
a set of optimal solutions in such a preference region. Extensive experiments
on several public benchmark datasets validate the effectiveness of our approach
by achieving state-of-the-art performance.",['cs.CV'],193,131
Variational Hyper RNN for Sequence Modeling,"In this work, we propose a novel probabilistic sequence model that excels at
capturing high variability in time series data, both across sequences and
within an individual sequence. Our method uses temporal latent variables to
capture information about the underlying data pattern and dynamically decodes
the latent information into modifications of weights of the base decoder and
recurrent model. The efficacy of the proposed method is demonstrated on a range
of synthetic and real-world sequential data that exhibit large scale
variations, regime shifts, and complex dynamics.","['cs.LG', 'stat.ML']",87,68
Auto-GNN: Neural Architecture Search of Graph Neural Networks,"Graph neural networks (GNN) has been successfully applied to operate on the
graph-structured data. Given a specific scenario, rich human expertise and
tremendous laborious trials are usually required to identify a suitable GNN
architecture. It is because the performance of a GNN architecture is
significantly affected by the choice of graph convolution components, such as
aggregate function and hidden dimension. Neural architecture search (NAS) has
shown its potential in discovering effective deep architectures for learning
tasks in image and language modeling. However, existing NAS algorithms cannot
be directly applied to the GNN search problem. First, the search space of GNN
is different from the ones in existing NAS work. Second, the representation
learning capacity of GNN architecture changes obviously with slight
architecture modifications. It affects the search efficiency of traditional
search methods. Third, widely used techniques in NAS such as parameter sharing
might become unstable in GNN.
  To bridge the gap, we propose the automated graph neural networks (AGNN)
framework, which aims to find an optimal GNN architecture within a predefined
search space. A reinforcement learning based controller is designed to greedily
validate architectures via small steps. AGNN has a novel parameter sharing
strategy that enables homogeneous architectures to share parameters, based on a
carefully-designed homogeneity definition. Experiments on real-world benchmark
datasets demonstrate that the GNN architecture identified by AGNN achieves the
best performance, comparing with existing handcrafted models and tradistional
search methods.","['cs.LG', 'stat.ML']",236,148
Single Image Super-Resolution with Dilated Convolution based Multi-Scale Information Learning Inception Module,"Traditional works have shown that patches in a natural image tend to
redundantly recur many times inside the image, both within the same scale, as
well as across different scales. Make full use of these multi-scale information
can improve the image restoration performance. However, the current proposed
deep learning based restoration methods do not take the multi-scale information
into account. In this paper, we propose a dilated convolution based inception
module to learn multi-scale information and design a deep network for single
image super-resolution. Different dilated convolution learns different scale
feature, then the inception module concatenates all these features to fuse
multi-scale information. In order to increase the reception field of our
network to catch more contextual information, we cascade multiple inception
modules to constitute a deep network to conduct single image super-resolution.
With the novel dilated convolution based inception module, the proposed
end-to-end single image super-resolution network can take advantage of
multi-scale information to improve image super-resolution performance.
Experimental results show that our proposed method outperforms many
state-of-the-art single image super-resolution methods.",['cs.CV'],188,99
Visual Evaluation of Generative Adversarial Networks for Time Series Data,"A crucial factor to trust Machine Learning (ML) algorithm decisions is a good
representation of its application field by the training dataset. This is
particularly true when parts of the training data have been artificially
generated to overcome common training problems such as lack of data or
imbalanced dataset. Over the last few years, Generative Adversarial Networks
(GANs) have shown remarkable results in generating realistic data. However,
this ML approach lacks an objective function to evaluate the quality of the
generated data. Numerous GAN applications focus on generating image data mostly
because they can be easily evaluated by a human eye. Less efforts have been
made to generate time series data. Assessing their quality is more complicated,
particularly for technical data. In this paper, we propose a human-centered
approach supporting a ML or domain expert to accomplish this task using Visual
Analytics (VA) techniques. The presented approach consists of two views, namely
a GAN Iteration View showing similarity metrics between real and generated data
over the iterations of the generation process and a Detailed Comparative View
equipped with different time series visualizations such as TimeHistograms, to
compare the generated data at different iteration steps. Starting from the GAN
Iteration View, the user can choose suitable iteration steps for detailed
inspection. We evaluate our approach with a usage scenario that enabled an
efficient comparison of two different GAN models.","['cs.LG', 'cs.HC', 'eess.IV']",229,150
Gaussian Error Linear Units (GELUs),"We propose the Gaussian Error Linear Unit (GELU), a high-performing neural
network activation function. The GELU activation function is $x\Phi(x)$, where
$\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU
nonlinearity weights inputs by their value, rather than gates inputs by their
sign as in ReLUs ($x\mathbf{1}_{x>0}$). We perform an empirical evaluation of
the GELU nonlinearity against the ReLU and ELU activations and find performance
improvements across all considered computer vision, natural language
processing, and speech tasks.",['cs.LG'],86,63
Separation and Concentration in Deep Networks,"Numerical experiments demonstrate that deep neural network classifiers
progressively separate class distributions around their mean, achieving linear
separability on the training set, and increasing the Fisher discriminant ratio.
We explain this mechanism with two types of operators. We prove that a
rectifier without biases applied to sign-invariant tight frames can separate
class means and increase Fisher ratios. On the opposite, a soft-thresholding on
tight frames can reduce within-class variabilities while preserving class
means. Variance reduction bounds are proved for Gaussian mixture models. For
image classification, we show that separation of class means can be achieved
with rectified wavelet tight frames that are not learned. It defines a
scattering transform. Learning $1 \times 1$ convolutional tight frames along
scattering channels and applying a soft-thresholding reduces within-class
variabilities. The resulting scattering network reaches the classification
accuracy of ResNet-18 on CIFAR-10 and ImageNet, with fewer layers and no
learned biases.","['cs.LG', 'cs.CV']",154,105
Lifelong Object Detection,"Recent advances in object detection have benefited significantly from rapid
developments in deep neural networks. However, neural networks suffer from the
well-known issue of catastrophic forgetting, which makes continual or lifelong
learning problematic. In this paper, we leverage the fact that new training
classes arrive in a sequential manner and incrementally refine the model so
that it additionally detects new object classes in the absence of previous
training data. Specifically, we consider the representative object detector,
Faster R-CNN, for both accurate and efficient prediction. To prevent abrupt
performance degradation due to catastrophic forgetting, we propose to apply
knowledge distillation on both the region proposal network and the region
classification network, to retain the detection of previously trained classes.
A pseudo-positive-aware sampling strategy is also introduced for distillation
sample selection. We evaluate the proposed method on PASCAL VOC 2007 and MS
COCO benchmarks and show competitive mAP and 6x inference speed improvement,
which makes the approach more suitable for real-time applications. Our
implementation will be publicly available.",['cs.CV'],171,126
Differentiable Hierarchical Graph Grouping for Multi-Person Pose Estimation,"Multi-person pose estimation is challenging because it localizes body
keypoints for multiple persons simultaneously. Previous methods can be divided
into two streams, i.e. top-down and bottom-up methods. The top-down methods
localize keypoints after human detection, while the bottom-up methods localize
keypoints directly and then cluster/group them for different persons, which are
generally more efficient than top-down methods. However, in existing bottom-up
methods, the keypoint grouping is usually solved independently from keypoint
detection, making them not end-to-end trainable and have sub-optimal
performance. In this paper, we investigate a new perspective of human part
grouping and reformulate it as a graph clustering task. Especially, we propose
a novel differentiable Hierarchical Graph Grouping (HGG) method to learn the
graph grouping in bottom-up multi-person pose estimation task. Moreover, HGG is
easily embedded into main-stream bottom-up methods. It takes human keypoint
candidates as graph nodes and clusters keypoints in a multi-layer graph neural
network model. The modules of HGG can be trained end-to-end with the keypoint
detection network and is able to supervise the grouping process in a
hierarchical manner. To improve the discrimination of the clustering, we add a
set of edge discriminators and macro-node discriminators. Extensive experiments
on both COCO and OCHuman datasets demonstrate that the proposed method improves
the performance of bottom-up pose estimation methods.",['cs.CV'],234,135
Distance to Center of Mass Encoding for Instance Segmentation,"The instance segmentation can be considered an extension of the object
detection problem where bounding boxes are replaced by object contours.
Strictly speaking the problem requires to identify each pixel instance and
class independently of the artifice used for this mean. The advantage of
instance segmentation over the usual object detection lies in the precise
delineation of objects improving object localization. Additionally, object
contours allow the evaluation of partial occlusion with basic image processing
algorithms. This work approaches the instance segmentation problem as an
annotation problem and presents a novel technique to encode and decode ground
truth annotations. We propose a mathematical representation of instances that
any deep semantic segmentation model can learn and generalize. Each individual
instance is represented by a center of mass and a field of vectors pointing to
it. This encoding technique has been denominated Distance to Center of Mass
Encoding (DCME).",['cs.CV'],146,100
Local Propagation for Few-Shot Learning,"The challenge in few-shot learning is that available data is not enough to
capture the underlying distribution. To mitigate this, two emerging directions
are (a) using local image representations, essentially multiplying the amount
of data by a constant factor, and (b) using more unlabeled data, for instance
by transductive inference, jointly on a number of queries. In this work, we
bring these two ideas together, introducing \emph{local propagation}. We treat
local image features as independent examples, we build a graph on them and we
use it to propagate both the features themselves and the labels, known and
unknown. Interestingly, since there is a number of features per image, even a
single query gives rise to transductive inference. As a result, we provide a
universally safe choice for few-shot inference under both non-transductive and
transductive settings, improving accuracy over corresponding methods. This is
in contrast to existing solutions, where one needs to choose the method
depending on the quantity of available data.",['cs.CV'],165,109
Federated Learning of Molecular Properties in a Heterogeneous Setting,"Chemistry research has both high material and computational costs to conduct
experiments. Institutions thus consider chemical data to be valuable and there
have been few efforts to construct large public datasets for machine learning.
Another challenge is that different intuitions are interested in different
classes of molecules, creating heterogeneous data that cannot be easily joined
by conventional distributed training. In this work, we introduce federated
heterogeneous molecular learning to address these challenges. Federated
learning allows end-users to build a global model collaboratively while
preserving the training data distributed over isolated clients. Due to the lack
of related research, we first simulate a federated heterogeneous benchmark
called FedChem. FedChem is constructed by jointly performing scaffold splitting
and Latent Dirichlet Allocation on existing datasets. Our results on FedChem
show that significant learning challenges arise when working with heterogeneous
molecules. We then propose a method to alleviate the problem, namely Federated
Learning by Instance reweighTing (FLIT). FLIT can align the local training
across heterogeneous clients by improving the performance for uncertain
samples. Comprehensive experiments conducted on our new benchmark FedChem
validate the advantages of this method over other federated learning schemes.
FedChem should enable a new type of collaboration for improving AI in chemistry
that mitigates concerns about valuable chemical data.","['cs.LG', 'physics.chem-ph']",209,139
Frequentist Regret Bounds for Randomized Least-Squares Value Iteration,"We consider the exploration-exploitation dilemma in finite-horizon
reinforcement learning (RL). When the state space is large or continuous,
traditional tabular approaches are unfeasible and some form of function
approximation is mandatory. In this paper, we introduce an
optimistically-initialized variant of the popular randomized least-squares
value iteration (RLSVI), a model-free algorithm where exploration is induced by
perturbing the least-squares approximation of the action-value function. Under
the assumption that the Markov decision process has low-rank transition
dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by
$\widetilde O(d^2 H^2 \sqrt{T})$ where $ d $ are the feature dimension, $ H $
is the horizon, and $ T $ is the total number of steps. To the best of our
knowledge, this is the first frequentist regret analysis for randomized
exploration with function approximation.","['cs.LG', 'stat.ML']",140,92
Instance-based Vision Transformer for Subtyping of Papillary Renal Cell Carcinoma in Histopathological Image,"Histological subtype of papillary (p) renal cell carcinoma (RCC), type 1 vs.
type 2, is an essential prognostic factor. The two subtypes of pRCC have a
similar pattern, i.e., the papillary architecture, yet some subtle differences,
including cellular and cell-layer level patterns. However, the cellular and
cell-layer level patterns almost cannot be captured by existing CNN-based
models in large-size histopathological images, which brings obstacles to
directly applying these models to such a fine-grained classification task. This
paper proposes a novel instance-based Vision Transformer (i-ViT) to learn
robust representations of histopathological images for the pRCC subtyping task
by extracting finer features from instance patches (by cropping around
segmented nuclei and assigning predicted grades). The proposed i-ViT takes
top-K instances as input and aggregates them for capturing both the cellular
and cell-layer level patterns by a position-embedding layer, a grade-embedding
layer, and a multi-head multi-layer self-attention module. To evaluate the
performance of the proposed framework, experienced pathologists are invited to
selected 1162 regions of interest from 171 whole slide images of type 1 and
type 2 pRCC. Experimental results show that the proposed method achieves better
performance than existing CNN-based models with a significant margin.",['cs.CV'],210,137
Time Series Anomaly Detection; Detection of anomalous drops with limited features and sparse examples in noisy highly periodic data,"Google uses continuous streams of data from industry partners in order to
deliver accurate results to users. Unexpected drops in traffic can be an
indication of an underlying issue and may be an early warning that remedial
action may be necessary. Detecting such drops is non-trivial because streams
are variable and noisy, with roughly regular spikes (in many different shapes)
in traffic data. We investigated the question of whether or not we can predict
anomalies in these data streams. Our goal is to utilize Machine Learning and
statistical approaches to classify anomalous drops in periodic, but noisy,
traffic patterns. Since we do not have a large body of labeled examples to
directly apply supervised learning for anomaly classification, we approached
the problem in two parts. First we used TensorFlow to train our various models
including DNNs, RNNs, and LSTMs to perform regression and predict the expected
value in the time series. Secondly we created anomaly detection rules that
compared the actual values to predicted values. Since the problem requires
finding sustained anomalies, rather than just short delays or momentary
inactivity in the data, our two detection methods focused on continuous
sections of activity rather than just single points. We tried multiple
combinations of our models and rules and found that using the intersection of
our two anomaly detection methods proved to be an effective method of detecting
anomalies on almost all of our models. In the process we also found that not
all data fell within our experimental assumptions, as one data stream had no
periodicity, and therefore no time based model could predict it.","['stat.ML', 'cs.LG']",266,163
Joint Demosaicking and Denoising in the Wild: The Case of Training Under Ground Truth Uncertainty,"Image demosaicking and denoising are the two key fundamental steps in digital
camera pipelines, aiming to reconstruct clean color images from noisy luminance
readings. In this paper, we propose and study Wild-JDD, a novel learning
framework for joint demosaicking and denoising in the wild. In contrast to
previous works which generally assume the ground truth of training data is a
perfect reflection of the reality, we consider here the more common imperfect
case of ground truth uncertainty in the wild. We first illustrate its
manifestation as various kinds of artifacts including zipper effect, color
moire and residual noise. Then we formulate a two-stage data degradation
process to capture such ground truth uncertainty, where a conjugate prior
distribution is imposed upon a base distribution. After that, we derive an
evidence lower bound (ELBO) loss to train a neural network that approximates
the parameters of the conjugate prior distribution conditioned on the degraded
input. Finally, to further enhance the performance for out-of-distribution
input, we design a simple but effective fine-tuning strategy by taking the
input as a weakly informative prior. Taking into account ground truth
uncertainty, Wild-JDD enjoys good interpretability during optimization.
Extensive experiments validate that it outperforms state-of-the-art schemes on
joint demosaicking and denoising tasks on both synthetic and realistic raw
datasets.","['cs.CV', 'eess.IV']",220,146
Zero-Shot Grounding of Objects from Natural Language Queries,"A phrase grounding system localizes a particular object in an image referred
to by a natural language query. In previous work, the phrases were restricted
to have nouns that were encountered in training, we extend the task to
Zero-Shot Grounding(ZSG) which can include novel, ""unseen"" nouns. Current
phrase grounding systems use an explicit object detection network in a 2-stage
framework where one stage generates sparse proposals and the other stage
evaluates them. In the ZSG setting, generating appropriate proposals itself
becomes an obstacle as the proposal generator is trained on the entities common
in the detection and grounding datasets. We propose a new single-stage model
called ZSGNet which combines the detector network and the grounding system and
predicts classification scores and regression parameters. Evaluation of ZSG
system brings additional subtleties due to the influence of the relationship
between the query and learned categories; we define four distinct conditions
that incorporate different levels of difficulty. We also introduce new
datasets, sub-sampled from Flickr30k Entities and Visual Genome, that enable
evaluations for the four conditions. Our experiments show that ZSGNet achieves
state-of-the-art performance on Flickr30k and ReferIt under the usual ""seen""
settings and performs significantly better than baseline in the zero-shot
setting.","['cs.CV', 'cs.CL']",209,138
Arena: a toolkit for Multi-Agent Reinforcement Learning,"We introduce Arena, a toolkit for multi-agent reinforcement learning (MARL)
research. In MARL, it usually requires customizing observations, rewards and
actions for each agent, changing cooperative-competitive agent-interaction, and
playing with/against a third-party agent, etc. We provide a novel modular
design, called Interface, for manipulating such routines in essentially two
ways: 1) Different interfaces can be concatenated and combined, which extends
the OpenAI Gym Wrappers concept to MARL scenarios. 2) During MARL training or
testing, interfaces can be embedded in either wrapped OpenAI Gym compatible
Environments or raw environment compatible Agents. We offer off-the-shelf
interfaces for several popular MARL platforms, including StarCraft II,
Pommerman, ViZDoom, Soccer, etc. The interfaces effectively support self-play
RL and cooperative-competitive hybrid MARL. Also, Arena can be conveniently
extended to your own favorite MARL platform.","['cs.LG', 'cs.AI', 'cs.MA']",137,100
SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks,"Single image super resolution is a very important computer vision task, with
a wide range of applications. In recent years, the depth of the
super-resolution model has been constantly increasing, but with a small
increase in performance, it has brought a huge amount of computation and memory
consumption. In this work, in order to make the super resolution models more
effective, we proposed a novel single image super resolution method via
recursive squeeze and excitation networks (SESR). By introducing the squeeze
and excitation module, our SESR can model the interdependencies and
relationships between channels and that makes our model more efficiency. In
addition, the recursive structure and progressive reconstruction method in our
model minimized the layers and parameters and enabled SESR to simultaneously
train multi-scale super resolution in a single model. After evaluating on four
benchmark test sets, our model is proved to be above the state-of-the-art
methods in terms of speed and accuracy.",['cs.CV'],159,98
Fusarium Damaged Kernels Detection Using Transfer Learning on Deep Neural Network Architecture,"The present work shows the application of transfer learning for a pre-trained
deep neural network (DNN), using a small image dataset ($\approx$ 12,000) on a
single workstation with enabled NVIDIA GPU card that takes up to 1 hour to
complete the training task and archive an overall average accuracy of $94.7\%$.
The DNN presents a $20\%$ score of misclassification for an external test
dataset. The accuracy of the proposed methodology is equivalent to ones using
HSI methodology $(81\%-91\%)$ used for the same task, but with the advantage of
being independent on special equipment to classify wheat kernel for FHB
symptoms.","['cs.LG', 'stat.ML']",104,76
Dissecting the Diffusion Process in Linear Graph Convolutional Networks,"Graph Convolutional Networks (GCNs) have attracted more and more attentions
in recent years. A typical GCN layer consists of a linear feature propagation
step and a nonlinear transformation step. Recent works show that a linear GCN
can achieve comparable performance to the original non-linear GCN while being
much more computationally efficient. In this paper, we dissect the feature
propagation steps of linear GCNs from a perspective of continuous graph
diffusion, and analyze why linear GCNs fail to benefit from more propagation
steps. Following that, we propose Decoupled Graph Convolution (DGC) that
decouples the terminal time and the feature propagation steps, making it more
flexible and capable of exploiting a very large number of feature propagation
steps. Experiments demonstrate that our proposed DGC improves linear GCNs by a
large margin and makes them competitive with many modern variants of non-linear
GCNs.",['cs.LG'],142,87
Near real-time map building with multi-class image set labelling and classification of road conditions using convolutional neural networks,"Weather is an important factor affecting transportation and road safety. In
this paper, we leverage state-of-the-art convolutional neural networks in
labelling images taken by street and highway cameras located across across
North America. Road camera snapshots were used in experiments with multiple
deep learning frameworks to classify images by road condition. The training
data for these experiments used images labelled as dry, wet, snow/ice, poor,
and offline. The experiments tested different configurations of six
convolutional neural networks (VGG-16, ResNet50, Xception, InceptionResNetV2,
EfficientNet-B0 and EfficientNet-B4) to assess their suitability to this
problem. The precision, accuracy, and recall were measured for each framework
configuration. In addition, the training sets were varied both in overall size
and by size of individual classes. The final training set included 47,000
images labelled using the five aforementioned classes. The EfficientNet-B4
framework was found to be most suitable to this problem, achieving validation
accuracy of 90.6%, although EfficientNet-B0 achieved an accuracy of 90.3% with
half the execution time. It was observed that VGG-16 with transfer learning
proved to be very useful for data acquisition and pseudo-labelling with limited
hardware resources, throughout this project. The EfficientNet-B4 framework was
then placed into a real-time production environment, where images could be
classified in real-time on an ongoing basis. The classified images were then
used to construct a map showing real-time road conditions at various camera
locations across North America. The choice of these frameworks and our analysis
take into account unique requirements of real-time map building functions. A
detailed analysis of the process of semi-automated dataset labelling using
these frameworks is also presented in this paper.","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']",286,166
Automating Motion Correction in Multishot MRI Using Generative Adversarial Networks,"Multishot Magnetic Resonance Imaging (MRI) has recently gained popularity as
it accelerates the MRI data acquisition process without compromising the
quality of final MR image. However, it suffers from motion artifacts caused by
patient movements which may lead to misdiagnosis. Modern state-of-the-art
motion correction techniques are able to counter small degree motion, however,
their adoption is hindered by their time complexity. This paper proposes a
Generative Adversarial Network (GAN) for reconstructing motion free
high-fidelity images while reducing the image reconstruction time by an
impressive two orders of magnitude.",['cs.CV'],92,76
Towards Interpretable-AI Policies Induction using Evolutionary Nonlinear Decision Trees for Discrete Action Systems,"Black-box AI induction methods such as deep reinforcement learning (DRL) are
increasingly being used to find optimal policies for a given control task.
Although policies represented using a black-box AI are capable of efficiently
executing the underlying control task and achieving optimal closed-loop
performance, the developed control rules are often complex and neither
interpretable nor explainable. In this paper, we use a recently proposed
nonlinear decision-tree (NLDT) approach to find a hierarchical set of control
rules in an attempt to maximize the open-loop performance for approximating and
explaining the pre-trained black-box DRL (oracle) agent using the labelled
state-action dataset. Recent advances in nonlinear optimization approaches
using evolutionary computation facilitates finding a hierarchical set of
nonlinear control rules as a function of state variables using a
computationally fast bilevel optimization procedure at each node of the
proposed NLDT. Additionally, we propose a re-optimization procedure for
enhancing closed-loop performance of an already derived NLDT. We evaluate our
proposed methodologies (open and closed-loop NLDTs) on different control
problems having multiple discrete actions. In all these problems our proposed
approach is able to find relatively simple and interpretable rules involving
one to four non-linear terms per rule, while simultaneously achieving on par
closed-loop performance when compared to a trained black-box DRL agent. A
post-processing approach for simplifying the NLDT is also suggested. The
obtained results are inspiring as they suggest the replacement of complicated
black-box DRL policies involving thousands of parameters (making them
non-interpretable) with relatively simple interpretable policies. Results are
encouraging and motivating to pursue further applications of proposed approach
in solving more complex control tasks.","['cs.LG', 'cs.NE', 'cs.SY', 'eess.SY', 'stat.ML']",280,158
Confounding-Robust Policy Evaluation in Infinite-Horizon Reinforcement Learning,"Off-policy evaluation of sequential decision policies from observational data
is necessary in applications of batch reinforcement learning such as education
and healthcare. In such settings, however, unobserved variables confound
observed actions, rendering exact evaluation of new policies impossible, i.e.,
unidentifiable. We develop a robust approach that estimates sharp bounds on the
(unidentifiable) value of a given policy in an infinite-horizon problem given
data from another policy with unobserved confounding, subject to a sensitivity
model. We consider stationary or baseline unobserved confounding and compute
bounds by optimizing over the set of all stationary state-occupancy ratios that
agree with a new partially identified estimating equation and the sensitivity
model. We prove convergence to the sharp bounds as we collect more confounded
data. Although checking set membership is a linear program, the support
function is given by a difficult nonconvex optimization problem. We develop
approximations based on nonconvex projected gradient descent and demonstrate
the resulting bounds empirically.","['cs.LG', 'math.OC', 'stat.ML']",158,103
Unified Generative Adversarial Networks for Controllable Image-to-Image Translation,"We propose a unified Generative Adversarial Network (GAN) for controllable
image-to-image translation, i.e., transferring an image from a source to a
target domain guided by controllable structures. In addition to conditioning on
a reference image, we show how the model can generate images conditioned on
controllable structures, e.g., class labels, object keypoints, human skeletons,
and scene semantic maps. The proposed model consists of a single generator and
a discriminator taking a conditional image and the target controllable
structure as input. In this way, the conditional image can provide appearance
information and the controllable structure can provide the structure
information for generating the target result. Moreover, our model learns the
image-to-image mapping through three novel losses, i.e., color loss,
controllable structure guided cycle-consistency loss, and controllable
structure guided self-content preserving loss. Also, we present the Fr\'echet
ResNet Distance (FRD) to evaluate the quality of the generated images.
Experiments on two challenging image translation tasks, i.e., hand
gesture-to-gesture translation and cross-view image translation, show that our
model generates convincing results, and significantly outperforms other
state-of-the-art methods on both tasks. Meanwhile, the proposed framework is a
unified solution, thus it can be applied to solving other controllable
structure guided image translation tasks such as landmark guided facial
expression translation and keypoint guided person image generation. To the best
of our knowledge, we are the first to make one GAN framework work on all such
controllable structure guided image translation tasks. Code is available at
https://github.com/Ha0Tang/GestureGAN.","['cs.CV', 'cs.LG', 'eess.IV']",263,145
Smoothed Geometry for Robust Attribution,"Feature attributions are a popular tool for explaining the behavior of Deep
Neural Networks (DNNs), but have recently been shown to be vulnerable to
attacks that produce divergent explanations for nearby inputs. This lack of
robustness is especially problematic in high-stakes applications where
adversarially-manipulated explanations could impair safety and trustworthiness.
Building on a geometric understanding of these attacks presented in recent
work, we identify Lipschitz continuity conditions on models' gradient that lead
to robust gradient-based attributions, and observe that smoothness may also be
related to the ability of an attack to transfer across multiple attribution
methods. To mitigate these attacks in practice, we propose an inexpensive
regularization method that promotes these conditions in DNNs, as well as a
stochastic smoothing technique that does not require re-training. Our
experiments on a range of image models demonstrate that both of these
mitigations consistently improve attribution robustness, and confirm the role
that smooth geometry plays in these attacks on real, large-scale models.","['cs.LG', 'stat.ML']",164,114
Towards Better Model Understanding with Path-Sufficient Explanations,"Feature based local attribution methods are amongst the most prevalent in
explainable artificial intelligence (XAI) literature. Going beyond standard
correlation, recently, methods have been proposed that highlight what should be
minimally sufficient to justify the classification of an input (viz. pertinent
positives). While minimal sufficiency is an attractive property, the resulting
explanations are often too sparse for a human to understand and evaluate the
local behavior of the model, thus making it difficult to judge its overall
quality. To overcome these limitations, we propose a novel method called
Path-Sufficient Explanations Method (PSEM) that outputs a sequence of
sufficient explanations for a given input of strictly decreasing size (or
value) -- from original input to a minimally sufficient explanation -- which
can be thought to trace the local boundary of the model in a smooth manner,
thus providing better intuition about the local model behavior for the specific
input. We validate these claims, both qualitatively and quantitatively, with
experiments that show the benefit of PSEM across all three modalities (image,
tabular and text). A user study depicts the strength of the method in
communicating the local behavior, where (many) users are able to correctly
determine the prediction made by a model.","['cs.LG', 'cs.AI']",199,136
Don't Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop,"The stochastic variance-reduced gradient method (SVRG) and its accelerated
variant (Katyusha) have attracted enormous attention in the machine learning
community in the last few years due to their superior theoretical properties
and empirical behaviour on training supervised machine learning models via the
empirical risk minimization paradigm. A key structural element in both of these
methods is the inclusion of an outer loop at the beginning of which a full pass
over the training data is made in order to compute the exact gradient, which is
then used to construct a variance-reduced estimator of the gradient. In this
work we design {\em loopless variants} of both of these methods. In particular,
we remove the outer loop and replace its function by a coin flip performed in
each iteration designed to trigger, with a small probability, the computation
of the gradient. We prove that the new methods enjoy the same superior
theoretical convergence properties as the original methods. However, we
demonstrate through numerical experiments that our methods have substantially
superior practical behavior.","['cs.LG', 'math.OC', 'stat.ML']",172,111
Temperate Fish Detection and Classification: a Deep Learning based Approach,"A wide range of applications in marine ecology extensively uses underwater
cameras. Still, to efficiently process the vast amount of data generated, we
need to develop tools that can automatically detect and recognize species
captured on film. Classifying fish species from videos and images in natural
environments can be challenging because of noise and variation in illumination
and the surrounding habitat. In this paper, we propose a two-step deep learning
approach for the detection and classification of temperate fishes without
pre-filtering. The first step is to detect each single fish in an image,
independent of species and sex. For this purpose, we employ the You Only Look
Once (YOLO) object detection technique. In the second step, we adopt a
Convolutional Neural Network (CNN) with the Squeeze-and-Excitation (SE)
architecture for classifying each fish in the image without pre-filtering. We
apply transfer learning to overcome the limited training samples of temperate
fishes and to improve the accuracy of the classification. This is done by
training the object detection model with ImageNet and the fish classifier via a
public dataset (Fish4Knowledge), whereupon both the object detection and
classifier are updated with temperate fishes of interest. The weights obtained
from pre-training are applied to post-training as a priori. Our solution
achieves the state-of-the-art accuracy of 99.27\% on the pre-training. The
percentage values for accuracy on the post-training are good; 83.68\% and
87.74\% with and without image augmentation, respectively, indicating that the
solution is viable with a more extensive dataset.","['cs.CV', 'cs.LG', 'eess.IV']",260,149
When Collaborative Filtering Meets Reinforcement Learning,"In this paper, we study a multi-step interactive recommendation problem,
where the item recommended at current step may affect the quality of future
recommendations. To address the problem, we develop a novel and effective
approach, named CFRL, which seamlessly integrates the ideas of both
collaborative filtering (CF) and reinforcement learning (RL). More
specifically, we first model the recommender-user interactive recommendation
problem as an agent-environment RL task, which is mathematically described by a
Markov decision process (MDP). Further, to achieve collaborative
recommendations for the entire user community, we propose a novel CF-based MDP
by encoding the states of all users into a shared latent vector space. Finally,
we propose an effective Q-network learning method to learn the agent's optimal
policy based on the CF-based MDP. The capability of CFRL is demonstrated by
comparing its performance against a variety of existing methods on real-world
datasets.","['cs.LG', 'cs.IR', 'stat.ML']",151,101
VoteNet: A Deep Learning Label Fusion Method for Multi-Atlas Segmentation,"Deep learning (DL) approaches are state-of-the-art for many medical image
segmentation tasks. They offer a number of advantages: they can be trained for
specific tasks, computations are fast at test time, and segmentation quality is
typically high. In contrast, previously popular multi-atlas segmentation (MAS)
methods are relatively slow (as they rely on costly registrations) and even
though sophisticated label fusion strategies have been proposed, DL approaches
generally outperform MAS. In this work, we propose a DL-based label fusion
strategy (VoteNet) which locally selects a set of reliable atlases whose labels
are then fused via plurality voting. Experiments on 3D brain MRI data show that
by selecting a good initial atlas set MAS with VoteNet significantly
outperforms a number of other label fusion strategies as well as a direct DL
segmentation approach. We also provide an experimental analysis of the upper
performance bound achievable by our method. While unlikely achievable in
practice, this bound suggests room for further performance improvements.
Lastly, to address the runtime disadvantage of standard MAS, all our results
make use of a fast DL registration approach.",['cs.CV'],184,130
BridgeDPI: A Novel Graph Neural Network for Predicting Drug-Protein Interactions,"Motivation: Exploring drug-protein interactions (DPIs) work as a pivotal step
in drug discovery. The fast expansion of available biological data enables
computational methods effectively assist in experimental methods. Among them,
deep learning methods extract features only from basic characteristics, such as
protein sequences, molecule structures. Others achieve significant improvement
by learning from not only sequences/molecules but the protein-protein and
drug-drug associations (PPAs and DDAs). The PPAs and DDAs are generally
obtained by using computational methods. However, existing computational
methods have some limitations, resulting in low-quality PPAs and DDAs that
hamper the prediction performance. Therefore, we hope to develop a novel
supervised learning method to learn the PPAs and DDAs effectively and thereby
improve the prediction performance of the specific task of DPI. Results: In
this research, we propose a novel deep learning framework, namely BridgeDPI.
BridgeDPI introduces a class of nodes named hyper-nodes, which bridge different
proteins/drugs to work as PPAs and DDAs. The hyper-nodes can be supervised
learned for the specific task of DPI since the whole process is an end-to-end
learning. Consequently, such a model would improve prediction performance of
DPI. In three real-world datasets, we further demonstrate that BridgeDPI
outperforms state-of-the-art methods. Moreover, ablation studies verify the
effectiveness of the hyper-nodes. Last, in an independent verification,
BridgeDPI explores the candidate bindings among COVID-19's proteins and various
antiviral drugs. And the predictive results accord with the statement of the
World Health Organization and Food and Drug Administration, showing the
validity and reliability of BridgeDPI.","['cs.LG', 'cs.SI', 'q-bio.QM']",263,155
An exact counterfactual-example-based approach to tree-ensemble models interpretability,"Explaining the decisions of machine learning models is becoming a necessity
in many areas where trust in ML models decision is key to their
accreditation/adoption. The ability to explain models decisions also allows to
provide diagnosis in addition to the model decision, which is highly valuable
in scenarios such as fault detection. Unfortunately, high-performance models do
not exhibit the necessary transparency to make their decisions fully
understandable. And the black-boxes approaches, which are used to explain such
model decisions, suffer from a lack of accuracy in tracing back the exact cause
of a model decision regarding a given input. Indeed, they do not have the
ability to explicitly describe the decision regions of the model around that
input, which is necessary to determine what influences the model towards one
decision or the other. We thus asked ourselves the question: is there a
category of high-performance models among the ones currently used for which we
could explicitly and exactly characterise the decision regions in the input
feature space using a geometrical characterisation? Surprisingly we came out
with a positive answer for any model that enters the category of tree ensemble
models, which encompasses a wide range of high-performance models such as
XGBoost, LightGBM, random forests ... We could derive an exact geometrical
characterisation of their decision regions under the form of a collection of
multidimensional intervals. This characterisation makes it straightforward to
compute the optimal counterfactual (CF) example associated with a query point.
We demonstrate several possibilities of the approach, such as computing the CF
example based only on a subset of features. This allows to obtain more
plausible explanations by adding prior knowledge about which variables the user
can control. An adaptation to CF reasoning on regression problems is also
envisaged.","['cs.LG', 'cs.AI']",295,169
Generative Multi-Adversarial Networks,"Generative adversarial networks (GANs) are a framework for producing a
generative model by way of a two-player minimax game. In this paper, we propose
the \emph{Generative Multi-Adversarial Network} (GMAN), a framework that
extends GANs to multiple discriminators. In previous work, the successful
training of GANs requires modifying the minimax objective to accelerate
training early on. In contrast, GMAN can be reliably trained with the original,
untampered objective. We explore a number of design perspectives with the
discriminator role ranging from formidable adversary to forgiving teacher.
Image generation tasks comparing the proposed framework to standard GANs
demonstrate GMAN produces higher quality samples in a fraction of the
iterations when measured by a pairwise GAM-type metric.","['cs.LG', 'cs.MA', 'cs.NE']",118,85
Real-time Visual Object Tracking with Natural Language Description,"In recent years, deep-learning-based visual object trackers have been studied
thoroughly, but handling occlusions and/or rapid motion of the target remains
challenging. In this work, we argue that conditioning on the natural language
(NL) description of a target provides information for longer-term invariance,
and thus helps cope with typical tracking challenges. However, deriving a
formulation to combine the strengths of appearance-based tracking with the
language modality is not straightforward. We propose a novel deep
tracking-by-detection formulation that can take advantage of NL descriptions.
Regions that are related to the given NL description are generated by a
proposal network during the detection phase of the tracker. Our LSTM based
tracker then predicts the update of the target from regions proposed by the NL
based detection phase. In benchmarks, our method is competitive with state of
the art trackers, while it outperforms all other trackers on targets with
unambiguous and precise language annotations. It also beats the
state-of-the-art NL tracker when initializing without a bounding box. Our
method runs at over 30 fps on a single GPU.",['cs.CV'],185,118
Robustness via Deep Low-Rank Representations,"We investigate the effect of the dimensionality of the representations
learned in Deep Neural Networks (DNNs) on their robustness to input
perturbations, both adversarial and random. To achieve low dimensionality of
learned representations, we propose an easy-to-use, end-to-end trainable,
low-rank regularizer (LR) that can be applied to any intermediate layer
representation of a DNN. This regularizer forces the feature representations to
(mostly) lie in a low-dimensional linear subspace. We perform a wide range of
experiments that demonstrate that the LR indeed induces low rank on the
representations, while providing modest improvements to accuracy as an added
benefit. Furthermore, the learned features make the trained model significantly
more robust to input perturbations such as Gaussian and adversarial noise (even
without adversarial training). Lastly, the low-dimensionality means that the
learned features are highly compressible; thus discriminative features of the
data can be stored using very little memory. Our experiments indicate that
models trained using the LR learn robust classifiers by discovering subspaces
that avoid non-robust features. Algorithmically, the LR is scalable, generic,
and straightforward to implement into existing deep learning frameworks.","['cs.LG', 'cs.AI', 'stat.ML']",187,117
Tensor Networks for Multi-Modal Non-Euclidean Data,"Modern data sources are typically of large scale and multi-modal natures, and
acquired on irregular domains, which poses serious challenges to traditional
deep learning models. These issues are partially mitigated by either extending
existing deep learning algorithms to irregular domains through graphs, or by
employing tensor methods to alleviate the computational bottlenecks imposed by
the Curse of Dimensionality. To simultaneously resolve both these issues, we
introduce a novel Multi-Graph Tensor Network (MGTN) framework, which leverages
on the desirable properties of graphs, tensors and neural networks in a
physically meaningful and compact manner. This equips MGTNs with the ability to
exploit local information in irregular data sources at a drastically reduced
parameter complexity, and over a range of learning paradigms such as
regression, classification and reinforcement learning. The benefits of the MGTN
framework, especially its ability to avoid overfitting through the inherent
low-rank regularization properties of tensor networks, are demonstrated through
its superior performance against competing models in the individual tensor,
graph, and neural network domains.",['cs.LG'],168,112
Camera-trap images segmentation using multi-layer robust principal component analysis,"The segmentation of animals from camera-trap images is a difficult task. To
illustrate, there are various challenges due to environmental conditions and
hardware limitation in these images. We proposed a multi-layer robust principal
component analysis (multi-layer RPCA) approach for background subtraction. Our
method computes sparse and low-rank images from a weighted sum of descriptors,
using color and texture features as case of study for camera-trap images
segmentation. The segmentation algorithm is composed of histogram equalization
or Gaussian filtering as pre-processing, and morphological filters with active
contour as post-processing. The parameters of our multi-layer RPCA were
optimized with an exhaustive search. The database consists of camera-trap
images from the Colombian forest taken by the Instituto de Investigaci\'on de
Recursos Biol\'ogicos Alexander von Humboldt. We analyzed the performance of
our method in inherent and therefore challenging situations of camera-trap
images. Furthermore, we compared our method with some state-of-the-art
algorithms of background subtraction, where our multi-layer RPCA outperformed
these other methods. Our multi-layer RPCA reached 76.17 and 69.97% of average
fine-grained F-measure for color and infrared sequences, respectively. To our
best knowledge, this paper is the first work proposing multi-layer RPCA and
using it for camera-trap images segmentation.",['cs.CV'],218,133
Smart Anomaly Detection in Sensor Systems: A Multi-Perspective Review,"Anomaly detection is concerned with identifying data patterns that deviate
remarkably from the expected behaviour. This is an important research problem,
due to its broad set of application domains, from data analysis to e-health,
cybersecurity, predictive maintenance, fault prevention, and industrial
automation. Herein, we review state-of-the-art methods that may be employed to
detect anomalies in the specific area of sensor systems, which poses hard
challenges in terms of information fusion, data volumes, data speed, and
network/energy efficiency, to mention but the most pressing ones. In this
context, anomaly detection is a particularly hard problem, given the need to
find computing-energy accuracy trade-offs in a constrained environment. We
taxonomize methods ranging from conventional techniques (statistical methods,
time-series analysis, signal processing, etc.) to data-driven techniques
(supervised learning, reinforcement learning, deep learning, etc.). We also
look at the impact that different architectural environments (Cloud, Fog, Edge)
can have on the sensors ecosystem. The review points to the most promising
intelligent-sensing methods, and pinpoints a set of interesting open issues and
challenges.",['cs.LG'],178,127
CollaGAN : Collaborative GAN for Missing Image Data Imputation,"In many applications requiring multiple inputs to obtain a desired output, if
any of the input data is missing, it often introduces large amounts of bias.
Although many techniques have been developed for imputing missing data, the
image imputation is still difficult due to complicated nature of natural
images. To address this problem, here we proposed a novel framework for missing
image data imputation, called Collaborative Generative Adversarial Network
(CollaGAN). CollaGAN converts an image imputation problem to a multi-domain
images-to-image translation task so that a single generator and discriminator
network can successfully estimate the missing data using the remaining clean
data set. We demonstrate that CollaGAN produces the images with a higher visual
quality compared to the existing competing approaches in various image
imputation tasks.","['cs.CV', 'cs.LG', 'stat.ML']",128,90
Learning More with Less: GAN-based Medical Image Augmentation,"Convolutional Neural Network (CNN)-based accurate prediction typically
requires large-scale annotated training data. In Medical Imaging, however, both
obtaining medical data and annotating them by expert physicians are
challenging; to overcome this lack of data, Data Augmentation (DA) using
Generative Adversarial Networks (GANs) is essential, since they can synthesize
additional annotated training data to handle small and fragmented medical
images from various scanners--those generated images, realistic but completely
novel, can further fill the real image distribution uncovered by the original
dataset. As a tutorial, this paper introduces GAN-based Medical Image
Augmentation, along with tricks to boost classification/object
detection/segmentation performance using them, based on our experience and
related work. Moreover, we show our first GAN-based DA work using automatic
bounding box annotation, for robust CNN-based brain metastases detection on 256
x 256 MR images; GAN-based DA can boost 10% sensitivity in diagnosis with a
clinically acceptable number of additional False Positives, even with
highly-rough and inconsistent bounding boxes.","['cs.CV', 'cs.AI']",166,121
Gait analysis with curvature maps: A simulation study,"Gait analysis is an important aspect of clinical investigation for detecting
neurological and musculoskeletal disorders and assessing the global health of a
patient. In this paper we propose to focus our attention on extracting relevant
curvature information from the body surface provided by a depth camera. We
assumed that the 3D mesh was made available in a previous step and demonstrated
how curvature maps could be useful to assess asymmetric anomalies with two
simple simulated abnormal gaits compared with a normal one. This research set
the grounds for the future development of a curvature-based gait analysis
system for healthcare professionals.",['cs.CV'],101,82
Post-training Quantization with Multiple Points: Mixed Precision without Mixed Precision,"We consider the post-training quantization problem, which discretizes the
weights of pre-trained deep neural networks without re-training the model. We
propose multipoint quantization, a quantization method that approximates a
full-precision weight vector using a linear combination of multiple vectors of
low-bit numbers; this is in contrast to typical quantization methods that
approximate each weight using a single low precision number. Computationally,
we construct the multipoint quantization with an efficient greedy selection
procedure, and adaptively decides the number of low precision points on each
quantized weight vector based on the error of its output. This allows us to
achieve higher precision levels for important weights that greatly influence
the outputs, yielding an 'effect of mixed precision' but without physical mixed
precision implementations (which requires specialized hardware accelerators).
Empirically, our method can be implemented by common operands, bringing almost
no memory and computation overhead. We show that our method outperforms a range
of state-of-the-art methods on ImageNet classification and it can be
generalized to more challenging tasks like PASCAL VOC object detection.","['cs.LG', 'cs.CV', 'stat.ML']",178,119
Generative networks as inverse problems with fractional wavelet scattering networks,"Deep learning is a hot research topic in the field of machine learning
methods and applications. Generative Adversarial Networks (GANs) and
Variational Auto-Encoders (VAEs) provide impressive image generations from
Gaussian white noise, but both of them are difficult to train since they need
to train the generator (or encoder) and the discriminator (or decoder)
simultaneously, which is easy to cause unstable training. In order to solve or
alleviate the synchronous training difficult problems of GANs and VAEs,
recently, researchers propose Generative Scattering Networks (GSNs), which use
wavelet scattering networks (ScatNets) as the encoder to obtain the features
(or ScatNet embeddings) and convolutional neural networks (CNNs) as the decoder
to generate the image. The advantage of GSNs is the parameters of ScatNets are
not needed to learn, and the disadvantage of GSNs is that the expression
ability of ScatNets is slightly weaker than CNNs and the dimensional reduction
method of Principal Component Analysis (PCA) is easy to lead overfitting in the
training of GSNs, and therefore affect the generated quality in the testing
process. In order to further improve the quality of generated images while keep
the advantages of GSNs, this paper proposes Generative Fractional Scattering
Networks (GFRSNs), which use more expressive fractional wavelet scattering
networks (FrScatNets) instead of ScatNets as the encoder to obtain the features
(or FrScatNet embeddings) and use the similar CNNs of GSNs as the decoder to
generate the image. Additionally, this paper develops a new dimensional
reduction method named Feature-Map Fusion (FMF) instead of PCA for better
keeping the information of FrScatNets and the effect of image fusion on the
quality of image generation is also discussed.","['cs.CV', 'eess.IV']",273,141
Policy Optimization in Adversarial MDPs: Improved Exploration via Dilated Bonuses,"Policy optimization is a widely-used method in reinforcement learning. Due to
its local-search nature, however, theoretical guarantees on global optimality
often rely on extra assumptions on the Markov Decision Processes (MDPs) that
bypass the challenge of global exploration. To eliminate the need of such
assumptions, in this work, we develop a general solution that adds dilated
bonuses to the policy update to facilitate global exploration. To showcase the
power and generality of this technique, we apply it to several episodic MDP
settings with adversarial losses and bandit feedback, improving and
generalizing the state-of-the-art. Specifically, in the tabular case, we obtain
$\widetilde{\mathcal{O}}(\sqrt{T})$ regret where $T$ is the number of episodes,
improving the $\widetilde{\mathcal{O}}({T}^{2/3})$ regret bound by Shani et al.
(2020). When the number of states is infinite, under the assumption that the
state-action values are linear in some low-dimensional features, we obtain
$\widetilde{\mathcal{O}}({T}^{2/3})$ regret with the help of a simulator,
matching the result of Neu and Olkhovskaya (2020) while importantly removing
the need of an exploratory policy that their algorithm requires. When a
simulator is unavailable, we further consider a linear MDP setting and obtain
$\widetilde{\mathcal{O}}({T}^{14/15})$ regret, which is the first result for
linear MDPs with adversarial losses and bandit feedback.","['cs.LG', 'stat.ML']",226,129
Vanishing Point Guided Natural Image Stitching,"Recently, works on improving the naturalness of stitching images gain more
and more extensive attention. Previous methods suffer the failures of severe
projective distortion and unnatural rotation, especially when the number of
involved images is large or images cover a very wide field of view. In this
paper, we propose a novel natural image stitching method, which takes into
account the guidance of vanishing points to tackle the mentioned failures.
Inspired by a vital observation that mutually orthogonal vanishing points in
Manhattan world can provide really useful orientation clues, we design a scheme
to effectively estimate prior of image similarity. Given such estimated prior
as global similarity constraints, we feed it into a popular mesh deformation
framework to achieve impressive natural stitching performances. Compared with
other existing methods, including APAP, SPHP, AANAP, and GSP, our method
achieves state-of-the-art performance in both quantitative and qualitative
experiments on natural image stitching.",['cs.CV'],152,110
Fair Generative Modeling via Weak Supervision,"Real-world datasets are often biased with respect to key demographic factors
such as race and gender. Due to the latent nature of the underlying factors,
detecting and mitigating bias is especially challenging for unsupervised
machine learning. We present a weakly supervised algorithm for overcoming
dataset bias for deep generative models. Our approach requires access to an
additional small, unlabeled reference dataset as the supervision signal, thus
sidestepping the need for explicit labels on the underlying bias factors. Using
this supplementary dataset, we detect the bias in existing datasets via a
density ratio technique and learn generative models which efficiently achieve
the twin goals of: 1) data efficiency by using training examples from both
biased and reference datasets for learning; and 2) data generation close in
distribution to the reference dataset at test time. Empirically, we demonstrate
the efficacy of our approach which reduces bias w.r.t. latent factors by an
average of up to 34.6% over baselines for comparable image generation using
generative adversarial networks.","['cs.LG', 'cs.CV', 'stat.ML']",168,112
Dynamic Graph Representation Learning via Self-Attention Networks,"Learning latent representations of nodes in graphs is an important and
ubiquitous task with widespread applications such as link prediction, node
classification, and graph visualization. Previous methods on graph
representation learning mainly focus on static graphs, however, many real-world
graphs are dynamic and evolve over time. In this paper, we present Dynamic
Self-Attention Network (DySAT), a novel neural architecture that operates on
dynamic graphs and learns node representations that capture both structural
properties and temporal evolutionary patterns. Specifically, DySAT computes
node representations by jointly employing self-attention layers along two
dimensions: structural neighborhood and temporal dynamics. We conduct link
prediction experiments on two classes of graphs: communication networks and
bipartite rating networks. Our experimental results show that DySAT has a
significant performance gain over several different state-of-the-art graph
embedding baselines.","['cs.LG', 'cs.SI', 'stat.ML']",135,101
Discriminative feature generation for classification of imbalanced data,"The data imbalance problem is a frequent bottleneck in the classification
performance of neural networks. In this paper, we propose a novel supervised
discriminative feature generation (DFG) method for a minority class dataset.
DFG is based on the modified structure of a generative adversarial network
consisting of four independent networks: generator, discriminator, feature
extractor, and classifier. To augment the selected discriminative features of
the minority class data by adopting an attention mechanism, the generator for
the class-imbalanced target task is trained, and the feature extractor and
classifier are regularized using the pre-trained features from a large source
data. The experimental results show that the DFG generator enhances the
augmentation of the label-preserved and diverse features, and the
classification results are significantly improved on the target task. The
feature generation model can contribute greatly to the development of data
augmentation methods through discriminative feature generation and supervised
attention methods.","['cs.CV', 'cs.LG']",151,85
Generating Realistic COVID19 X-rays with a Mean Teacher + Transfer Learning GAN,"COVID-19 is a novel infectious disease responsible for over 800K deaths
worldwide as of August 2020. The need for rapid testing is a high priority and
alternative testing strategies including X-ray image classification are a
promising area of research. However, at present, public datasets for COVID19
x-ray images have low data volumes, making it challenging to develop accurate
image classifiers. Several recent papers have made use of Generative
Adversarial Networks (GANs) in order to increase the training data volumes. But
realistic synthetic COVID19 X-rays remain challenging to generate. We present a
novel Mean Teacher + Transfer GAN (MTT-GAN) that generates COVID19 chest X-ray
images of high quality. In order to create a more accurate GAN, we employ
transfer learning from the Kaggle Pneumonia X-Ray dataset, a highly relevant
data source orders of magnitude larger than public COVID19 datasets.
Furthermore, we employ the Mean Teacher algorithm as a constraint to improve
stability of training. Our qualitative analysis shows that the MTT-GAN
generates X-ray images that are greatly superior to a baseline GAN and visually
comparable to real X-rays. Although board-certified radiologists can
distinguish MTT-GAN fakes from real COVID19 X-rays. Quantitative analysis shows
that MTT-GAN greatly improves the accuracy of both a binary COVID19 classifier
as well as a multi-class Pneumonia classifier as compared to a baseline GAN.
Our classification accuracy is favourable as compared to recently reported
results in the literature for similar binary and multi-class COVID19 screening
tasks.","['cs.LG', 'cs.CV', 'eess.IV']",253,143
Image-to-image translation for cross-domain disentanglement,"Deep image translation methods have recently shown excellent results,
outputting high-quality images covering multiple modes of the data
distribution. There has also been increased interest in disentangling the
internal representations learned by deep methods to further improve their
performance and achieve a finer control. In this paper, we bridge these two
objectives and introduce the concept of cross-domain disentanglement. We aim to
separate the internal representation into three parts. The shared part contains
information for both domains. The exclusive parts, on the other hand, contain
only factors of variation that are particular to each domain. We achieve this
through bidirectional image translation based on Generative Adversarial
Networks and cross-domain autoencoders, a novel network component. Our model
offers multiple advantages. We can output diverse samples covering multiple
modes of the distributions of both domains, perform domain-specific image
transfer and interpolation, and cross-domain retrieval without the need of
labeled data, only paired images. We compare our model to the state-of-the-art
in multi-modal image translation and achieve better results for translation on
challenging datasets as well as for cross-domain retrieval on realistic
datasets.",['cs.CV'],190,123
Jointly Trained Image and Video Generation using Residual Vectors,"In this work, we propose a modeling technique for jointly training image and
video generation models by simultaneously learning to map latent variables with
a fixed prior onto real images and interpolate over images to generate videos.
The proposed approach models the variations in representations using residual
vectors encoding the change at each time step over a summary vector for the
entire video. We utilize the technique to jointly train an image generation
model with a fixed prior along with a video generation model lacking
constraints such as disentanglement. The joint training enables the image
generator to exploit temporal information while the video generation model
learns to flexibly share information across frames. Moreover, experimental
results verify our approach's compatibility with pre-training on videos or
images and training on datasets containing a mixture of both. A comprehensive
set of quantitative and qualitative evaluations reveal the improvements in
sample quality and diversity over both video generation and image generation
baselines. We further demonstrate the technique's capabilities of exploiting
similarity in features across frames by applying it to a model based on
decomposing the video into motion and content. The proposed model allows minor
variations in content across frames while maintaining the temporal dependence
through latent vectors encoding the pose or motion features.","['cs.LG', 'cs.CV', 'stat.ML']",213,121
"A tale of two toolkits, report the first: benchmarking time series classification algorithms for correctness and efficiency","sktime is an open source, Python based, sklearn compatible toolkit for time
series analysis developed by researchers at the University of East Anglia
(UEA), University College London and the Alan Turing Institute. A key initial
goal for sktime was to provide time series classification functionality
equivalent to that available in a related java package, tsml, also developed at
UEA. We describe the implementation of six such classifiers in sktime and
compare them to their tsml equivalents. We demonstrate correctness through
equivalence of accuracy on a range of standard test problems and compare the
build time of the different implementations. We find that there is significant
difference in accuracy on only one of the six algorithms we look at (Proximity
Forest). This difference is causing us some pain in debugging. We found a much
wider range of difference in efficiency. Again, this was not unexpected, but it
does highlight ways both toolkits could be improved.","['cs.LG', 'stat.ML']",154,107
Making Online Sketching Hashing Even Faster,"Data-dependent hashing methods have demonstrated good performance in various
machine learning applications to learn a low-dimensional representation from
the original data. However, they still suffer from several obstacles: First,
most of existing hashing methods are trained in a batch mode, yielding
inefficiency for training streaming data. Second, the computational cost and
the memory consumption increase extraordinarily in the big data setting, which
perplexes the training procedure. Third, the lack of labeled data hinders the
improvement of the model performance. To address these difficulties, we utilize
online sketching hashing (OSH) and present a FasteR Online Sketching Hashing
(FROSH) algorithm to sketch the data in a more compact form via an independent
transformation. We provide theoretical justification to guarantee that our
proposed FROSH consumes less time and achieves a comparable sketching precision
under the same memory cost of OSH. We also extend FROSH to its distributed
implementation, namely DFROSH, to further reduce the training time cost of
FROSH while deriving the theoretical bound of the sketching precision. Finally,
we conduct extensive experiments on both synthetic and real datasets to
demonstrate the attractive merits of FROSH and DFROSH.","['cs.LG', 'stat.ML']",187,125
Common Spatial Generative Adversarial Networks based EEG Data Augmentation for Cross-Subject Brain-Computer Interface,"The cross-subject application of EEG-based brain-computer interface (BCI) has
always been limited by large individual difference and complex characteristics
that are difficult to perceive. Therefore, it takes a long time to collect the
training data of each user for calibration. Even transfer learning method
pre-training with amounts of subject-independent data cannot decode different
EEG signal categories without enough subject-specific data. Hence, we proposed
a cross-subject EEG classification framework with a generative adversarial
networks (GANs) based method named common spatial GAN (CS-GAN), which used
adversarial training between a generator and a discriminator to obtain
high-quality data for augmentation. A particular module in the discriminator
was employed to maintain the spatial features of the EEG signals and increase
the difference between different categories, with two losses for further
enhancement. Through adaptive training with sufficient augmentation data, our
cross-subject classification accuracy yielded a significant improvement of
15.85% than leave-one subject-out (LOO) test and 8.57% than just adapting 100
original samples on the dataset 2a of BCI competition IV. Moreover, We designed
a convolutional neural networks (CNNs) based classification method as a
benchmark with a similar spatial enhancement idea, which achieved remarkable
results to classify motor imagery EEG data. In summary, our framework provides
a promising way to deal with the cross-subject problem and promote the
practical application of BCI.","['cs.LG', 'cs.AI', 'eess.SP']",231,147
S2DNAS:Transforming Static CNN Model for Dynamic Inference via Neural Architecture Search,"Recently, dynamic inference has emerged as a promising way to reduce the
computational cost of deep convolutional neural network (CNN). In contrast to
static methods (e.g. weight pruning), dynamic inference adaptively adjusts the
inference process according to each input sample, which can considerably reduce
the computational cost on ""easy"" samples while maintaining the overall model
performance. In this paper, we introduce a general framework, S2DNAS, which can
transform various static CNN models to support dynamic inference via neural
architecture search. To this end, based on a given CNN model, we first generate
a CNN architecture space in which each architecture is a multi-stage CNN
generated from the given model using some predefined transformations. Then, we
propose a reinforcement learning based approach to automatically search for the
optimal CNN architecture in the generated space. At last, with the searched
multi-stage network, we can perform dynamic inference by adaptively choosing a
stage to evaluate for each sample. Unlike previous works that introduce
irregular computations or complex controllers in the inference or re-design a
CNN model from scratch, our method can generalize to most of the popular CNN
architectures and the searched dynamic network can be directly deployed using
existing deep learning frameworks in various hardware devices.","['cs.CV', 'cs.LG']",208,119
YouTube-VOS: Sequence-to-Sequence Video Object Segmentation,"Learning long-term spatial-temporal features are critical for many video
analysis tasks. However, existing video segmentation methods predominantly rely
on static image segmentation techniques, and methods capturing temporal
dependency for segmentation have to depend on pretrained optical flow models,
leading to suboptimal solutions for the problem. End-to-end sequential learning
to explore spatial-temporal features for video segmentation is largely limited
by the scale of available video segmentation datasets, i.e., even the largest
video segmentation dataset only contains 90 short video clips. To solve this
problem, we build a new large-scale video object segmentation dataset called
YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains
3,252 YouTube video clips and 78 categories including common objects and human
activities. This is by far the largest video object segmentation dataset to our
knowledge and we have released it at https://youtube-vos.org. Based on this
dataset, we propose a novel sequence-to-sequence network to fully exploit
long-term spatial-temporal information in videos for segmentation. We
demonstrate that our method is able to achieve the best results on our
YouTube-VOS test set and comparable results on DAVIS 2016 compared to the
current state-of-the-art methods. Experiments show that the large scale dataset
is indeed a key factor to the success of our model.",['cs.CV'],222,131
Artificial Intelligence in the Creative Industries: A Review,"This paper reviews the current state of the art in Artificial Intelligence
(AI) technologies and applications in the context of the creative industries. A
brief background of AI, and specifically Machine Learning (ML) algorithms, is
provided including Convolutional Neural Network (CNNs), Generative Adversarial
Networks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement
Learning (DRL). We categorise creative applications into five groups related to
how AI technologies are used: i) content creation, ii) information analysis,
iii) content enhancement and post production workflows, iv) information
extraction and enhancement, and v) data compression. We critically examine the
successes and limitations of this rapidly advancing technology in each of these
areas. We further differentiate between the use of AI as a creative tool and
its potential as a creator in its own right. We foresee that, in the near
future, machine learning-based AI will be adopted widely as a tool or
collaborative assistant for creativity. In contrast, we observe that the
successes of machine learning in domains with fewer constraints, where AI is
the `creator', remain modest. The potential of AI (or its developers) to win
awards for its original creations in competition with human creatives is also
limited, based on contemporary technologies. We therefore conclude that, in the
context of creative industries, maximum benefit from AI will be derived where
its focus is human centric -- where it is designed to augment, rather than
replace, human creativity.","['cs.CV', 'cs.AI', 'cs.LG']",234,145
Attn-HybridNet: Improving Discriminability of Hybrid Features with Attention Fusion,"The principal component analysis network (PCANet) is an unsupervised
parsimonious deep network, utilizing principal components as filters in its
convolution layers. Albeit powerful, the PCANet consists of basic operations
such as principal components and spatial pooling, which suffers from two
fundamental problems. First, the principal components obtain information by
transforming it to column vectors (which we call the amalgamated view), which
incurs the loss of the spatial information in the data. Second, the generalized
spatial pooling utilized in the PCANet induces feature redundancy and also
fails to accommodate spatial statistics of natural images. In this research, we
first propose a tensor-factorization based deep network called the Tensor
Factorization Network (TFNet). The TFNet extracts features from the spatial
structure of the data (which we call the minutiae view). We then show that the
information obtained by the PCANet and the TFNet are distinctive and
non-trivial but individually insufficient. This phenomenon necessitates the
development of proposed HybridNet, which integrates the information discovery
with the two views of the data. To enhance the discriminability of hybrid
features, we propose Attn-HybridNet, which alleviates the feature redundancy by
performing attention-based feature fusion. The significance of our proposed
Attn-HybridNet is demonstrated on multiple real-world datasets where the
features obtained with Attn-HybridNet achieves better classification
performance over other popular baseline methods, demonstrating the
effectiveness of the proposed technique.","['cs.CV', 'cs.LG']",228,134
Attention-based Dropout Layer for Weakly Supervised Object Localization,"Weakly Supervised Object Localization (WSOL) techniques learn the object
location only using image-level labels, without location annotations. A common
limitation for these techniques is that they cover only the most discriminative
part of the object, not the entire object. To address this problem, we propose
an Attention-based Dropout Layer (ADL), which utilizes the self-attention
mechanism to process the feature maps of the model. The proposed method is
composed of two key components: 1) hiding the most discriminative part from the
model for capturing the integral extent of object, and 2) highlighting the
informative region for improving the recognition power of the model. Based on
extensive experiments, we demonstrate that the proposed method is effective to
improve the accuracy of WSOL, achieving a new state-of-the-art localization
accuracy in CUB-200-2011 dataset. We also show that the proposed method is much
more efficient in terms of both parameter and computation overheads than
existing techniques.",['cs.CV'],159,107
Synthesizing Visual Illusions Using Generative Adversarial Networks,"Visual illusions are a very useful tool for vision scientists, because they
allow them to better probe the limits, thresholds and errors of the visual
system. In this work we introduce the first ever framework to generate novel
visual illusions with an artificial neural network (ANN). It takes the form of
a generative adversarial network, with a generator of visual illusion
candidates and two discriminator modules, one for the inducer background and
another that decides whether or not the candidate is indeed an illusion. The
generality of the model is exemplified by synthesizing illusions of different
types, and validated with psychophysical experiments that corroborate that the
outputs of our ANN are indeed visual illusions to human observers. Apart from
synthesizing new visual illusions, which may help vision researchers, the
proposed model has the potential to open new ways to study the similarities and
differences between ANN and human visual perception.",['cs.CV'],150,97
Applying Lie Groups Approaches for Rigid Registration of Point Clouds,"In the last decades, some literature appeared using the Lie groups theory to
solve problems in computer vision. On the other hand, Lie algebraic
representations of the transformations therein were introduced to overcome the
difficulties behind group structure by mapping the transformation groups to
linear spaces. In this paper we focus on application of Lie groups and Lie
algebras to find the rigid transformation that best register two surfaces
represented by point clouds. The so called pairwise rigid registration can be
formulated by comparing intrinsic second-order orientation tensors that encode
local geometry. These tensors can be (locally) represented by symmetric
non-negative definite matrices. In this paper we interpret the obtained tensor
field as a multivariate normal model. So, we start with the fact that the space
of Gaussians can be equipped with a Lie group structure, that is isomorphic to
a subgroup of the upper triangular matrices. Consequently, the associated Lie
algebra structure enables us to handle Gaussians, and consequently, to compare
orientation tensors, with Euclidean operations. We apply this methodology to
variants of the Iterative Closest Point (ICP), a known technique for pairwise
registration. We compare the obtained results with the original implementations
that apply the comparative tensor shape factor (CTSF), which is a similarity
notion based on the eigenvalues of the orientation tensors. We notice that the
similarity measure in tensor spaces directly derived from Lie's approach is not
invariant under rotations, which is a problem in terms of rigid registration.
Despite of this, the performed computational experiments show promising results
when embedding orientation tensor fields in Lie algebras.",['cs.CV'],264,155
A Novel Approach to Artistic Textual Visualization via GAN,"While the visualization of statistical data tends to a mature technology, the
visualization of textual data is still in its infancy, especially for the
artistic text. Due to the fact that visualization of artistic text is valuable
and attractive in both art and information science, we attempt to realize this
tentative idea in this article. We propose the Generative Adversarial Network
based Artistic Textual Visualization (GAN-ATV) which can create paintings after
analyzing the semantic content of existing poems. Our GAN-ATV consists of two
main sections: natural language analysis section and visual information
synthesis section. In natural language analysis section, we use Bag-of-Word
(BoW) feature descriptors and a two-layer network to mine and analyze the
high-level semantic information from poems. In visual information synthesis
section, we design a cross-modal semantic understanding module and integrate it
with Generative Adversarial Network (GAN) to create paintings, whose content
are corresponding to the original poems. Moreover, in order to train our
GAN-ATV and verify its performance, we establish a cross-modal artistic dataset
named ""Cross-Art"". In the Cross-Art dataset, there are six topics and each
topic has their corresponding paintings and poems. The experimental results on
Cross-Art dataset are shown in this article.",['cs.CV'],209,119
Unsupervised Noisy Tracklet Person Re-identification,"Existing person re-identification (re-id) methods mostly rely on supervised
model learning from a large set of person identity labelled training data per
domain. This limits their scalability and usability in large scale deployments.
In this work, we present a novel selective tracklet learning (STL) approach
that can train discriminative person re-id models from unlabelled tracklet data
in an unsupervised manner. This avoids the tedious and costly process of
exhaustively labelling person image/tracklet true matching pairs across camera
views. Importantly, our method is particularly more robust against arbitrary
noisy data of raw tracklets therefore scalable to learning discriminative
models from unconstrained tracking data. This differs from a handful of
existing alternative methods that often assume the existence of true matches
and balanced tracklet samples per identity class. This is achieved by
formulating a data adaptive image-to-tracklet selective matching loss function
explored in a multi-camera multi-task deep learning model structure. Extensive
comparative experiments demonstrate that the proposed STL model surpasses
significantly the state-of-the-art unsupervised learning and one-shot learning
re-id methods on three large tracklet person re-id benchmarks.",['cs.CV'],189,117
Modulated Policy Hierarchies,"Solving tasks with sparse rewards is a main challenge in reinforcement
learning. While hierarchical controllers are an intuitive approach to this
problem, current methods often require manual reward shaping, alternating
training phases, or manually defined sub tasks. We introduce modulated policy
hierarchies (MPH), that can learn end-to-end to solve tasks from sparse
rewards. To achieve this, we study different modulation signals and exploration
for hierarchical controllers. Specifically, we find that communicating via
bit-vectors is more efficient than selecting one out of multiple skills, as it
enables mixing between them. To facilitate exploration, MPH uses its different
time scales for temporally extended intrinsic motivation at each level of the
hierarchy. We evaluate MPH on the robotics tasks of pushing and sparse block
stacking, where it outperforms recent baselines.","['cs.LG', 'cs.AI']",130,103
OrthoNet: Multilayer Network Data Clustering,"Network data appears in very diverse applications, like biological, social,
or sensor networks. Clustering of network nodes into categories or communities
has thus become a very common task in machine learning and data mining. Network
data comes with some information about the network edges. In some cases, this
network information can even be given with multiple views or multiple layers,
each one representing a different type of relationship between the network
nodes. Increasingly often, network nodes also carry a feature vector. We
propose in this paper to extend the node clustering problem, that commonly
considers only the network information, to a problem where both the network
information and the node features are considered together for learning a
clustering-friendly representation of the feature space. Specifically, we
design a generic two-step algorithm for multilayer network data clustering. The
first step aggregates the different layers of network information into a graph
representation given by the geometric mean of the network Laplacian matrices.
The second step uses a neural net to learn a feature embedding that is
consistent with the structure given by the network layers. We propose a novel
algorithm for efficiently training the neural net via stochastic gradient
descent, which encourages the neural net outputs to span the leading
eigenvectors of the aggregated Laplacian matrix, in order to capture the
pairwise interactions on the network, and provide a clustering-friendly
representation of the feature space. We demonstrate with an extensive set of
experiments on synthetic and real datasets that our method leads to a
significant improvement w.r.t. state-of-the-art multilayer graph clustering
algorithms, as it judiciously combines nodes features and network information
in the node embedding algorithms.","['cs.LG', 'stat.ML']",281,152
Fuzzy c-Means Clustering for Persistence Diagrams,"Persistence diagrams concisely represent the topology of a point cloud whilst
having strong theoretical guarantees, but the question of how to best integrate
this information into machine learning workflows remains open. In this paper we
extend the ubiquitous Fuzzy c-Means (FCM) clustering algorithm to the space of
persistence diagrams, enabling unsupervised learning that automatically
captures the topological structure of data without the topological prior
knowledge or additional processing of persistence diagrams that many other
techniques require. We give theoretical convergence guarantees that correspond
to the Euclidean case, and empirically demonstrate the capability of our
algorithm to capture topological information via the fuzzy RAND index. We end
with experiments on two datasets that utilise both the topological and fuzzy
nature of our algorithm: pre-trained model selection in machine learning and
lattices structures from materials science. As pre-trained models can perform
well on multiple tasks, selecting the best model is a naturally fuzzy problem;
we show that fuzzy clustering persistence diagrams allows for model selection
using the topology of decision boundaries. In materials science, we classify
transformed lattice structure datasets for the first time, whilst the
probabilistic membership values let us rank candidate lattices in a scenario
where further investigation requires expensive laboratory time and expertise.","['cs.LG', 'stat.ML']",207,132
On Learning Language-Invariant Representations for Universal Machine Translation,"The goal of universal machine translation is to learn to translate between
any pair of languages, given a corpus of paired translated documents for
\emph{a small subset} of all pairs of languages. Despite impressive empirical
results and an increasing interest in massively multilingual models,
theoretical analysis on translation errors made by such universal machine
translation models is only nascent. In this paper, we formally prove certain
impossibilities of this endeavour in general, as well as prove positive results
in the presence of additional (but natural) structure of data.
  For the former, we derive a lower bound on the translation error in the
many-to-many translation setting, which shows that any algorithm aiming to
learn shared sentence representations among multiple language pairs has to make
a large translation error on at least one of the translation tasks, if no
assumption on the structure of the languages is made. For the latter, we show
that if the paired documents in the corpus follow a natural
\emph{encoder-decoder} generative process, we can expect a natural notion of
``generalization'': a linear number of language pairs, rather than quadratic,
suffices to learn a good representation. Our theory also explains what kinds of
connection graphs between pairs of languages are better suited: ones with
longer paths result in worse sample complexity in terms of the total number of
documents per language pair needed. We believe our theoretical insights and
implications contribute to the future algorithmic design of universal machine
translation.","['cs.LG', 'cs.CL', 'stat.ML']",247,147
Unsupervised Domain Adaptation through Iterative Consensus Shift in a Multi-Task Graph,"Babies learn with very little supervision by observing the surrounding world.
They synchronize the feedback from all their senses and learn to maintain
consistency and stability among their internal states. Such observations
inspired recent works in multi-task and multi-modal learning, but existing
methods rely on expensive manual supervision. In contrast, our proposed
multi-task graph, with consensus shift learning, relies only on pseudo-labels
provided by expert models. In our graph, every node represents a task, and
every edge learns to transform one input node into another. Once initialized,
the graph learns by itself on virtually any novel target domain. An adaptive
selection mechanism finds consensus among multiple paths reaching a given node
and establishes the pseudo-ground truth at that node. Such pseudo-labels, given
by ensemble pathways in the graph, are used during the next learning iteration
when single edges distill this distributed knowledge. We validate our key
contributions experimentally and demonstrate strong performance on the Replica
dataset, superior to the very few published methods on multi-task learning with
minimal supervision.","['cs.LG', 'cs.CV']",176,118
Stochastic Neural Radiance Fields:Quantifying Uncertainty in Implicit 3D Representations,"Neural Radiance Fields (NeRF) has become a popular framework for learning
implicit 3D representations and addressing different tasks such as novel-view
synthesis or depth-map estimation. However, in downstream applications where
decisions need to be made based on automatic predictions, it is critical to
leverage the confidence associated with the model estimations. Whereas
uncertainty quantification is a long-standing problem in Machine Learning, it
has been largely overlooked in the recent NeRF literature. In this context, we
propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of
standard NeRF that learns a probability distribution over all the possible
radiance fields modeling the scene. This distribution allows to quantify the
uncertainty associated with the scene information provided by the model. S-NeRF
optimization is posed as a Bayesian learning problem which is efficiently
addressed using the Variational Inference framework. Exhaustive experiments
over benchmark datasets demonstrate that S-NeRF is able to provide more
reliable predictions and confidence values than generic approaches previously
proposed for uncertainty estimation in other domains.",['cs.CV'],169,117
ShapeFlow: Dynamic Shape Interpreter for TensorFlow,"We present ShapeFlow, a dynamic abstract interpreter for TensorFlow which
quickly catches tensor shape incompatibility errors, one of the most common
bugs in deep learning code. ShapeFlow shares the same APIs as TensorFlow but
only captures and emits tensor shapes, its abstract domain. ShapeFlow
constructs a custom shape computational graph, similar to the computational
graph used by TensorFlow. ShapeFlow requires no code annotation or code
modification by the programmer, and therefore is convenient to use. We evaluate
ShapeFlow on 52 programs collected by prior empirical studies to show how fast
and accurately it can catch shape incompatibility errors compared to
TensorFlow. We use two baselines: a worst-case training dataset size and a more
realistic dataset size. ShapeFlow detects shape incompatibility errors highly
accurately -- with no false positives and a single false negative -- and highly
efficiently -- with an average speed-up of 499X and 24X for the first and
second baseline, respectively. We believe ShapeFlow is a practical tool that
benefits machine learning developers. We will open-source ShapeFlow on GitHub
to make it publicly available to both the developer and research communities.","['cs.LG', 'cs.SE']",183,117
MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection,"Weakly supervised video anomaly detection (WS-VAD) is to distinguish
anomalies from normal events based on discriminative representations. Most
existing works are limited in insufficient video representations. In this work,
we develop a multiple instance self-training framework (MIST)to efficiently
refine task-specific discriminative representations with only video-level
annotations. In particular, MIST is composed of 1) a multiple instance pseudo
label generator, which adapts a sparse continuous sampling strategy to produce
more reliable clip-level pseudo labels, and 2) a self-guided attention boosted
feature encoder that aims to automatically focus on anomalous regions in frames
while extracting task-specific representations. Moreover, we adopt a
self-training scheme to optimize both components and finally obtain a
task-specific feature encoder. Extensive experiments on two public datasets
demonstrate the efficacy of our method, and our method performs comparably to
or even better than existing supervised and weakly supervised methods,
specifically obtaining a frame-level AUC 94.83% on ShanghaiTech.",['cs.CV'],160,112
On the Evaluation of Conditional GANs,"Conditional Generative Adversarial Networks (cGANs) are finding increasingly
widespread use in many application domains. Despite outstanding progress,
quantitative evaluation of such models often involves multiple distinct metrics
to assess different desirable properties, such as image quality, conditional
consistency, and intra-conditioning diversity. In this setting, model
benchmarking becomes a challenge, as each metric may indicate a different
""best"" model. In this paper, we propose the Frechet Joint Distance (FJD), which
is defined as the Frechet distance between joint distributions of images and
conditioning, allowing it to implicitly capture the aforementioned properties
in a single metric. We conduct proof-of-concept experiments on a controllable
synthetic dataset, which consistently highlight the benefits of FJD when
compared to currently established metrics. Moreover, we use the newly
introduced metric to compare existing cGAN-based models for a variety of
conditioning modalities (e.g. class labels, object masks, bounding boxes,
images, and text captions). We show that FJD can be used as a promising single
metric for cGAN benchmarking and model selection. Code can be found at
https://github.com/facebookresearch/fjd.","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']",178,127
ConiVAT: Cluster Tendency Assessment and Clustering with Partial Background Knowledge,"The VAT method is a visual technique for determining the potential cluster
structure and the possible number of clusters in numerical data. Its improved
version, iVAT, uses a path-based distance transform to improve the
effectiveness of VAT for ""tough"" cases. Both VAT and iVAT have also been used
in conjunction with a single-linkage(SL) hierarchical clustering algorithm.
However, they are sensitive to noise and bridge points between clusters in the
dataset, and consequently, the corresponding VAT/iVAT images are often
in-conclusive for such cases. In this paper, we propose a constraint-based
version of iVAT, which we call ConiVAT, that makes use of background knowledge
in the form of constraints, to improve VAT/iVAT for challenging and complex
datasets. ConiVAT uses the input constraints to learn the underlying similarity
metric and builds a minimum transitive dissimilarity matrix, before applying
VAT to it. We demonstrate ConiVAT approach to visual assessment and single
linkage clustering on nine datasets to show that, it improves the quality of
iVAT images for complex datasets, and it also overcomes the limitation of SL
clustering with VAT/iVAT due to ""noisy"" bridges between clusters. Extensive
experiment results on nine datasets suggest that ConiVAT outperforms the other
three semi-supervised clustering algorithms in terms of improved clustering
accuracy.","['cs.LG', 'stat.ML']",213,121
Temporal Network Embedding via Tensor Factorization,"Representation learning on static graph-structured data has shown a
significant impact on many real-world applications. However, less attention has
been paid to the evolving nature of temporal networks, in which the edges are
often changing over time. The embeddings of such temporal networks should
encode both graph-structured information and the temporally evolving pattern.
Existing approaches in learning temporally evolving network representations
fail to capture the temporal interdependence. In this paper, we propose Toffee,
a novel approach for temporal network representation learning based on tensor
decomposition. Our method exploits the tensor-tensor product operator to encode
the cross-time information, so that the periodic changes in the evolving
networks can be captured. Experimental results demonstrate that Toffee
outperforms existing methods on multiple real-world temporal networks in
generating effective embeddings for the link prediction tasks.",['cs.LG'],137,91
3D Face Modeling From Diverse Raw Scan Data,"Traditional 3D face models learn a latent representation of faces using
linear subspaces from limited scans of a single database. The main roadblock of
building a large-scale face model from diverse 3D databases lies in the lack of
dense correspondence among raw scans. To address these problems, this paper
proposes an innovative framework to jointly learn a nonlinear face model from a
diverse set of raw 3D scan databases and establish dense point-to-point
correspondence among their scans. Specifically, by treating input scans as
unorganized point clouds, we explore the use of PointNet architectures for
converting point clouds to identity and expression feature representations,
from which the decoder networks recover their 3D face shapes. Further, we
propose a weakly supervised learning approach that does not require
correspondence label for the scans. We demonstrate the superior dense
correspondence and representation power of our proposed method, and its
contribution to single-image 3D face reconstruction.",['cs.CV'],155,99
Fast-Slow Streamflow Model Using Mass-Conserving LSTM,"Streamflow forecasting is key to effectively managing water resources and
preparing for the occurrence of natural calamities being exacerbated by climate
change. Here we use the concept of fast and slow flow components to create a
new mass-conserving Long Short-Term Memory (LSTM) neural network model. It uses
hydrometeorological time series and catchment attributes to predict daily river
discharges. Preliminary results evidence improvement in skills for different
scores compared to the recent literature.","['cs.LG', 'physics.ao-ph', '68T07', 'J.2; I.2.m']",74,65
When Liebig's Barrel Meets Facial Landmark Detection: A Practical Model,"In recent years, significant progress has been made in the research of facial
landmark detection. However, few prior works have thoroughly discussed about
models for practical applications. Instead, they often focus on improving a
couple of issues at a time while ignoring the others. To bridge this gap, we
aim to explore a practical model that is accurate, robust, efficient,
generalizable, and end-to-end trainable at the same time. To this end, we first
propose a baseline model equipped with one transformer decoder as detection
head. In order to achieve a better accuracy, we further propose two lightweight
modules, namely dynamic query initialization (DQInit) and query-aware memory
(QAMem). Specifically, DQInit dynamically initializes the queries of decoder
from the inputs, enabling the model to achieve as good accuracy as the ones
with multiple decoder layers. QAMem is designed to enhance the discriminative
ability of queries on low-resolution feature maps by assigning separate memory
values to each query rather than a shared one. With the help of QAMem, our
model removes the dependence on high-resolution feature maps and is still able
to obtain superior accuracy. Extensive experiments and analysis on three
popular benchmarks show the effectiveness and practical advantages of the
proposed model. Notably, our model achieves new state of the art on WFLW as
well as competitive results on 300W and COFW, while still running at 50+ FPS.",['cs.CV'],231,147
Reinforcement and Imitation Learning via Interactive No-Regret Learning,"Recent work has demonstrated that problems-- particularly imitation learning
and structured prediction-- where a learner's predictions influence the
input-distribution it is tested on can be naturally addressed by an interactive
approach and analyzed using no-regret online learning. These approaches to
imitation learning, however, neither require nor benefit from information about
the cost of actions. We extend existing results in two directions: first, we
develop an interactive imitation learning approach that leverages cost
information; second, we extend the technique to address reinforcement learning.
The results provide theoretical support to the commonly observed successes of
online approximate policy iteration. Our approach suggests a broad new family
of algorithms and provides a unifying view of existing techniques for imitation
and reinforcement learning.","['cs.LG', 'stat.ML']",122,88
Multi-Layered Gradient Boosting Decision Trees,"Multi-layered representation is believed to be the key ingredient of deep
neural networks especially in cognitive tasks like computer vision. While
non-differentiable models such as gradient boosting decision trees (GBDTs) are
the dominant methods for modeling discrete or tabular data, they are hard to
incorporate with such representation learning ability. In this work, we propose
the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring
the ability to learn hierarchical representations by stacking several layers of
regression GBDTs as its building block. The model can be jointly trained by a
variant of target propagation across layers, without the need to derive
back-propagation nor differentiability. Experiments and visualizations
confirmed the effectiveness of the model in terms of performance and
representation learning ability.","['cs.LG', 'stat.ML']",126,95
Flying Objects Detection from a Single Moving Camera,"We propose an approach to detect flying objects such as UAVs and aircrafts
when they occupy a small portion of the field of view, possibly moving against
complex backgrounds, and are filmed by a camera that itself moves.
  Solving such a difficult problem requires combining both appearance and
motion cues. To this end we propose a regression-based approach to motion
stabilization of local image patches that allows us to achieve effective
classification on spatio-temporal image cubes and outperform state-of-the-art
techniques.
  As the problem is relatively new, we collected two challenging datasets for
UAVs and Aircrafts, which can be used as benchmarks for flying objects
detection and vision-guided collision avoidance.",['cs.CV'],115,87
Video Cloze Procedure for Self-Supervised Spatio-Temporal Learning,"We propose a novel self-supervised method, referred to as Video Cloze
Procedure (VCP), to learn rich spatial-temporal representations. VCP first
generates ""blanks"" by withholding video clips and then creates ""options"" by
applying spatio-temporal operations on the withheld clips. Finally, it fills
the blanks with ""options"" and learns representations by predicting the
categories of operations applied on the clips. VCP can act as either a proxy
task or a target task in self-supervised learning. As a proxy task, it converts
rich self-supervised representations into video clip operations (options),
which enhances the flexibility and reduces the complexity of representation
learning. As a target task, it can assess learned representation models in a
uniform and interpretable manner. With VCP, we train spatial-temporal
representation models (3D-CNNs) and apply such models on action recognition and
video retrieval tasks. Experiments on commonly used benchmarks show that the
trained models outperform the state-of-the-art self-supervised models with
significant margins.",['cs.CV'],162,93
Sparse Network Inversion for Key Instance Detection in Multiple Instance Learning,"Multiple Instance Learning (MIL) involves predicting a single label for a bag
of instances, given positive or negative labels at bag-level, without accessing
to label for each instance in the training phase. Since a positive bag contains
both positive and negative instances, it is often required to detect positive
instances (key instances) when a set of instances is categorized as a positive
bag. The attention-based deep MIL model is a recent advance in both bag-level
classification and key instance detection (KID). However, if the positive and
negative instances in a positive bag are not clearly distinguishable, the
attention-based deep MIL model has limited KID performance as the attention
scores are skewed to few positive instances. In this paper, we present a method
to improve the attention-based deep MIL model in the task of KID. The main idea
is to use the neural network inversion to find which instances made
contribution to the bag-level prediction produced by the trained MIL model.
Moreover, we incorporate a sparseness constraint into the neural network
inversion, leading to the sparse network inversion which is solved by the
proximal gradient method. Numerical experiments on an MNIST-based image MIL
dataset and two real-world histopathology datasets verify the validity of our
method, demonstrating the KID performance is significantly improved while the
performance of bag-level prediction is maintained.","['cs.LG', 'stat.ML']",228,117
On implicit regularization: Morse functions and applications to matrix factorization,"In this paper, we revisit implicit regularization from the ground up using
notions from dynamical systems and invariant subspaces of Morse functions. The
key contributions are a new criterion for implicit regularization---a leading
contender to explain the generalization power of deep models such as neural
networks---and a general blueprint to study it. We apply these techniques to
settle a conjecture on implicit regularization in matrix factorization.","['cs.LG', 'stat.ML']",68,55
Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training,"Learning to navigate in a visual environment following natural-language
instructions is a challenging task, because the multimodal inputs to the agent
are highly variable, and the training data on a new task is often limited. In
this paper, we present the first pre-training and fine-tuning paradigm for
vision-and-language navigation (VLN) tasks. By training on a large amount of
image-text-action triplets in a self-supervised learning manner, the
pre-trained model provides generic representations of visual environments and
language instructions. It can be easily used as a drop-in for existing VLN
frameworks, leading to the proposed agent called Prevalent. It learns more
effectively in new tasks and generalizes better in a previously unseen
environment. The performance is validated on three VLN tasks. On the
Room-to-Room benchmark, our model improves the state-of-the-art from 47% to 51%
on success rate weighted by path length. Further, the learned representation is
transferable to other VLN tasks. On two recent tasks, vision-and-dialog
navigation and ""Help, Anna!"" the proposed Prevalent leads to significant
improvement over existing methods, achieving a new state of the art.","['cs.CV', 'cs.CL', 'cs.LG', 'cs.RO']",192,118
Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators,"Convolutional Neural Networks (CNNs) have emerged as highly successful tools
for image generation, recovery, and restoration. A major contributing factor to
this success is that convolutional networks impose strong prior assumptions
about natural images. A surprising experiment that highlights this
architectural bias towards natural images is that one can remove noise and
corruptions from a natural image without using any training data, by simply
fitting (via gradient descent) a randomly initialized, over-parameterized
convolutional generator to the corrupted image. While this over-parameterized
network can fit the corrupted image perfectly, surprisingly after a few
iterations of gradient descent it generates an almost uncorrupted image. This
intriguing phenomenon enables state-of-the-art CNN-based denoising and
regularization of other inverse problems. In this paper, we attribute this
effect to a particular architectural choice of convolutional networks, namely
convolutions with fixed interpolating filters. We then formally characterize
the dynamics of fitting a two-layer convolutional generator to a noisy signal
and prove that early-stopped gradient descent denoises/regularizes. Our proof
relies on showing that convolutional generators fit the structured part of an
image significantly faster than the corrupted portion.","['cs.LG', 'cs.CV', 'cs.IT', 'math.IT', 'stat.ML']",189,131
Explaining the Black-box Smoothly- A Counterfactual Approach,"We propose a BlackBox \emph{Counterfactual Explainer} that is explicitly
developed for medical imaging applications. Classical approaches (e.g. saliency
maps) assessing feature importance do not explain \emph{how} and \emph{why}
variations in a particular anatomical region is relevant to the outcome, which
is crucial for transparent decision making in healthcare application. Our
framework explains the outcome by gradually \emph{exaggerating} the semantic
effect of the given outcome label. Given a query input to a classifier,
Generative Adversarial Networks produce a progressive set of perturbations to
the query image that gradually changes the posterior probability from its
original class to its negation. We design the loss function to ensure that
essential and potentially relevant details, such as support devices, are
preserved in the counterfactually generated images. We provide an extensive
evaluation of different classification tasks on the chest X-Ray images. Our
experiments show that a counterfactually generated visual explanation is
consistent with the disease's clinical relevant measurements, both
quantitatively and qualitatively.","['cs.CV', 'eess.IV']",164,117
A Bayesian Nonparametric Approach for Estimating Individualized Treatment-Response Curves,"We study the problem of estimating the continuous response over time to
interventions using observational time series---a retrospective dataset where
the policy by which the data are generated is unknown to the learner. We are
motivated by applications where response varies by individuals and therefore,
estimating responses at the individual-level is valuable for personalizing
decision-making. We refer to this as the problem of estimating individualized
treatment response (ITR) curves. In statistics, G-computation formula (Robins,
1986) has been commonly used for estimating treatment responses from
observational data containing sequential treatment assignments. However, past
studies have focused predominantly on obtaining point-in-time estimates at the
population level. We leverage the G-computation formula and develop a novel
Bayesian nonparametric (BNP) method that can flexibly model functional data and
provide posterior inference over the treatment response curves at both the
individual and population level. On a challenging dataset containing time
series from patients admitted to a hospital, we estimate responses to
treatments used in managing kidney function and show that the resulting fits
are more accurate than alternative approaches. Accurate methods for obtaining
ITRs from observational data can dramatically accelerate the pace at which
personalized treatment plans become possible.","['cs.LG', 'stat.ML']",201,122
Self-Ensembling with GAN-based Data Augmentation for Domain Adaptation in Semantic Segmentation,"Deep learning-based semantic segmentation methods have an intrinsic
limitation that training a model requires a large amount of data with
pixel-level annotations. To address this challenging issue, many researchers
give attention to unsupervised domain adaptation for semantic segmentation.
Unsupervised domain adaptation seeks to adapt the model trained on the source
domain to the target domain. In this paper, we introduce a self-ensembling
technique, one of the successful methods for domain adaptation in
classification. However, applying self-ensembling to semantic segmentation is
very difficult because heavily-tuned manual data augmentation used in
self-ensembling is not useful to reduce the large domain gap in the semantic
segmentation. To overcome this limitation, we propose a novel framework
consisting of two components, which are complementary to each other. First, we
present a data augmentation method based on Generative Adversarial Networks
(GANs), which is computationally efficient and effective to facilitate domain
alignment. Given those augmented images, we apply self-ensembling to enhance
the performance of the segmentation network on the target domain. The proposed
method outperforms state-of-the-art semantic segmentation methods on
unsupervised domain adaptation benchmarks.",['cs.CV'],187,110
Dense Scene Flow from Stereo Disparity and Optical Flow,"Scene flow describes 3D motion in a 3D scene. It can either be modeled as a
single task, or it can be reconstructed from the auxiliary tasks of stereo
depth and optical flow estimation. While the second method can achieve
real-time performance by using real-time auxiliary methods, it will typically
produce non-dense results. In this representation of a basic combination
approach for scene flow estimation, we will tackle the problem of non-density
by interpolation.",['cs.CV'],78,57
Proposal Learning for Semi-Supervised Object Detection,"In this paper, we focus on semi-supervised object detection to boost
performance of proposal-based object detectors (a.k.a. two-stage object
detectors) by training on both labeled and unlabeled data. However, it is
non-trivial to train object detectors on unlabeled data due to the
unavailability of ground truth labels. To address this problem, we present a
proposal learning approach to learn proposal features and predictions from both
labeled and unlabeled data. The approach consists of a self-supervised proposal
learning module and a consistency-based proposal learning module. In the
self-supervised proposal learning module, we present a proposal location loss
and a contrastive loss to learn context-aware and noise-robust proposal
features respectively. In the consistency-based proposal learning module, we
apply consistency losses to both bounding box classification and regression
predictions of proposals to learn noise-robust proposal features and
predictions. Our approach enjoys the following benefits: 1) encouraging more
context information to delivered in the proposals learning procedure; 2) noisy
proposal features and enforcing consistency to allow noise-robust object
detection; 3) building a general and high-performance semi-supervised object
detection framework, which can be easily adapted to proposal-based object
detectors with different backbone architectures. Experiments are conducted on
the COCO dataset with all available labeled and unlabeled data. Results
demonstrate that our approach consistently improves the performance of
fully-supervised baselines. In particular, after combining with data
distillation, our approach improves AP by about 2.0% and 0.9% on average
compared to fully-supervised baselines and data distillation baselines
respectively.",['cs.CV'],262,124
Fighting deepfakes by detecting GAN DCT anomalies,"To properly contrast the Deepfake phenomenon the need to design new Deepfake
detection algorithms arises; the misuse of this formidable A.I. technology
brings serious consequences in the private life of every involved person.
State-of-the-art proliferates with solutions using deep neural networks to
detect a fake multimedia content but unfortunately these algorithms appear to
be neither generalizable nor explainable. However, traces left by Generative
Adversarial Network (GAN) engines during the creation of the Deepfakes can be
detected by analyzing ad-hoc frequencies. For this reason, in this paper we
propose a new pipeline able to detect the so-called GAN Specific Frequencies
(GSF) representing a unique fingerprint of the different generative
architectures. By employing Discrete Cosine Transform (DCT), anomalous
frequencies were detected. The \BETA statistics inferred by the AC coefficients
distribution have been the key to recognize GAN-engine generated data.
Robustness tests were also carried out in order to demonstrate the
effectiveness of the technique using different attacks on images such as JPEG
Compression, mirroring, rotation, scaling, addition of random sized rectangles.
Experiments demonstrated that the method is innovative, exceeds the state of
the art and also give many insights in terms of explainability.",['cs.CV'],198,147
Text-Guided Neural Image Inpainting,"Image inpainting task requires filling the corrupted image with contents
coherent with the context. This research field has achieved promising progress
by using neural image inpainting methods. Nevertheless, there is still a
critical challenge in guessing the missed content with only the context pixels.
The goal of this paper is to fill the semantic information in corrupted images
according to the provided descriptive text. Unique from existing text-guided
image generation works, the inpainting models are required to compare the
semantic content of the given text and the remaining part of the image, then
find out the semantic content that should be filled for missing part. To
fulfill such a task, we propose a novel inpainting model named Text-Guided Dual
Attention Inpainting Network (TDANet). Firstly, a dual multimodal attention
mechanism is designed to extract the explicit semantic information about the
corrupted regions, which is done by comparing the descriptive text and
complementary image areas through reciprocal attention. Secondly, an image-text
matching loss is applied to maximize the semantic similarity of the generated
image and the text. Experiments are conducted on two open datasets. Results
show that the proposed TDANet model reaches new state-of-the-art on both
quantitative and qualitative measures. Result analysis suggests that the
generated images are consistent with the guidance text, enabling the generation
of various results by providing different descriptions. Codes are available at
https://github.com/idealwhite/TDANet","['cs.CV', 'cs.CL']",236,148
Open-Ended Fine-Grained 3D Object Categorization by Combining Shape and Texture Features in Multiple Colorspaces,"As a consequence of an ever-increasing number of service robots, there is a
growing demand for highly accurate real-time 3D object recognition. Considering
the expansion of robot applications in more complex and dynamic environments,it
is evident that it is not possible to pre-program all object categories and
anticipate all exceptions in advance. Therefore, robots should have the
functionality to learn about new object categories in an open-ended fashion
while working in the environment.Towards this goal, we propose a deep transfer
learning approach to generate a scale- and pose-invariant object representation
by considering shape and texture information in multiple colorspaces. The
obtained global object representation is then fed to an instance-based object
category learning and recognition,where a non-expert human user exists in the
learning loop and can interactively guide the process of experience acquisition
by teaching new object categories, or by correcting insufficient or erroneous
categories. In this work, shape information encodes the common patterns of all
categories, while texture information is used to describes the appearance of
each instance in detail.Multiple color space combinations and network
architectures are evaluated to find the most descriptive system. Experimental
results showed that the proposed network architecture out-performed the
selected state-of-the-art approaches in terms of object classification accuracy
and scalability. Furthermore, we performed a real robot experiment in the
context of serve-a-beer scenario to show the real-time performance of the
proposed approach.","['cs.CV', 'cs.RO']",246,151
A Selective Overview of Deep Learning,"Deep learning has arguably achieved tremendous success in recent years. In
simple words, deep learning uses the composition of many nonlinear functions to
model the complex dependency between input features and labels. While neural
networks have a long history, recent advances have greatly improved their
performance in computer vision, natural language processing, etc. From the
statistical and scientific perspective, it is natural to ask: What is deep
learning? What are the new characteristics of deep learning, compared with
classical methods? What are the theoretical foundations of deep learning? To
answer these questions, we introduce common neural network models (e.g.,
convolutional neural nets, recurrent neural nets, generative adversarial nets)
and training techniques (e.g., stochastic gradient descent, dropout, batch
normalization) from a statistical point of view. Along the way, we highlight
new characteristics of deep learning (including depth and over-parametrization)
and explain their practical and theoretical benefits. We also sample recent
results on theories of deep learning, many of which are only suggestive. While
a complete understanding of deep learning remains elusive, we hope that our
perspectives and discussions serve as a stimulus for new statistical research.","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",188,125
Deep Learning for Multi-Task Medical Image Segmentation in Multiple Modalities,"Automatic segmentation of medical images is an important task for many
clinical applications. In practice, a wide range of anatomical structures are
visualised using different imaging modalities. In this paper, we investigate
whether a single convolutional neural network (CNN) can be trained to perform
different segmentation tasks.
  A single CNN is trained to segment six tissues in MR brain images, the
pectoral muscle in MR breast images, and the coronary arteries in cardiac CTA.
The CNN therefore learns to identify the imaging modality, the visualised
anatomical structures, and the tissue classes.
  For each of the three tasks (brain MRI, breast MRI and cardiac CTA), this
combined training procedure resulted in a segmentation performance equivalent
to that of a CNN trained specifically for that task, demonstrating the high
capacity of CNN architectures. Hence, a single system could be used in clinical
practice to automatically perform diverse segmentation tasks without
task-specific training.",['cs.CV'],151,89
Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx,"We extend the neural basis expansion analysis (NBEATS) to incorporate
exogenous factors. The resulting method, called NBEATSx, improves on a well
performing deep learning model, extending its capabilities by including
exogenous variables and allowing it to integrate multiple sources of useful
information. To showcase the utility of the NBEATSx model, we conduct a
comprehensive study of its application to electricity price forecasting (EPF)
tasks across a broad range of years and markets. We observe state-of-the-art
performance, significantly improving the forecast accuracy by nearly 20% over
the original NBEATS model, and by up to 5% over other well established
statistical and machine learning methods specialized for these tasks.
Additionally, the proposed neural network has an interpretable configuration
that can structurally decompose time series, visualizing the relative impact of
trend and seasonal components and revealing the modeled processes' interactions
with exogenous factors. To assist related work we made the code available in
https://github.com/cchallu/nbeatsx.","['cs.LG', 'cs.AI']",158,116
Transformation Consistent Self-ensembling Model for Semi-supervised Medical Image Segmentation,"Deep convolutional neural networks have achieved remarkable progress on a
variety of medical image computing tasks. A common problem when applying
supervised deep learning methods to medical images is the lack of labeled data,
which is very expensive and time-consuming to be collected. In this paper, we
present a novel semi-supervised method for medical image segmentation, where
the network is optimized by the weighted combination of a common supervised
loss for labeled inputs only and a regularization loss for both labeled and
unlabeled data. To utilize the unlabeled data, our method encourages the
consistent predictions of the network-in-training for the same input under
different regularizations. Aiming for the semi-supervised segmentation problem,
we enhance the effect of regularization for pixel-level predictions by
introducing a transformation, including rotation and flipping, consistent
scheme in our self-ensembling model. With the aim of semi-supervised
segmentation tasks, we introduce a transformation consistent strategy in our
self-ensembling model to enhance the regularization effect for pixel-level
predictions. We have extensively validated the proposed semi-supervised method
on three typical yet challenging medical image segmentation tasks: (i) skin
lesion segmentation from dermoscopy images on International Skin Imaging
Collaboration (ISIC) 2017 dataset, (ii) optic disc segmentation from fundus
images on Retinal Fundus Glaucoma Challenge (REFUGE) dataset, and (iii) liver
segmentation from volumetric CT scans on Liver Tumor Segmentation Challenge
(LiTS) dataset. Compared to the state-of-the-arts, our proposed method shows
superior segmentation performance on challenging 2D/3D medical images,
demonstrating the effectiveness of our semi-supervised method for medical image
segmentation.",['cs.CV'],263,141
A step towards neural genome assembly,"De novo genome assembly focuses on finding connections between a vast amount
of short sequences in order to reconstruct the original genome. The central
problem of genome assembly could be described as finding a Hamiltonian path
through a large directed graph with a constraint that an unknown number of
nodes and edges should be avoided. However, due to local structures in the
graph and biological features, the problem can be reduced to graph
simplification, which includes removal of redundant information. Motivated by
recent advancements in graph representation learning and neural execution of
algorithms, in this work we train the MPNN model with max-aggregator to execute
several algorithms for graph simplification. We show that the algorithms were
learned successfully and can be scaled to graphs of sizes up to 20 times larger
than the ones used in training. We also test on graphs obtained from real-world
genomic data---that of a lambda phage and E. coli.","['cs.LG', 'q-bio.GN']",157,107
Kernel-Based Training of Generative Networks,"Generative adversarial networks (GANs) are designed with the help of min-max
optimization problems that are solved with stochastic gradient-type algorithms
which are known to be non-robust. In this work we revisit a non-adversarial
method based on kernels which relies on a pure minimization problem and propose
a simple stochastic gradient algorithm for the computation of its solution.
Using simplified tools from Stochastic Approximation theory we demonstrate that
batch versions of the algorithm or smoothing of the gradient do not improve
convergence. These observations allow for the development of a training
algorithm that enjoys reduced computational complexity and increased robustness
while exhibiting similar synthesis characteristics as classical GANs.","['cs.LG', 'stat.ML']",111,82
An analytic theory of generalization dynamics and transfer learning in deep linear networks,"Much attention has been devoted recently to the generalization puzzle in deep
learning: large, deep networks can generalize well, but existing theories
bounding generalization error are exceedingly loose, and thus cannot explain
this striking performance. Furthermore, a major hope is that knowledge may
transfer across tasks, so that multi-task learning can improve generalization
on individual tasks. However we lack analytic theories that can quantitatively
predict how the degree of knowledge transfer depends on the relationship
between the tasks. We develop an analytic theory of the nonlinear dynamics of
generalization in deep linear networks, both within and across tasks. In
particular, our theory provides analytic solutions to the training and testing
error of deep networks as a function of training time, number of examples,
network size and initialization, and the task structure and SNR. Our theory
reveals that deep networks progressively learn the most important task
structure first, so that generalization error at the early stopping time
primarily depends on task structure and is independent of network size. This
suggests any tight bound on generalization error must take into account task
structure, and explains observations about real data being learned faster than
random data. Intriguingly our theory also reveals the existence of a learning
algorithm that proveably out-performs neural network training through gradient
descent. Finally, for transfer learning, our theory reveals that knowledge
transfer depends sensitively, but computably, on the SNRs and input feature
alignments of pairs of tasks.","['stat.ML', 'cs.LG', 'I.2.6; F.m']",240,139
Reinforcement Learning with Prototypical Representations,"Learning effective representations in image-based environments is crucial for
sample efficient Reinforcement Learning (RL). Unfortunately, in RL,
representation learning is confounded with the exploratory experience of the
agent -- learning a useful representation requires diverse data, while
effective exploration is only possible with coherent representations.
Furthermore, we would like to learn representations that not only generalize
across tasks but also accelerate downstream exploration for efficient
task-specific training. To address these challenges we propose Proto-RL, a
self-supervised framework that ties representation learning with exploration
through prototypical representations. These prototypes simultaneously serve as
a summarization of the exploratory experience of an agent as well as a basis
for representing observations. We pre-train these task-agnostic representations
and prototypes on environments without downstream task information. This
enables state-of-the-art downstream policy learning on a set of difficult
continuous control tasks.","['cs.LG', 'cs.AI']",143,92
Optical Flow Estimation from a Single Motion-blurred Image,"In most of computer vision applications, motion blur is regarded as an
undesirable artifact. However, it has been shown that motion blur in an image
may have practical interests in fundamental computer vision problems. In this
work, we propose a novel framework to estimate optical flow from a single
motion-blurred image in an end-to-end manner. We design our network with
transformer networks to learn globally and locally varying motions from encoded
features of a motion-blurred input, and decode left and right frame features
without explicit frame supervision. A flow estimator network is then used to
estimate optical flow from the decoded features in a coarse-to-fine manner. We
qualitatively and quantitatively evaluate our model through a large set of
experiments on synthetic and real motion-blur datasets. We also provide
in-depth analysis of our model in connection with related approaches to
highlight the effectiveness and favorability of our approach. Furthermore, we
showcase the applicability of the flow estimated by our method on deblurring
and moving object segmentation tasks.",['cs.CV'],174,109
A Partial Regularization Method for Network Compression,"Deep Neural Networks have achieved remarkable success relying on the
developing availability of GPUs and large-scale datasets with increasing
network depth and width. However, due to the expensive computation and
intensive memory, researchers have concentrated on designing compression
methods in order to make them practical for constrained platforms. In this
paper, we propose an approach of partial regularization rather than the
original form of penalizing all parameters, which is said to be full
regularization, to conduct model compression at a higher speed. It is
reasonable and feasible according to the existence of the permutation invariant
property of neural networks. Experimental results show that as we expected, the
computational complexity is reduced by observing less running time in almost
all situations. It should be owing to the fact that partial regularization
method invovles a lower number of elements for calculation. Surprisingly, it
helps to improve some important metrics such as regression fitting results and
classification accuracy in both training and test phases on multiple datasets,
telling us that the pruned models have better performance and generalization
ability. What's more, we analyze the results and draw a conclusion that an
optimal network structure must exist and depend on the input data.","['cs.LG', 'stat.ML']",201,143
Towards unconstrained joint hand-object reconstruction from RGB videos,"Our work aims to obtain 3D reconstruction of hands and manipulated objects
from monocular videos. Reconstructing hand-object manipulations holds a great
potential for robotics and learning from human demonstrations. The supervised
learning approach to this problem, however, requires 3D supervision and remains
limited to constrained laboratory settings and simulators for which 3D ground
truth is available. In this paper we first propose a learning-free fitting
approach for hand-object reconstruction which can seamlessly handle two-hand
object interactions. Our method relies on cues obtained with common methods for
object detection, hand pose estimation and instance segmentation. We
quantitatively evaluate our approach and show that it can be applied to
datasets with varying levels of difficulty for which training data is
unavailable.",['cs.CV'],123,88
Unified Vision-Language Pre-Training for Image Captioning and VQA,"This paper presents a unified Vision-Language Pre-training (VLP) model. The
model is unified in that (1) it can be fine-tuned for either vision-language
generation (e.g., image captioning) or understanding (e.g., visual question
answering) tasks, and (2) it uses a shared multi-layer transformer network for
both encoding and decoding, which differs from many existing methods where the
encoder and decoder are implemented using separate models. The unified VLP
model is pre-trained on a large amount of image-text pairs using the
unsupervised learning objectives of two tasks: bidirectional and
sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks
differ solely in what context the prediction conditions on. This is controlled
by utilizing specific self-attention masks for the shared transformer network.
To the best of our knowledge, VLP is the first reported model that achieves
state-of-the-art results on both vision-language generation and understanding
tasks, as disparate as image captioning and visual question answering, across
three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and
VQA 2.0. The code and the pre-trained models are available at
https://github.com/LuoweiZhou/VLP.",['cs.CV'],194,121
Micro-Batch Training with Batch-Channel Normalization and Weight Standardization,"Batch Normalization (BN) has become an out-of-box technique to improve deep
network training. However, its effectiveness is limited for micro-batch
training, i.e., each GPU typically has only 1-2 images for training, which is
inevitable for many computer vision tasks, e.g., object detection and semantic
segmentation, constrained by memory consumption. To address this issue, we
propose Weight Standardization (WS) and Batch-Channel Normalization (BCN) to
bring two success factors of BN into micro-batch training: 1) the smoothing
effects on the loss landscape and 2) the ability to avoid harmful elimination
singularities along the training trajectory. WS standardizes the weights in
convolutional layers to smooth the loss landscape by reducing the Lipschitz
constants of the loss and the gradients; BCN combines batch and channel
normalizations and leverages estimated statistics of the activations in
convolutional layers to keep networks away from elimination singularities. We
validate WS and BCN on comprehensive computer vision tasks, including image
classification, object detection, instance segmentation, video recognition and
semantic segmentation. All experimental results consistently show that WS and
BCN improve micro-batch training significantly. Moreover, using WS and BCN with
micro-batch training is even able to match or outperform the performances of BN
with large-batch training.","['cs.CV', 'cs.LG']",207,124
InSeGAN: A Generative Approach to Segmenting Identical Instances in Depth Images,"In this paper, we present InSeGAN, an unsupervised 3D generative adversarial
network (GAN) for segmenting (nearly) identical instances of rigid objects in
depth images. Using an analysis-by-synthesis approach, we design a novel GAN
architecture to synthesize a multiple-instance depth image with independent
control over each instance. InSeGAN takes in a set of code vectors (e.g.,
random noise vectors), each encoding the 3D pose of an object that is
represented by a learned implicit object template. The generator has two
distinct modules. The first module, the instance feature generator, uses each
encoded pose to transform the implicit template into a feature map
representation of each object instance. The second module, the depth image
renderer, aggregates all of the single-instance feature maps output by the
first module and generates a multiple-instance depth image. A discriminator
distinguishes the generated multiple-instance depth images from the
distribution of true depth images. To use our model for instance segmentation,
we propose an instance pose encoder that learns to take in a generated depth
image and reproduce the pose code vectors for all of the object instances. To
evaluate our approach, we introduce a new synthetic dataset, ""Insta-10"",
consisting of 100,000 depth images, each with 5 instances of an object from one
of 10 classes. Our experiments on Insta-10, as well as on real-world noisy
depth images, show that InSeGAN achieves state-of-the-art performance, often
outperforming prior methods by large margins.","['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']",247,134
Frequency-based Search-control in Dyna,"Model-based reinforcement learning has been empirically demonstrated as a
successful strategy to improve sample efficiency. In particular, Dyna is an
elegant model-based architecture integrating learning and planning that
provides huge flexibility of using a model. One of the most important
components in Dyna is called search-control, which refers to the process of
generating state or state-action pairs from which we query the model to acquire
simulated experiences. Search-control is critical in improving learning
efficiency. In this work, we propose a simple and novel search-control strategy
by searching high frequency regions of the value function. Our main intuition
is built on Shannon sampling theorem from signal processing, which indicates
that a high frequency signal requires more samples to reconstruct. We
empirically show that a high frequency function is more difficult to
approximate. This suggests a search-control strategy: we should use states from
high frequency regions of the value function to query the model to acquire more
samples. We develop a simple strategy to locally measure the frequency of a
function by gradient and hessian norms, and provide theoretical justification
for this approach. We then apply our strategy to search-control in Dyna, and
conduct experiments to show its property and effectiveness on benchmark
domains.","['cs.LG', 'cs.AI', 'stat.ML']",210,116
Center Emphasized Visual Saliency and a Contrast-based Full Reference Image Quality Index,"Objective image quality assessment (IQA) is imperative in the current
multimedia-intensive world, in order to assess the visual quality of an image
at close to a human level of ability. Many~parameters such as color intensity,
structure, sharpness, contrast, presence of an object, etc., draw human
attention to an image. Psychological vision research suggests that human vision
is biased to the center area of an image and display screen. As a result, if
the center part contains any visually salient information, it draws human
attention even more and any distortion in that part will be better perceived
than other parts. To the best of our knowledge, previous IQA methods have not
considered this fact. In this paper, we propose a full reference image quality
assessment (FR-IQA) approach using visual saliency and contrast; however, we
give extra attention to the center by increasing the sensitivity of the
similarity maps in that region. We evaluated our method on three large-scale
popular benchmark databases used by most of the current IQA researchers
(TID2008, CSIQ~and LIVE), having a total of 3345 distorted images with
28~different kinds of distortions. Our~method is compared with 13
state-of-the-art approaches. This comparison reveals the stronger correlation
of our method with human-evaluated values. The prediction-of-quality score is
consistent for distortion specific as well as distortion independent cases.
Moreover, faster processing makes it applicable to any real-time application.
The MATLAB code is publicly available to test the algorithm and can be found
online at http://layek.khu.ac.kr/CEQI.",['cs.CV'],262,171
Data-Driven Color Augmentation Techniques for Deep Skin Image Analysis,"Dermoscopic skin images are often obtained with different imaging devices,
under varying acquisition conditions. In this work, instead of attempting to
perform intensity and color normalization, we propose to leverage computational
color constancy techniques to build an artificial data augmentation technique
suitable for this kind of images. Specifically, we apply the \emph{shades of
gray} color constancy technique to color-normalize the entire training set of
images, while retaining the estimated illuminants. We then draw one sample from
the distribution of training set illuminants and apply it on the normalized
image. We employ this technique for training two deep convolutional neural
networks for the tasks of skin lesion segmentation and skin lesion
classification, in the context of the ISIC 2017 challenge and without using any
external dermatologic image set. Our results on the validation set are
promising, and will be supplemented with extended results on the hidden test
set when available.",['cs.CV'],151,98
Residual Attention Net for Superior Cross-Domain Time Sequence Modeling,"We present a novel architecture, residual attention net (RAN), which merges a
sequence architecture, universal transformer, and a computer vision
architecture, residual net, with a high-way architecture for cross-domain
sequence modeling. The architecture aims at addressing the long dependency
issue often faced by recurrent-neural-net-based structures. This paper serves
as a proof-of-concept for a new architecture, with RAN aiming at providing the
model a higher level understanding of sequence patterns. To our best knowledge,
we are the first to propose such an architecture. Out of the standard 85 UCR
data sets, we have achieved 35 state-of-the-art results with 10 results
matching current state-of-the-art results without further model fine-tuning.
The results indicate that such architecture is promising in complex,
long-sequence modeling and may have vast, cross-domain applications.","['cs.LG', 'cs.CV']",141,93
Spatial Transformer Networks for Curriculum Learning,"Curriculum learning is a bio-inspired training technique that is widely
adopted to machine learning for improved optimization and better training of
neural networks regarding the convergence rate or obtained accuracy. The main
concept in curriculum learning is to start the training with simpler tasks and
gradually increase the level of difficulty. Therefore, a natural question is
how to determine or generate these simpler tasks. In this work, we take
inspiration from Spatial Transformer Networks (STNs) in order to form an
easy-to-hard curriculum. As STNs have been proven to be capable of removing the
clutter from the input images and obtaining higher accuracy in image
classification tasks, we hypothesize that images processed by STNs can be seen
as easier tasks and utilized in the interest of curriculum learning. To this
end, we study multiple strategies developed for shaping the training
curriculum, using the data generated by STNs. We perform various experiments on
cluttered MNIST and Fashion-MNIST datasets, where on the former, we obtain an
improvement of $3.8$pp in classification accuracy compared to the baseline.",['cs.CV'],179,115
Think before you act: A simple baseline for compositional generalization,"Contrarily to humans who have the ability to recombine familiar expressions
to create novel ones, modern neural networks struggle to do so. This has been
emphasized recently with the introduction of the benchmark dataset ""gSCAN""
(Ruis et al. 2020), aiming to evaluate models' performance at compositional
generalization in grounded language understanding. In this work, we challenge
the gSCAN benchmark by proposing a simple model that achieves surprisingly good
performance on two of the gSCAN test splits. Our model is based on the
observation that, to succeed on gSCAN tasks, the agent must (i) identify the
target object (think) before (ii) navigating to it successfully (act).
Concretely, we propose an attention-inspired modification of the baseline model
from (Ruis et al. 2020), together with an auxiliary loss, that takes into
account the sequential nature of steps (i) and (ii). While two compositional
tasks are trivially solved with our approach, we also find that the other tasks
remain unsolved, validating the relevance of gSCAN as a benchmark for
evaluating models' compositional abilities.","['cs.LG', 'stat.ML']",170,117
3DCapsule: Extending the Capsule Architecture to Classify 3D Point Clouds,"This paper introduces the 3DCapsule, which is a 3D extension of the recently
introduced Capsule concept that makes it applicable to unordered point sets.
The original Capsule relies on the existence of a spatial relationship between
the elements in the feature map it is presented with, whereas in point
permutation invariant formulations of 3D point set classification methods, such
relationships are typically lost. Here, a new layer called ComposeCaps is
introduced that, in lieu of a spatially relevant feature mapping, learns a new
mapping that can be exploited by the 3DCapsule. Previous works in the 3D point
set classification domain have focused on other parts of the architecture,
whereas instead, the 3DCapsule is a drop-in replacement of the commonly used
fully connected classifier. It is demonstrated via an ablation study, that when
the 3DCapsule is applied to recent 3D point set classification architectures,
it consistently shows an improvement, in particular when subjected to noisy
data. Similarly, the ComposeCaps layer is evaluated and demonstrates an
improvement over the baseline. In an apples-to-apples comparison against
state-of-the-art methods, again, better performance is demonstrated by the
3DCapsule.",['cs.CV'],189,111
Logic Explained Networks,"The large and still increasing popularity of deep learning clashes with a
major limit of neural network architectures, that consists in their lack of
capability in providing human-understandable motivations of their decisions. In
situations in which the machine is expected to support the decision of human
experts, providing a comprehensible explanation is a feature of crucial
importance. The language used to communicate the explanations must be formal
enough to be implementable in a machine and friendly enough to be
understandable by a wide audience. In this paper, we propose a general approach
to Explainable Artificial Intelligence in the case of neural architectures,
showing how a mindful design of the networks leads to a family of interpretable
deep learning models called Logic Explained Networks (LENs). LENs only require
their inputs to be human-understandable predicates, and they provide
explanations in terms of simple First-Order Logic (FOL) formulas involving such
predicates. LENs are general enough to cover a large number of scenarios.
Amongst them, we consider the case in which LENs are directly used as special
classifiers with the capability of being explainable, or when they act as
additional networks with the role of creating the conditions for making a
black-box classifier explainable by FOL formulas. Despite supervised learning
problems are mostly emphasized, we also show that LENs can learn and provide
explanations in unsupervised learning settings. Experimental results on several
datasets and tasks show that LENs may yield better classifications than
established white-box models, such as decision trees and Bayesian rule lists,
while providing more compact and meaningful explanations.","['cs.LG', 'cs.AI', 'cs.LO', 'cs.NE']",262,152
Object detection for crabs in top-view seabed imagery,"This report presents the application of object detection on a database of
underwater images of different species of crabs, as well as aerial images of
sea lions and finally the Pascal VOC dataset. The model is an end-to-end object
detection neural network based on a convolutional network base and a Long
Short-Term Memory detector.","['cs.CV', 'cs.LG']",57,42
Segmentation for radar images based on active contour,"We exam various geometric active contour methods for radar image
segmentation. Due to special properties of radar images, we propose our new
model based on modified Chan-Vese functional. Our method is efficient in
separating non-meteorological noises from meteorological images.",['cs.CV'],41,38
Benchmarking time series classification -- Functional data vs machine learning approaches,"Time series classification problems have drawn increasing attention in the
machine learning and statistical community. Closely related is the field of
functional data analysis (FDA): it refers to the range of problems that deal
with the analysis of data that is continuously indexed over some domain. While
often employing different methods, both fields strive to answer similar
questions, a common example being classification or regression problems with
functional covariates. We study methods from functional data analysis, such as
functional generalized additive models, as well as functionality to concatenate
(functional-) feature extraction or basis representations with traditional
machine learning algorithms like support vector machines or classification
trees. In order to assess the methods and implementations, we run a benchmark
on a wide variety of representative (time series) data sets, with in-depth
analysis of empirical results, and strive to provide a reference ranking for
which method(s) to use for non-expert practitioners. Additionally, we provide a
software framework in R for functional data analysis for supervised learning,
including machine learning and more linear approaches from statistics. This
allows convenient access, and in connection with the machine-learning toolbox
mlr, those methods can now also be tuned and benchmarked.","['stat.ML', 'cs.LG']",198,127
Data augmentation for low resource sentiment analysis using generative adversarial networks,"Sentiment analysis is a task that may suffer from a lack of data in certain
cases, as the datasets are often generated and annotated by humans. In cases
where data is inadequate for training discriminative models, generate models
may aid training via data augmentation. Generative Adversarial Networks (GANs)
are one such model that has advanced the state of the art in several tasks,
including as image and text generation. In this paper, I train GAN models on
low resource datasets, then use them for the purpose of data augmentation
towards improving sentiment classifier generalization. Given the constraints of
limited data, I explore various techniques to train the GAN models. I also
present an analysis of the quality of generated GAN data as more training data
for the GAN is made available. In this analysis, the generated data is
evaluated as a test set (against a model trained on real data points) as well
as a training set to train classification models. Finally, I also conduct a
visual analysis by projecting the generated and the real data into a
two-dimensional space using the t-Distributed Stochastic Neighbor Embedding
(t-SNE) method.","['cs.LG', 'stat.ML']",191,109
PAC Learning Guarantees Under Covariate Shift,"We consider the Domain Adaptation problem, also known as the covariate shift
problem, where the distributions that generate the training and test data
differ while retaining the same labeling function. This problem occurs across a
large range of practical applications, and is related to the more general
challenge of transfer learning. Most recent work on the topic focuses on
optimization techniques that are specific to an algorithm or practical use case
rather than a more general approach. The sparse literature attempting to
provide general bounds seems to suggest that efficient learning even under
strong assumptions is not possible for covariate shift. Our main contribution
is to recontextualize these results by showing that any Probably Approximately
Correct (PAC) learnable concept class is still PAC learnable under covariate
shift conditions with only a polynomial increase in the number of training
samples. This approach essentially demonstrates that the Domain Adaptation
learning problem is as hard as the underlying PAC learning problem, provided
some conditions over the training and test distributions. We also present
bounds for the rejection sampling algorithm, justifying it as a solution to the
Domain Adaptation problem in certain scenarios.","['cs.LG', 'stat.ML']",189,116
Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models,"We extend the task of composed image retrieval, where an input query consists
of an image and short textual description of how to modify the image. Existing
methods have only been applied to non-complex images within narrow domains,
such as fashion products, thereby limiting the scope of study on in-depth
visual reasoning in rich image and language contexts. To address this issue, we
collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which
consists of over 36,000 pairs of crowd-sourced, open-domain images with
human-generated modifying text. To extend current methods to the open-domain,
we propose CIRPLANT, a transformer based model that leverages rich pre-trained
vision-and-language (V&L) knowledge for modifying visual features conditioned
on natural language. Retrieval is then done by nearest neighbor lookup on the
modified features. We demonstrate that with a relatively simple architecture,
CIRPLANT outperforms existing methods on open-domain images, while matching
state-of-the-art accuracy on the existing narrow datasets, such as fashion.
Together with the release of CIRR, we believe this work will inspire further
research on composed image retrieval.","['cs.CV', 'cs.CL', 'cs.IR']",189,122
Crowdfunding Dynamics Tracking: A Reinforcement Learning Approach,"Recent years have witnessed the increasing interests in research of
crowdfunding mechanism. In this area, dynamics tracking is a significant issue
but is still under exploration. Existing studies either fit the fluctuations of
time-series or employ regularization terms to constrain learned tendencies.
However, few of them take into account the inherent decision-making process
between investors and crowdfunding dynamics. To address the problem, in this
paper, we propose a Trajectory-based Continuous Control for Crowdfunding (TC3)
algorithm to predict the funding progress in crowdfunding. Specifically,
actor-critic frameworks are employed to model the relationship between
investors and campaigns, where all of the investors are viewed as an agent that
could interact with the environment derived from the real dynamics of
campaigns. Then, to further explore the in-depth implications of patterns
(i.e., typical characters) in funding series, we propose to subdivide them into
$\textit{fast-growing}$ and $\textit{slow-growing}$ ones. Moreover, for the
purpose of switching from different kinds of patterns, the actor component of
TC3 is extended with a structure of options, which comes to the TC3-Options.
Finally, extensive experiments on the Indiegogo dataset not only demonstrate
the effectiveness of our methods, but also validate our assumption that the
entire pattern learned by TC3-Options is indeed the U-shaped one.","['cs.LG', 'cs.SI', 'stat.ML']",216,143
STAR: A Structure and Texture Aware Retinex Model,"Retinex theory is developed mainly to decompose an image into the
illumination and reflectance components by analyzing local image derivatives.
In this theory, larger derivatives are attributed to the changes in
reflectance, while smaller derivatives are emerged in the smooth illumination.
In this paper, we utilize exponentiated local derivatives (with an exponent
{\gamma}) of an observed image to generate its structure map and texture map.
The structure map is produced by been amplified with {\gamma} > 1, while the
texture map is generated by been shrank with {\gamma} < 1. To this end, we
design exponential filters for the local derivatives, and present their
capability on extracting accurate structure and texture maps, influenced by the
choices of exponents {\gamma}. The extracted structure and texture maps are
employed to regularize the illumination and reflectance components in Retinex
decomposition. A novel Structure and Texture Aware Retinex (STAR) model is
further proposed for illumination and reflectance decomposition of a single
image. We solve the STAR model by an alternating optimization algorithm. Each
sub-problem is transformed into a vectorized least squares regression, with
closed-form solutions. Comprehensive experiments on commonly tested datasets
demonstrate that, the proposed STAR model produce better quantitative and
qualitative performance than previous competing methods, on illumination and
reflectance decomposition, low-light image enhancement, and color correction.
The code is publicly available at https://github.com/csjunxu/STAR.",['cs.CV'],226,127
Resolution Enhancement of Range Images via Color-Image Segmentation,"We report a method for super-resolution of range images. Our approach
leverages the interpretation of LR image as sparse samples on the HR grid.
Based on this interpretation, we demonstrate that our recently reported
approach, which reconstructs dense range images from sparse range data by
exploiting a registered colour image, can be applied for the task of resolution
enhancement of range images. Our method only uses a single colour image in
addition to the range observation in the super-resolution process. Using the
proposed approach, we demonstrate super-resolution results for large factors
(e.g. 4) with good localization accuracy.",['cs.CV'],101,65
Demystification of Few-shot and One-shot Learning,"Few-shot and one-shot learning have been the subject of active and intensive
research in recent years, with mounting evidence pointing to successful
implementation and exploitation of few-shot learning algorithms in practice.
Classical statistical learning theories do not fully explain why few- or
one-shot learning is at all possible since traditional generalisation bounds
normally require large training and testing samples to be meaningful. This
sharply contrasts with numerous examples of successful one- and few-shot
learning systems and applications.
  In this work we present mathematical foundations for a theory of one-shot and
few-shot learning and reveal conditions specifying when such learning schemes
are likely to succeed. Our theory is based on intrinsic properties of
high-dimensional spaces. We show that if the ambient or latent decision space
of a learning machine is sufficiently high-dimensional than a large class of
objects in this space can indeed be easily learned from few examples provided
that certain data non-concentration conditions are met.","['cs.LG', 'cs.AI', 'math.ST', 'stat.TH', '68T05, 68T07']",166,110
A Survey on Contrastive Self-supervised Learning,"Self-supervised learning has gained popularity because of its ability to
avoid the cost of annotating large-scale datasets. It is capable of adopting
self-defined pseudo labels as supervision and use the learned representations
for several downstream tasks. Specifically, contrastive learning has recently
become a dominant component in self-supervised learning methods for computer
vision, natural language processing (NLP), and other domains. It aims at
embedding augmented versions of the same sample close to each other while
trying to push away embeddings from different samples. This paper provides an
extensive review of self-supervised methods that follow the contrastive
approach. The work explains commonly used pretext tasks in a contrastive
learning setup, followed by different architectures that have been proposed so
far. Next, we have a performance comparison of different methods for multiple
downstream tasks such as image classification, object detection, and action
recognition. Finally, we conclude with the limitations of the current methods
and the need for further techniques and future directions to make substantial
progress.",['cs.CV'],168,119
Audiomer: A Convolutional Transformer for Keyword Spotting,"Transformers have seen an unprecedented rise in Natural Language Processing
and Computer Vision tasks. However, in audio tasks, they are either infeasible
to train due to extremely large sequence length of audio waveforms or reach
competitive performance after feature extraction through Fourier-based methods,
incurring a loss-floor. In this work, we introduce an architecture, Audiomer,
where we combine 1D Residual Networks with Performer Attention to achieve
state-of-the-art performance in Keyword Spotting with raw audio waveforms,
out-performing all previous methods while also being computationally cheaper,
much more parameter and data-efficient. Audiomer allows for deployment in
compute-constrained devices and training on smaller datasets.","['cs.LG', 'cs.CL', 'cs.SD', 'eess.AS']",108,90
AU-Expression Knowledge Constrained Representation Learning for Facial Expression Recognition,"Recognizing human emotion/expressions automatically is quite an expected
ability for intelligent robotics, as it can promote better communication and
cooperation with humans. Current deep-learning-based algorithms may achieve
impressive performance in some lab-controlled environments, but they always
fail to recognize the expressions accurately for the uncontrolled in-the-wild
situation. Fortunately, facial action units (AU) describe subtle facial
behaviors, and they can help distinguish uncertain and ambiguous expressions.
In this work, we explore the correlations among the action units and facial
expressions, and devise an AU-Expression Knowledge Constrained Representation
Learning (AUE-CRL) framework to learn the AU representations without AU
annotations and adaptively use representations to facilitate facial expression
recognition. Specifically, it leverages AU-expression correlations to guide the
learning of the AU classifiers, and thus it can obtain AU representations
without incurring any AU annotations. Then, it introduces a knowledge-guided
attention mechanism that mines useful AU representations under the constraint
of AU-expression correlations. In this way, the framework can capture local
discriminative and complementary features to enhance facial representation for
facial expression recognition. We conduct experiments on the challenging
uncontrolled datasets to demonstrate the superiority of the proposed framework
over current state-of-the-art methods. Codes and trained models are available
at https://github.com/HCPLab-SYSU/AUE-CRL.",['cs.CV'],217,139
Joint Learning of Distributed Representations for Images and Texts,"This technical report provides extra details of the deep multimodal
similarity model (DMSM) which was proposed in (Fang et al. 2015,
arXiv:1411.4952). The model is trained via maximizing global semantic
similarity between images and their captions in natural language using the
public Microsoft COCO database, which consists of a large set of images and
their corresponding captions. The learned representations attempt to capture
the combination of various visual concepts and cues.",['cs.CV'],73,58
3D attention mechanism for fine-grained classification of table tennis strokes using a Twin Spatio-Temporal Convolutional Neural Networks,"The paper addresses the problem of recognition of actions in video with low
inter-class variability such as Table Tennis strokes. Two stream, ""twin""
convolutional neural networks are used with 3D convolutions both on RGB data
and optical flow. Actions are recognized by classification of temporal windows.
We introduce 3D attention modules and examine their impact on classification
efficiency. In the context of the study of sportsmen performances, a corpus of
the particular actions of table tennis strokes is considered. The use of
attention blocks in the network speeds up the training step and improves the
classification scores up to 5% with our twin model. We visualize the impact on
the obtained features and notice correlation between attention and player
movements and position. Score comparison of state-of-the-art action
classification method and proposed approach with attentional blocks is
performed on the corpus. Proposed model with attention blocks outperforms
previous model without them and our baseline.","['cs.CV', 'cs.HC', 'cs.LG', 'cs.MM']",157,101
Discovering Salient Anatomical Landmarks by Predicting Human Gaze,"Anatomical landmarks are a crucial prerequisite for many medical imaging
tasks. Usually, the set of landmarks for a given task is predefined by experts.
The landmark locations for a given image are then annotated manually or via
machine learning methods trained on manual annotations. In this paper, in
contrast, we present a method to automatically discover and localize anatomical
landmarks in medical images. Specifically, we consider landmarks that attract
the visual attention of humans, which we term visually salient landmarks. We
illustrate the method for fetal neurosonographic images. First, full-length
clinical fetal ultrasound scans are recorded with live sonographer
gaze-tracking. Next, a convolutional neural network (CNN) is trained to predict
the gaze point distribution (saliency map) of the sonographers on scan video
frames. The CNN is then used to predict saliency maps of unseen fetal
neurosonographic images, and the landmarks are extracted as the local maxima of
these saliency maps. Finally, the landmarks are matched across images by
clustering the landmark CNN features. We show that the discovered landmarks can
be used within affine image registration, with average landmark alignment
errors between 4.1% and 10.9% of the fetal head long axis length.","['cs.CV', 'cs.LG', 'eess.IV']",196,124
Online Learning for Offloading and Autoscaling in Energy Harvesting Mobile Edge Computing,"Mobile edge computing (a.k.a. fog computing) has recently emerged to enable
in-situ processing of delay-sensitive applications at the edge of mobile
networks. Providing grid power supply in support of mobile edge computing,
however, is costly and even infeasible (in certain rugged or under-developed
areas), thus mandating on-site renewable energy as a major or even sole power
supply in increasingly many scenarios. Nonetheless, the high intermittency and
unpredictability of renewable energy make it very challenging to deliver a high
quality of service to users in energy harvesting mobile edge computing systems.
In this paper, we address the challenge of incorporating renewables into mobile
edge computing and propose an efficient reinforcement learning-based resource
management algorithm, which learns on-the-fly the optimal policy of dynamic
workload offloading (to the centralized cloud) and edge server provisioning to
minimize the long-term system cost (including both service delay and
operational cost). Our online learning algorithm uses a decomposition of the
(offline) value iteration and (online) reinforcement learning, thus achieving a
significant improvement of learning rate and run-time performance when compared
to standard reinforcement learning algorithms such as Q-learning. We prove the
convergence of the proposed algorithm and analytically show that the learned
policy has a simple monotone structure amenable to practical implementation.
Our simulation results validate the efficacy of our algorithm, which
significantly improves the edge computing performance compared to fixed or
myopic optimization schemes and conventional reinforcement learning algorithms.","['cs.LG', 'cs.NI']",245,149
Graph Context Encoder: Graph Feature Inpainting for Graph Generation and Self-supervised Pretraining,"We propose the Graph Context Encoder (GCE), a simple but efficient approach
for graph representation learning based on graph feature masking and
reconstruction.
  GCE models are trained to efficiently reconstruct input graphs similarly to a
graph autoencoder where node and edge labels are masked. In particular, our
model is also allowed to change graph structures by masking and reconstructing
graphs augmented by random pseudo-edges.
  We show that GCE can be used for novel graph generation, with applications
for molecule generation. Used as a pretraining method, we also show that GCE
improves baseline performances in supervised classification tasks tested on
multiple standard benchmark graph datasets.","['cs.LG', '68T07']",105,79
Spirit Distillation: A Model Compression Method with Multi-domain Knowledge Transfer,"Recent applications pose requirements of both cross-domain knowledge transfer
and model compression to machine learning models due to insufficient training
data and limited computational resources. In this paper, we propose a new
knowledge distillation model, named Spirit Distillation (SD), which is a model
compression method with multi-domain knowledge transfer. The compact student
network mimics out a representation equivalent to the front part of the teacher
network, through which the general knowledge can be transferred from the source
domain (teacher) to the target domain (student). To further improve the
robustness of the student, we extend SD to Enhanced Spirit Distillation (ESD)
in exploiting a more comprehensive knowledge by introducing the proximity
domain which is similar to the target domain for feature extraction. Results
demonstrate that our method can boost mIOU and high-precision accuracy by 1.4%
and 8.2% respectively with 78.2% segmentation variance, and can gain a precise
compact network with only 41.8% FLOPs.",['cs.CV'],159,101
Exploring Contrastive Learning in Human Activity Recognition for Healthcare,"Human Activity Recognition (HAR) constitutes one of the most important tasks
for wearable and mobile sensing given its implications in human well-being and
health monitoring. Motivated by the limitations of labeled datasets in HAR,
particularly when employed in healthcare-related applications, this work
explores the adoption and adaptation of SimCLR, a contrastive learning
technique for visual representations, to HAR. The use of contrastive learning
objectives causes the representations of corresponding views to be more
similar, and those of non-corresponding views to be more different. After an
extensive evaluation exploring 64 combinations of different signal
transformations for augmenting the data, we observed significant performance
differences owing to the order and the function thereof. In particular,
preliminary results indicated an improvement over supervised and unsupervised
learning methods when using fine-tuning and random rotation for augmentation,
however, future work should explore under which conditions SimCLR is beneficial
for HAR systems and other healthcare-related applications.","['cs.LG', 'eess.SP']",155,108
Two-level Attention with Two-stage Multi-task Learning for Facial Emotion Recognition,"Compared with facial emotion recognition on categorical model, the
dimensional emotion recognition can describe numerous emotions of the real
world more accurately. Most prior works of dimensional emotion estimation only
considered laboratory data and used video, speech or other multi-modal
features. The effect of these methods applied on static images in the real
world is unknown. In this paper, a two-level attention with two-stage
multi-task learning (2Att-2Mt) framework is proposed for facial emotion
estimation on only static images. Firstly, the features of corresponding
region(position-level features) are extracted and enhanced automatically by
first-level attention mechanism. In the following, we utilize Bi-directional
Recurrent Neural Network(Bi-RNN) with self-attention(second-level attention) to
make full use of the relationship features of different layers(layer-level
features) adaptively. Owing to the inherent complexity of dimensional emotion
recognition, we propose a two-stage multi-task learning structure to exploited
categorical representations to ameliorate the dimensional representations and
estimate valence and arousal simultaneously in view of the correlation of the
two targets. The quantitative results conducted on AffectNet dataset show
significant advancement on Concordance Correlation Coefficient(CCC) and Root
Mean Square Error(RMSE), illustrating the superiority of the proposed
framework. Besides, extensive comparative experiments have also fully
demonstrated the effectiveness of different components.",['cs.CV'],218,136
Large-scale image segmentation based on distributed clustering algorithms,"Many approaches to 3D image segmentation are based on hierarchical clustering
of supervoxels into image regions. Here we describe a distributed algorithm
capable of handling a tremendous number of supervoxels. The algorithm works
recursively, the regions are divided into chunks that are processed
independently in parallel by multiple workers. At each round of the recursive
procedure, the chunk size in all dimensions are doubled until a single chunk
encompasses the entire image. The final result is provably independent of the
chunking scheme, and the same as if the entire image were processed without
division into chunks. This is nontrivial because a pair of adjacent regions is
scored by some statistical property (e.g. mean or median) of the affinities at
the interface, and the interface may extend over arbitrarily many chunks. The
trick is to delay merge decisions for regions that touch chunk boundaries, and
only complete them in a later round after the regions are fully contained
within a chunk. We demonstrate the algorithm by clustering an affinity graph
with over 1.5 trillion edges between 135 billion supervoxels derived from a 3D
electron microscopic brain image.",['cs.CV'],188,122
Weakly-Supervised Semantic Segmentation using Motion Cues,"Fully convolutional neural networks (FCNNs) trained on a large number of
images with strong pixel-level annotations have become the new state of the art
for the semantic segmentation task. While there have been recent attempts to
learn FCNNs from image-level weak annotations, they need additional
constraints, such as the size of an object, to obtain reasonable performance.
To address this issue, we present motion-CNN (M-CNN), a novel FCNN framework
which incorporates motion cues and is learned from video-level weak
annotations. Our learning scheme to train the network uses motion segments as
soft constraints, thereby handling noisy motion information. When trained on
weakly-annotated videos, our method outperforms the state-of-the-art EM-Adapt
approach on the PASCAL VOC 2012 image segmentation benchmark. We also
demonstrate that the performance of M-CNN learned with 150 weak video
annotations is on par with state-of-the-art weakly-supervised methods trained
with thousands of images. Finally, M-CNN substantially outperforms recent
approaches in a related task of video co-localization on the YouTube-Objects
dataset.",['cs.CV'],179,113
Multi-view Low-rank Preserving Embedding: A Novel Method for Multi-view Representation,"In recent years, we have witnessed a surge of interest in multi-view
representation learning, which is concerned with the problem of learning
representations of multi-view data. When facing multiple views that are highly
related but sightly different from each other, most of existing multi-view
methods might fail to fully integrate multi-view information. Besides,
correlations between features from multiple views always vary seriously, which
makes multi-view representation challenging. Therefore, how to learn
appropriate embedding from multi-view information is still an open problem but
challenging. To handle this issue, this paper proposes a novel multi-view
learning method, named Multi-view Low-rank Preserving Embedding (MvLPE). It
integrates different views into one centroid view by minimizing the
disagreement term, based on distance or similarity matrix among instances,
between the centroid view and each view meanwhile maintaining low-rank
reconstruction relations among samples for each view, which could make more
full use of compatible and complementary information from multi-view features.
Unlike existing methods with additive parameters, the proposed method could
automatically allocate a suitable weight for each view in multi-view
information fusion. However, MvLPE couldn't be directly solved, which makes the
proposed MvLPE difficult to obtain an analytic solution. To this end, we
approximate this solution based on stationary hypothesis and normalization
post-processing to efficiently obtain the optimal solution. Furthermore, an
iterative alternating strategy is provided to solve this multi-view
representation problem. The experiments on six benchmark datasets demonstrate
that the proposed method outperforms its counterparts while achieving very
competitive performance.","['cs.LG', 'cs.CV', 'stat.ML']",259,158
Automatic extraction of road intersection points from USGS historical map series using deep convolutional neural networks,"Road intersections data have been used across different geospatial
applications and analysis. The road network datasets dating from pre-GIS years
are only available in the form of historical printed maps. Before they can be
analyzed by a GIS software, they need to be scanned and transformed into the
usable vector-based format. Due to the great bulk of scanned historical maps,
automated methods of transforming them into digital datasets need to be
employed. Frequently, this process is based on computer vision algorithms.
However, low conversion accuracy for low quality and visually complex maps and
setting optimal parameters are the two challenges of using those algorithms. In
this paper, we employed the standard paradigm of using deep convolutional
neural network for object detection task named region-based CNN for
automatically identifying road intersections in scanned historical USGS maps of
several U.S. cities. We have found that the algorithm showed higher conversion
accuracy for the double line cartographic representations of the road maps than
the single line ones. Also, compared to the majority of traditional computer
vision algorithms RCNN provides more accurate extraction. Finally, the results
show that the amount of errors in the detection outputs is sensitive to
complexity and blurriness of the maps as well as the number of distinct RGB
combinations within them.","['cs.CV', 'I.4']",216,139
Interpretable Control by Reinforcement Learning,"In this paper, three recently introduced reinforcement learning (RL) methods
are used to generate human-interpretable policies for the cart-pole balancing
benchmark. The novel RL methods learn human-interpretable policies in the form
of compact fuzzy controllers and simple algebraic equations. The
representations as well as the achieved control performances are compared with
two classical controller design methods and three non-interpretable RL methods.
All eight methods utilize the same previously generated data batch and produce
their controller offline - without interaction with the real benchmark
dynamics. The experiments show that the novel RL methods are able to
automatically generate well-performing policies which are at the same time
human-interpretable. Furthermore, one of the methods is applied to
automatically learn an equation-based policy for a hardware cart-pole
demonstrator by using only human-player-generated batch data. The solution
generated in the first attempt already represents a successful balancing
policy, which demonstrates the methods applicability to real-world problems.","['cs.LG', 'cs.AI', 'cs.RO', 'cs.SC', 'cs.SY', 'eess.SY']",161,98
Graph-Based Neural Network Models with Multiple Self-Supervised Auxiliary Tasks,"Self-supervised learning is currently gaining a lot of attention, as it
allows neural networks to learn robust representations from large quantities of
unlabeled data. Additionally, multi-task learning can further improve
representation learning by training networks simultaneously on related tasks,
leading to significant performance improvements. In this paper, we propose
three novel self-supervised auxiliary tasks to train graph-based neural network
models in a multi-task fashion. Since Graph Convolutional Networks are among
the most promising approaches for capturing relationships among structured data
points, we use them as a building block to achieve competitive results on
standard semi-supervised graph classification tasks.",['cs.LG'],104,82
Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features,"Accurate segmentation of different sub-regions of gliomas including
peritumoral edema, necrotic core, enhancing and non-enhancing tumor core from
multimodal MRI scans has important clinical relevance in diagnosis, prognosis
and treatment of brain tumors. However, due to the highly heterogeneous
appearance and shape, segmentation of the sub-regions is very challenging.
Recent development using deep learning models has proved its effectiveness in
the past several brain segmentation challenges as well as other semantic and
medical image segmentation problems. Most models in brain tumor segmentation
use a 2D/3D patch to predict the class label for the center voxel and variant
patch sizes and scales are used to improve the model performance. However, it
has low computation efficiency and also has limited receptive field. U-Net is a
widely used network structure for end-to-end segmentation and can be used on
the entire image or extracted patches to provide classification labels over the
entire input voxels so that it is more efficient and expect to yield better
performance with larger input size. Furthermore, instead of picking the best
network structure, an ensemble of multiple models, trained on different dataset
or different hyper-parameters, can generally improve the segmentation
performance. In this study we propose to use an ensemble of 3D U-Nets with
different hyper-parameters for brain tumor segmentation. Preliminary results
showed effectiveness of this model. In addition, we developed a linear model
for survival prediction using extracted imaging and non-imaging features,
which, despite the simplicity, can effectively reduce overfitting and
regression errors.",['cs.CV'],256,153
Fully Bayesian Recurrent Neural Networks for Safe Reinforcement Learning,"Reinforcement Learning (RL) has demonstrated state-of-the-art results in a
number of autonomous system applications, however many of the underlying
algorithms rely on black-box predictions. This results in poor explainability
of the behaviour of these systems, raising concerns as to their use in
safety-critical applications. Recent work has demonstrated that
uncertainty-aware models exhibit more cautious behaviours through the
incorporation of model uncertainty estimates. In this work, we build on
Probabilistic Backpropagation to introduce a fully Bayesian Recurrent Neural
Network architecture. We apply this within a Safe RL scenario, and demonstrate
that the proposed method significantly outperforms a popular approach for
obtaining model uncertainties in collision avoidance tasks. Furthermore, we
demonstrate that the proposed approach requires less training and is far more
efficient than the current leading method, both in terms of compute resource
and memory footprint.","['cs.LG', 'stat.ML']",141,101
Using Autoencoders To Learn Interesting Features For Detecting Surveillance Aircraft,"This paper explores using a Long short-term memory (LSTM) based sequence
autoencoder to learn interesting features for detecting surveillance aircraft
using ADS-B flight data. An aircraft periodically broadcasts ADS-B (Automatic
Dependent Surveillance - Broadcast) data to ground receivers. The ability of
LSTM networks to model varying length time series data and remember
dependencies that span across events makes it an ideal candidate for
implementing a sequence autoencoder for ADS-B data because of its possible
variable length time series, irregular sampling and dependencies that span
across events.","['cs.LG', 'stat.ML']",89,62
Residual Non-local Attention Networks for Image Restoration,"In this paper, we propose a residual non-local attention network for
high-quality image restoration. Without considering the uneven distribution of
information in the corrupted images, previous methods are restricted by local
convolutional operation and equal treatment of spatial- and channel-wise
features. To address this issue, we design local and non-local attention blocks
to extract features that capture the long-range dependencies between pixels and
pay more attention to the challenging parts. Specifically, we design trunk
branch and (non-)local mask branch in each (non-)local attention block. The
trunk branch is used to extract hierarchical features. Local and non-local mask
branches aim to adaptively rescale these hierarchical features with mixed
attentions. The local mask branch concentrates on more local structures with
convolutional operations, while non-local attention considers more about
long-range dependencies in the whole feature map. Furthermore, we propose
residual local and non-local attention learning to train the very deep network,
which further enhance the representation ability of the network. Our proposed
method can be generalized for various image restoration applications, such as
image denoising, demosaicing, compression artifacts reduction, and
super-resolution. Experiments demonstrate that our method obtains comparable or
better results compared with recently leading methods quantitatively and
visually.",['cs.CV'],208,127
Real-time monitoring of driver drowsiness on mobile platforms using 3D neural networks,"Driver drowsiness increases crash risk, leading to substantial road trauma
each year. Drowsiness detection methods have received considerable attention,
but few studies have investigated the implementation of a detection approach on
a mobile phone. Phone applications reduce the need for specialised hardware and
hence, enable a cost-effective roll-out of the technology across the driving
population. While it has been shown that three-dimensional (3D) operations are
more suitable for spatiotemporal feature learning, current methods for
drowsiness detection commonly use frame-based, multi-step approaches. However,
computationally expensive techniques that achieve superior results on action
recognition benchmarks (e.g. 3D convolutions, optical flow extraction) create
bottlenecks for real-time, safety-critical applications on mobile devices.
Here, we show how depthwise separable 3D convolutions, combined with an early
fusion of spatial and temporal information, can achieve a balance between high
prediction accuracy and real-time inference requirements. In particular,
increased accuracy is achieved when assessment requires motion information, for
example, when sunglasses conceal the eyes. Further, a custom TensorFlow-based
smartphone application shows the true impact of various approaches on inference
times and demonstrates the effectiveness of real-time monitoring based on
out-of-sample data to alert a drowsy driver. Our model is pre-trained on
ImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness
Detection dataset. Fine-tuning on large naturalistic driving datasets could
further improve accuracy to obtain robust in-vehicle performance. Overall, our
research is a step towards practical deep learning applications, potentially
preventing micro-sleeps and reducing road trauma.","['cs.CV', 'stat.ML', '68T45 (Primary) 68U10 (Secondary)', 'I.4.9; I.4.8; I.2.10']",257,185
Graph-Embedded Subspace Support Vector Data Description,"In this paper, we propose a novel subspace learning framework for one-class
classification. The proposed framework presents the problem in the form of
graph embedding. It includes the previously proposed subspace one-class
techniques as its special cases and provides further insight on what these
techniques actually optimize. The framework allows to incorporate other
meaningful optimization goals via the graph preserving criterion and reveals
spectral and spectral regression-based solutions as alternatives to the
previously used gradient-based technique. We combine the subspace learning
framework iteratively with Support Vector Data Description applied in the
subspace to formulate Graph-Embedded Subspace Support Vector Data Description.
We experimentally analyzed the performance of newly proposed different
variants. We demonstrate improved performance against the baselines and the
recently proposed subspace learning methods for one-class classification.",['cs.LG'],133,85
Tensor-Train Networks for Learning Predictive Modeling of Multidimensional Data,"In this work, we firstly apply the Train-Tensor (TT) networks to construct a
compact representation of the classical Multilayer Perceptron, representing a
reduction of up to 95% of the coefficients. A comparative analysis between
tensor model and standard multilayer neural networks is also carried out in the
context of prediction of the Mackey-Glass noisy chaotic time series and NASDAQ
index. We show that the weights of a multidimensional regression model can be
learned by means of TT network and the optimization of TT weights is a more
robust to the impact of coefficient initialization and hyper-parameter setting.
Furthermore, an efficient algorithm based on alternating least squares has been
proposed for approximating the weights in TT-format with a reduction of
computational calculus, providing a much faster convergence than the well-known
adaptive learning-method algorithms, widely applied for optimizing neural
networks.",['cs.LG'],144,103
Linguistically Driven Graph Capsule Network for Visual Question Reasoning,"Recently, studies of visual question answering have explored various
architectures of end-to-end networks and achieved promising results on both
natural and synthetic datasets, which require explicitly compositional
reasoning. However, it has been argued that these black-box approaches lack
interpretability of results, and thus cannot perform well on generalization
tasks due to overfitting the dataset bias. In this work, we aim to combine the
benefits of both sides and overcome their limitations to achieve an end-to-end
interpretable structural reasoning for general images without the requirement
of layout annotations. Inspired by the property of a capsule network that can
carve a tree structure inside a regular convolutional neural network (CNN), we
propose a hierarchical compositional reasoning model called the ""Linguistically
driven Graph Capsule Network"", where the compositional process is guided by the
linguistic parse tree. Specifically, we bind each capsule in the lowest layer
to bridge the linguistic embedding of a single word in the original question
with visual evidence and then route them to the same capsule if they are
siblings in the parse tree. This compositional process is achieved by
performing inference on a linguistically driven conditional random field (CRF)
and is performed across multiple graph capsule layers, which results in a
compositional reasoning process inside a CNN. Experiments on the CLEVR dataset,
CLEVR compositional generation test, and FigureQA dataset demonstrate the
effectiveness and composition generalization ability of our end-to-end model.",['cs.CV'],238,146
Few Labeled Atlases are Necessary for Deep-Learning-Based Segmentation,"We tackle biomedical image segmentation in the scenario of only a few labeled
brain MR images. This is an important and challenging task in medical
applications, where manual annotations are time-consuming. Current multi-atlas
based segmentation methods use image registration to warp segments from labeled
images onto a new scan. In a different paradigm, supervised learning-based
segmentation strategies have gained popularity. These method consistently use
relatively large sets of labeled training data, and their behavior in the
regime of a few labeled biomedical images has not been thoroughly evaluated. In
this work, we provide two important results for segmentation in the scenario
where few labeled images are available. First, we propose a straightforward
implementation of efficient semi-supervised learning-based registration method,
which we showcase in a multi-atlas segmentation framework. Second, through an
extensive empirical study, we evaluate the performance of a supervised
segmentation approach, where the training images are augmented via random
deformations. Surprisingly, we find that in both paradigms, accurate
segmentation is generally possible even in the context of few labeled images.",['cs.CV'],177,109
Neural Architecture Search for Deep Image Prior,"We present a neural architecture search (NAS) technique to enhance the
performance of unsupervised image de-noising, in-painting and super-resolution
under the recently proposed Deep Image Prior (DIP). We show that evolutionary
search can automatically optimize the encoder-decoder (E-D) structure and
meta-parameters of the DIP network, which serves as a content-specific prior to
regularize these single image restoration tasks. Our binary representation
encodes the design space for an asymmetric E-D network that typically converges
to yield a content-specific DIP within 10-20 generations using a population
size of 500. The optimized architectures consistently improve upon the visual
quality of classical DIP for a diverse range of photographic and artistic
content.",['cs.CV'],118,87
Feature Fusion Encoder Decoder Network For Automatic Liver Lesion Segmentation,"Liver lesion segmentation is a difficult yet critical task for medical image
analysis. Recently, deep learning based image segmentation methods have
achieved promising performance, which can be divided into three categories: 2D,
2.5D and 3D, based on the dimensionality of the models. However, 2.5D and 3D
methods can have very high complexity and 2D methods may not perform
satisfactorily. To obtain competitive performance with low complexity, in this
paper, we propose a Feature-fusion Encoder-Decoder Network (FED-Net) based 2D
segmentation model to tackle the challenging problem of liver lesion
segmentation from CT images. Our feature fusion method is based on the
attention mechanism, which fuses high-level features carrying semantic
information with low-level features having image details. Additionally, to
compensate for the information loss during the upsampling process, a dense
upsampling convolution and a residual convolutional structure are proposed. We
tested our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS)
Challenge and achieved competitive results compared with other state-of-the-art
methods.","['cs.CV', 'cs.LG']",171,114
Measuring and Characterizing Generalization in Deep Reinforcement Learning,"Deep reinforcement-learning methods have achieved remarkable performance on
challenging control tasks. Observations of the resulting behavior give the
impression that the agent has constructed a generalized representation that
supports insightful action decisions. We re-examine what is meant by
generalization in RL, and propose several definitions based on an agent's
performance in on-policy, off-policy, and unreachable states. We propose a set
of practical methods for evaluating agents with these definitions of
generalization. We demonstrate these techniques on a common benchmark task for
deep RL, and we show that the learned networks make poor decisions for states
that differ only slightly from on-policy states, even though those states are
not selected adversarially. Taken together, these results call into question
the extent to which deep Q-networks learn generalized representations, and
suggest that more experimentation and analysis is necessary before claims of
representation learning can be supported.","['cs.LG', 'cs.AI', 'stat.ML']",150,103
A New State-of-the-Art Transformers-Based Load Forecaster on the Smart Grid Domain,"Meter-level load forecasting is crucial for efficient energy management and
power system planning for Smart Grids (SGs), in tasks associated with
regulation, dispatching, scheduling, and unit commitment of power grids.
Although a variety of algorithms have been proposed and applied on the field,
more accurate and robust models are still required: the overall utility cost of
operations in SGs increases 10 million currency units if the load forecasting
error increases 1%, and the mean absolute percentage error (MAPE) in
forecasting is still much higher than 1%. Transformers have become the new
state-of-the-art in a variety of tasks, including the ones in computer vision,
natural language processing and time series forecasting, surpassing alternative
neural models such as convolutional and recurrent neural networks. In this
letter, we present a new state-of-the-art Transformer-based algorithm for the
meter-level load forecasting task, which has surpassed the former
state-of-the-art, LSTM, and the traditional benchmark, vanilla RNN, in all
experiments by a margin of at least 13% in MAPE.",['cs.LG'],174,113
Attention-guided Progressive Mapping for Profile Face Recognition,"The past few years have witnessed great progress in the domain of face
recognition thanks to advances in deep learning. However, cross pose face
recognition remains a significant challenge. It is difficult for many deep
learning algorithms to narrow the performance gap caused by pose variations;
the main reasons for this relate to the intra-class discrepancy between face
images in different poses and the pose imbalances of training datasets.
Learning pose-robust features by traversing to the feature space of frontal
faces provides an effective and cheap way to alleviate this problem. In this
paper, we present a method for progressively transforming profile face
representations to the canonical pose with an attentive pair-wise loss.
Firstly, to reduce the difficulty of directly transforming the profile face
features into a frontal pose, we propose to learn the feature residual between
the source pose and its nearby pose in a block-byblock fashion, and thus
traversing to the feature space of a smaller pose by adding the learned
residual. Secondly, we propose an attentive pair-wise loss to guide the feature
transformation progressing in the most effective direction. Finally, our
proposed progressive module and attentive pair-wise loss are light-weight and
easy to implement, adding only about 7:5% extra parameters. Evaluations on the
CFP and CPLFW datasets demonstrate the superiority of our proposed method. Code
is available at https://github.com/hjy1312/AGPM.",['cs.CV'],234,137
Unsupervised Video Summarization with a Convolutional Attentive Adversarial Network,"With the explosive growth of video data, video summarization, which attempts
to seek the minimum subset of frames while still conveying the main story, has
become one of the hottest topics. Nowadays, substantial achievements have been
made by supervised learning techniques, especially after the emergence of deep
learning. However, it is extremely expensive and difficult to collect human
annotation for large-scale video datasets. To address this problem, we propose
a convolutional attentive adversarial network (CAAN), whose key idea is to
build a deep summarizer in an unsupervised way. Upon the generative adversarial
network, our overall framework consists of a generator and a discriminator. The
former predicts importance scores for all frames of a video while the latter
tries to distinguish the score-weighted frame features from original frame
features. Specifically, the generator employs a fully convolutional sequence
network to extract global representation of a video, and an attention-based
network to output normalized importance scores. To learn the parameters, our
objective function is composed of three loss functions, which can guide the
frame-level importance score prediction collaboratively. To validate this
proposed method, we have conducted extensive experiments on two public
benchmarks SumMe and TVSum. The results show the superiority of our proposed
method against other state-of-the-art unsupervised approaches. Our method even
outperforms some published supervised approaches.","['cs.CV', 'cs.MM']",221,146
On Inductive Biases in Deep Reinforcement Learning,"Many deep reinforcement learning algorithms contain inductive biases that
sculpt the agent's objective and its interface to the environment. These
inductive biases can take many forms, including domain knowledge and pretuned
hyper-parameters. In general, there is a trade-off between generality and
performance when algorithms use such biases. Stronger biases can lead to faster
learning, but weaker biases can potentially lead to more general algorithms.
This trade-off is important because inductive biases are not free; substantial
effort may be required to obtain relevant domain knowledge or to tune
hyper-parameters effectively. In this paper, we re-examine several
domain-specific components that bias the objective and the environmental
interface of common deep reinforcement learning agents. We investigated whether
the performance deteriorates when these components are replaced with adaptive
solutions from the literature. In our experiments, performance sometimes
decreased with the adaptive components, as one might expect when comparing to
components crafted for the domain, but sometimes the adaptive components
performed better. We investigated the main benefit of having fewer
domain-specific components, by comparing the learning performance of the two
systems on a different set of continuous control problems, without additional
tuning of either system. As hypothesized, the system with adaptive components
performed better on many of the new tasks.","['cs.LG', 'cs.AI', 'stat.ML']",213,125
Investigating maximum likelihood based training of infinite mixtures for uncertainty quantification,"Uncertainty quantification in neural networks gained a lot of attention in
the past years. The most popular approaches, Bayesian neural networks (BNNs),
Monte Carlo dropout, and deep ensembles have one thing in common: they are all
based on some kind of mixture model. While the BNNs build infinite mixture
models and are derived via variational inference, the latter two build finite
mixtures trained with the maximum likelihood method. In this work we
investigate the effect of training an infinite mixture distribution with the
maximum likelihood method instead of variational inference. We find that the
proposed objective leads to stochastic networks with an increased predictive
variance, which improves uncertainty based identification of
miss-classification and robustness against adversarial attacks in comparison to
a standard BNN with equivalent network structure. The new model also displays
higher entropy on out-of-distribution data.","['cs.LG', 'cs.AI', 'stat.ML']",140,99
TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning,"Our understanding of reinforcement learning (RL) has been shaped by
theoretical and empirical results that were obtained decades ago using tabular
representations and linear function approximators. These results suggest that
RL methods that use temporal differencing (TD) are superior to direct Monte
Carlo estimation (MC). How do these results hold up in deep RL, which deals
with perceptually complex environments and deep nonlinear models? In this
paper, we re-examine the role of TD in modern deep RL, using specially designed
environments that control for specific factors that affect performance, such as
reward sparsity, reward delay, and the perceptual complexity of the task. When
comparing TD with infinite-horizon MC, we are able to reproduce classic results
in modern settings. Yet we also find that finite-horizon MC is not inferior to
TD, even when rewards are sparse or delayed. This makes MC a viable alternative
to TD in deep RL.","['cs.LG', 'cs.AI', 'stat.ML']",151,106
An Algorithm for Pattern Discovery in Time Series,"We present a new algorithm for discovering patterns in time series and other
sequential data. We exhibit a reliable procedure for building the minimal set
of hidden, Markovian states that is statistically capable of producing the
behavior exhibited in the data -- the underlying process's causal states.
Unlike conventional methods for fitting hidden Markov models (HMMs) to data,
our algorithm makes no assumptions about the process's causal architecture (the
number of hidden states and their transition structure), but rather infers it
from the data. It starts with assumptions of minimal structure and introduces
complexity only when the data demand it. Moreover, the causal states it infers
have important predictive optimality properties that conventional HMM states
lack. We introduce the algorithm, review the theory behind it, prove its
asymptotic reliability, use large deviation theory to estimate its rate of
convergence, and compare it to other algorithms which also construct HMMs from
data. We also illustrate its behavior on an example process, and report
selected numerical results from an implementation.","['cs.LG', 'cs.CL', 'I.2.6;H.1.1;E.4']",169,104
"DeepActsNet: Spatial and Motion features from Face, Hands, and Body Combined with Convolutional and Graph Networks for Improved Action Recognition","Existing action recognition methods mainly focus on joint and bone
information in human body skeleton data due to its robustness to complex
backgrounds and dynamic characteristics of the environments. In this paper, we
combine body skeleton data with spatial and motion features from face and two
hands, and present ""Deep Action Stamps (DeepActs)"", a novel data representation
to encode actions from video sequences. We also present ""DeepActsNet"", a deep
learning based ensemble model which learns convolutional and structural
features from Deep Action Stamps for highly accurate action recognition.
Experiments on three challenging action recognition datasets (NTU60, NTU120,
and SYSU) show that the proposed model trained using Deep Action Stamps produce
considerable improvements in the action recognition accuracy with less
computational cost compared to the state-of-the-art methods.",['cs.CV'],129,89
Feature-metric Registration: A Fast Semi-supervised Approach for Robust Point Cloud Registration without Correspondences,"We present a fast feature-metric point cloud registration framework, which
enforces the optimisation of registration by minimising a feature-metric
projection error without correspondences. The advantage of the feature-metric
projection error is robust to noise, outliers and density difference in
contrast to the geometric projection error. Besides, minimising the
feature-metric projection error does not need to search the correspondences so
that the optimisation speed is fast. The principle behind the proposed method
is that the feature difference is smallest if point clouds are aligned very
well. We train the proposed method in a semi-supervised or unsupervised
approach, which requires limited or no registration label data. Experiments
demonstrate our method obtains higher accuracy and robustness than the
state-of-the-art methods. Besides, experimental results show that the proposed
method can handle significant noise and density difference, and solve both
same-source and cross-source point cloud registration.",['cs.CV'],151,86
Regularized Densely-connected Pyramid Network for Salient Instance Segmentation,"Much of the recent efforts on salient object detection (SOD) have been
devoted to producing accurate saliency maps without being aware of their
instance labels. To this end, we propose a new pipeline for end-to-end salient
instance segmentation (SIS) that predicts a class-agnostic mask for each
detected salient instance. To better use the rich feature hierarchies in deep
networks and enhance the side predictions, we propose the regularized dense
connections, which attentively promote informative features and suppress
non-informative ones from all feature pyramids. A novel multi-level RoIAlign
based decoder is introduced to adaptively aggregate multi-level features for
better mask predictions. Such strategies can be well-encapsulated into the Mask
R-CNN pipeline. Extensive experiments on popular benchmarks demonstrate that
our design significantly outperforms existing \sArt competitors by 6.3\%
(58.6\% vs. 52.3\%) in terms of the AP metric.The code is available at
https://github.com/yuhuan-wu/RDPNet.","['cs.CV', 'cs.LG', 'eess.IV']",157,120
Multi-Perspective LSTM for Joint Visual Representation Learning,"We present a novel LSTM cell architecture capable of learning both intra- and
inter-perspective relationships available in visual sequences captured from
multiple perspectives. Our architecture adopts a novel recurrent joint learning
strategy that uses additional gates and memories at the cell level. We
demonstrate that by using the proposed cell to create a network, more effective
and richer visual representations are learned for recognition tasks. We
validate the performance of our proposed architecture in the context of two
multi-perspective visual recognition tasks namely lip reading and face
recognition. Three relevant datasets are considered and the results are
compared against fusion strategies, other existing multi-input LSTM
architectures, and alternative recognition solutions. The experiments show the
superior performance of our solution over the considered benchmarks, both in
terms of recognition accuracy and complexity. We make our code publicly
available at https://github.com/arsm/MPLSTM.",['cs.CV'],146,96
Similarity and symmetry measures based on fuzzy descriptors of image objects` composition,"The paper describes a method for measuring the similarity and symmetry of an
image annotated with bounding boxes indicating image objects. The latter
representation became popular recently due to the rapid development of fast and
efficient deep-learning-based object-detection methods. The proposed approach
allows for comparing sets of bounding boxes to estimate the degree of
similarity of their underlying images. It is based on the fuzzy approach that
uses the fuzzy mutual position (FMP) matrix to describe spatial composition and
relations between bounding boxes within an image. A method of computing the
similarity of two images described by their FMP matrices is proposed and the
algorithm of its computation. It outputs the single scalar value describing the
degree of content-based image similarity. By modifying the method`s parameters,
instead of similarity, the reflectional symmetry of object composition may also
be measured. The proposed approach allows for measuring differences in objects`
composition of various intensities. It is also invariant to translation and
scaling and - in case of symmetry detection - position and orientation of the
symmetry axis. A couple of examples illustrate the method.","['cs.CV', '03B52, 94A08', 'I.4.8; I.4.10']",185,101
"DAF:re: A Challenging, Crowd-Sourced, Large-Scale, Long-Tailed Dataset For Anime Character Recognition","In this work we tackle the challenging problem of anime character
recognition. Anime, referring to animation produced within Japan and work
derived or inspired from it. For this purpose we present DAF:re
(DanbooruAnimeFaces:revamped), a large-scale, crowd-sourced, long-tailed
dataset with almost 500 K images spread across more than 3000 classes.
Additionally, we conduct experiments on DAF:re and similar datasets using a
variety of classification models, including CNN based ResNets and
self-attention based Vision Transformer (ViT). Our results give new insights
into the generalization and transfer learning properties of ViT models on
substantially different domain datasets from those used for the upstream
pre-training, including the influence of batch and image size in their
training. Additionally, we share our dataset, source-code, pre-trained
checkpoints and results, as Animesion, the first end-to-end framework for
large-scale anime character recognition: https://github.com/arkel23/animesion","['cs.CV', 'I.2; I.4']",151,111
Unsupervised Learning Facial Parameter Regressor for Action Unit Intensity Estimation via Differentiable Renderer,"Facial action unit (AU) intensity is an index to describe all visually
discernible facial movements. Most existing methods learn intensity estimator
with limited AU data, while they lack generalization ability out of the
dataset. In this paper, we present a framework to predict the facial parameters
(including identity parameters and AU parameters) based on a bone-driven face
model (BDFM) under different views. The proposed framework consists of a
feature extractor, a generator, and a facial parameter regressor. The regressor
can fit the physical meaning parameters of the BDFM from a single face image
with the help of the generator, which maps the facial parameters to the
game-face images as a differentiable renderer. Besides, identity loss, loopback
loss, and adversarial loss can improve the regressive results. Quantitative
evaluations are performed on two public databases BP4D and DISFA, which
demonstrates that the proposed method can achieve comparable or better
performance than the state-of-the-art methods. What's more, the qualitative
results also demonstrate the validity of our method in the wild.",['cs.CV'],173,114
Learning a Lie Algebra from Unlabeled Data Pairs,"Deep convolutional networks (convnets) show a remarkable ability to learn
disentangled representations. In recent years, the generalization of deep
learning to Lie groups beyond rigid motion in $\mathbb{R}^n$ has allowed to
build convnets over datasets with non-trivial symmetries, such as patterns over
the surface of a sphere. However, one limitation of this approach is the need
to explicitly define the Lie group underlying the desired invariance property
before training the convnet. Whereas rotations on the sphere have a well-known
symmetry group ($\mathrm{SO}(3)$), the same cannot be said of many real-world
factors of variability. For example, the disentanglement of pitch, intensity
dynamics, and playing technique remains a challenging task in music information
retrieval.
  This article proposes a machine learning method to discover a nonlinear
transformation of the space $\mathbb{R}^n$ which maps a collection of
$n$-dimensional vectors $(\boldsymbol{x}_i)_i$ onto a collection of target
vectors $(\boldsymbol{y}_i)_i$. The key idea is to approximate every target
$\boldsymbol{y}_i$ by a matrix--vector product of the form
$\boldsymbol{\widetilde{y}}_i = \boldsymbol{\phi}(t_i) \boldsymbol{x}_i$, where
the matrix $\boldsymbol{\phi}(t_i)$ belongs to a one-parameter subgroup of
$\mathrm{GL}_n (\mathbb{R})$. Crucially, the value of the parameter $t_i \in
\mathbb{R}$ may change between data pairs $(\boldsymbol{x}_i,
\boldsymbol{y}_i)$ and does not need to be known in advance.","['cs.LG', 'cs.AI', 'cs.CV', 'cs.SD', 'stat.ML']",236,143
Quadruply Stochastic Gradients for Large Scale Nonlinear Semi-Supervised AUC Optimization,"Semi-supervised learning is pervasive in real-world applications, where only
a few labeled data are available and large amounts of instances remain
unlabeled. Since AUC is an important model evaluation metric in classification,
directly optimizing AUC in semi-supervised learning scenario has drawn much
attention in the machine learning community. Recently, it has been shown that
one could find an unbiased solution for the semi-supervised AUC maximization
problem without knowing the class prior distribution. However, this method is
hardly scalable for nonlinear classification problems with kernels. To address
this problem, in this paper, we propose a novel scalable quadruply stochastic
gradient algorithm (QSG-S2AUC) for nonlinear semi-supervised AUC optimization.
In each iteration of the stochastic optimization process, our method randomly
samples a positive instance, a negative instance, an unlabeled instance and
their random features to compute the gradient and then update the model by
using this quadruply stochastic gradient to approach the optimal solution. More
importantly, we prove that QSG-S2AUC can converge to the optimal solution in
O(1/t), where t is the iteration number. Extensive experimental results on a
variety of benchmark datasets show that QSG-S2AUC is far more efficient than
the existing state-of-the-art algorithms for semi-supervised AUC maximization
while retaining the similar generalization performance.","['cs.LG', 'stat.ML']",216,134
Improving 3D Object Detection with Channel-wise Transformer,"Though 3D object detection from point clouds has achieved rapid progress in
recent years, the lack of flexible and high-performance proposal refinement
remains a great hurdle for existing state-of-the-art two-stage detectors.
Previous works on refining 3D proposals have relied on human-designed
components such as keypoints sampling, set abstraction and multi-scale feature
fusion to produce powerful 3D object representations. Such methods, however,
have limited ability to capture rich contextual dependencies among points. In
this paper, we leverage the high-quality region proposal network and a
Channel-wise Transformer architecture to constitute our two-stage 3D object
detection framework (CT3D) with minimal hand-crafted design. The proposed CT3D
simultaneously performs proposal-aware embedding and channel-wise context
aggregation for the point features within each proposal. Specifically, CT3D
uses proposal's keypoints for spatial contextual modelling and learns attention
propagation in the encoding module, mapping the proposal to point embeddings.
Next, a new channel-wise decoding module enriches the query-key interaction via
channel-wise re-weighting to effectively merge multi-level contexts, which
contributes to more accurate object predictions. Extensive experiments
demonstrate that our CT3D method has superior performance and excellent
scalability. Remarkably, CT3D achieves the AP of 81.77% in the moderate car
category on the KITTI test 3D detection benchmark, outperforms state-of-the-art
3D detectors.",['cs.CV'],224,153
Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations,"Contrastive self-supervised learning has outperformed supervised pretraining
on many downstream tasks like segmentation and object detection. However,
current methods are still primarily applied to curated datasets like ImageNet.
In this paper, we first study how biases in the dataset affect existing
methods. Our results show that current contrastive approaches work surprisingly
well across: (i) object- versus scene-centric, (ii) uniform versus long-tailed
and (iii) general versus domain-specific datasets. Second, given the generality
of the approach, we try to realize further gains with minor modifications. We
show that learning additional invariances -- through the use of multi-scale
cropping, stronger augmentations and nearest neighbors -- improves the
representations. Finally, we observe that MoCo learns spatially structured
representations when trained with a multi-crop strategy. The representations
can be used for semantic segment retrieval and video instance segmentation
without finetuning. Moreover, the results are on par with specialized models.
We hope this work will serve as a useful study for other researchers. The code
and models will be available at
https://github.com/wvangansbeke/Revisiting-Contrastive-SSL.","['cs.CV', 'cs.LG']",176,131
A Deep Value-network Based Approach for Multi-Driver Order Dispatching,"Recent works on ride-sharing order dispatching have highlighted the
importance of taking into account both the spatial and temporal dynamics in the
dispatching process for improving the transportation system efficiency. At the
same time, deep reinforcement learning has advanced to the point where it
achieves superhuman performance in a number of fields. In this work, we propose
a deep reinforcement learning based solution for order dispatching and we
conduct large scale online A/B tests on DiDi's ride-dispatching platform to
show that the proposed method achieves significant improvement on both total
driver income and user experience related metrics. In particular, we model the
ride dispatching problem as a Semi Markov Decision Process to account for the
temporal aspect of the dispatching actions. To improve the stability of the
value iteration with nonlinear function approximators like neural networks, we
propose Cerebellar Value Networks (CVNet) with a novel distributed state
representation layer. We further derive a regularized policy evaluation scheme
for CVNet that penalizes large Lipschitz constant of the value network for
additional robustness against adversarial perturbation and noises. Finally, we
adapt various transfer learning methods to CVNet for increased learning
adaptability and efficiency across multiple cities. We conduct extensive
offline simulations based on real dispatching data as well as online AB tests
through the DiDi's platform. Results show that CVNet consistently outperforms
other recently proposed dispatching methods. We finally show that the
performance can be further improved through the efficient use of transfer
learning.","['cs.LG', 'cs.AI']",247,152
Dynamic Relevance Learning for Few-Shot Object Detection,"Expensive bounding-box annotations have limited the development of object
detection task. Thus, it is necessary to focus on more challenging task of
few-shot object detection. It requires the detector to recognize objects of
novel classes with only a few training samples. Nowadays, many existing popular
methods based on meta-learning have achieved promising performance, such as
Meta R-CNN series. However, only a single category of support data is used as
the attention to guide the detecting of query images each time. Their relevance
to each other remains unexploited. Moreover, a lot of recent works treat the
support data and query images as independent branch without considering the
relationship between them. To address this issue, we propose a dynamic
relevance learning model, which utilizes the relationship between all support
images and Region of Interest (RoI) on the query images to construct a dynamic
graph convolutional network (GCN). By adjusting the prediction distribution of
the base detector using the output of this GCN, the proposed model can guide
the detector to improve the class representation implicitly. Comprehensive
experiments have been conducted on Pascal VOC and MS-COCO dataset. The proposed
model achieves the best overall performance, which shows its effectiveness of
learning more generalized features. Our code is available at
https://github.com/liuweijie19980216/DRL-for-FSOD.","['cs.CV', 'cs.LG']",218,144
Dynamic Fusion Network for RGBT Tracking,"For both visible and infrared images have their own advantages and
disadvantages, RGBT tracking has attracted more and more attention. The key
points of RGBT tracking lie in feature extraction and feature fusion of visible
and infrared images. Current RGBT tracking methods mostly pay attention to both
individual features (features extracted from images of a single camera) and
common features (features extracted and fused from an RGB camera and a thermal
camera), while pay less attention to the different and dynamic contributions of
individual features and common features for different sequences of registered
image pairs. This paper proposes a novel RGBT tracking method, called Dynamic
Fusion Network (DFNet), which adopts a two-stream structure, in which two
non-shared convolution kernels are employed in each layer to extract individual
features. Besides, DFNet has shared convolution kernels for each layer to
extract common features. Non-shared convolution kernels and shared convolution
kernels are adaptively weighted and summed according to different image pairs,
so that DFNet can deal with different contributions for different sequences.
DFNet has a fast speed, which is 28.658 FPS. The experimental results show that
when DFNet only increases the Mult-Adds of 0.02% than the
non-shared-convolution-kernel-based fusion method, Precision Rate (PR) and
Success Rate (SR) reach 88.1% and 71.9% respectively.",['cs.CV'],220,119
Multi-Channel Pyramid Person Matching Network for Person Re-Identification,"In this work, we present a Multi-Channel deep convolutional Pyramid Person
Matching Network (MC-PPMN) based on the combination of the semantic-components
and the color-texture distributions to address the problem of person
re-identification. In particular, we learn separate deep representations for
semantic-components and color-texture distributions from two person images and
then employ pyramid person matching network (PPMN) to obtain correspondence
representations. These correspondence representations are fused to perform the
re-identification task. Further, the proposed framework is optimized via a
unified end-to-end deep learning scheme. Extensive experiments on several
benchmark datasets demonstrate the effectiveness of our approach against the
state-of-the-art literature, especially on the rank-1 recognition rate.",['cs.CV'],119,81
Predicting the Co-Evolution of Event and Knowledge Graphs,"Embedding learning, a.k.a. representation learning, has been shown to be able
to model large-scale semantic knowledge graphs. A key concept is a mapping of
the knowledge graph to a tensor representation whose entries are predicted by
models using latent representations of generalized entities. Knowledge graphs
are typically treated as static: A knowledge graph grows more links when more
facts become available but the ground truth values associated with links is
considered time invariant. In this paper we address the issue of knowledge
graphs where triple states depend on time. We assume that changes in the
knowledge graph always arrive in form of events, in the sense that the events
are the gateway to the knowledge graph. We train an event prediction model
which uses both knowledge graph background information and information on
recent events. By predicting future events, we also predict likely changes in
the knowledge graph and thus obtain a model for the evolution of the knowledge
graph as well. Our experiments demonstrate that our approach performs well in a
clinical application, a recommendation engine and a sensor network application.",['cs.LG'],184,114
Adversarial Feature Augmentation for Unsupervised Domain Adaptation,"Recent works showed that Generative Adversarial Networks (GANs) can be
successfully applied in unsupervised domain adaptation, where, given a labeled
source dataset and an unlabeled target dataset, the goal is to train powerful
classifiers for the target samples. In particular, it was shown that a GAN
objective function can be used to learn target features indistinguishable from
the source ones. In this work, we extend this framework by (i) forcing the
learned feature extractor to be domain-invariant, and (ii) training it through
data augmentation in the feature space, namely performing feature augmentation.
While data augmentation in the image space is a well established technique in
deep learning, feature augmentation has not yet received the same level of
attention. We accomplish it by means of a feature generator trained by playing
the GAN minimax game against source features. Results show that both enforcing
domain-invariance and performing feature augmentation lead to superior or
comparable performance to state-of-the-art results in several unsupervised
domain adaptation benchmarks.",['cs.CV'],167,108
Stochastic analysis of heterogeneous porous material with modified neural architecture search (NAS) based physics-informed neural networks using transfer learning,"In this work, a modified neural architecture search method (NAS) based
physics-informed deep learning model is presented for stochastic analysis in
heterogeneous porous material. Monte Carlo method based on a randomized
spectral representation is first employed to construct a stochastic model for
simulation of flow through porous media. To solve the governing equations for
stochastic groundwater flow problem, we build a modified NAS model based on
physics-informed neural networks (PINNs) with transfer learning in this paper
that will be able to fit different partial differential equations (PDEs) with
less calculation. The performance estimation strategies adopted is constructed
from an error estimation model using the method of manufactured solutions. A
sensitivity analysis is performed to obtain the prior knowledge of the PINNs
model and narrow down the range of parameters for search space and use
hyper-parameter optimization algorithms to further determine the values of the
parameters. Further the NAS based PINNs model also saves the weights and biases
of the most favorable architectures, then used in the fine-tuning process. It
is found that the log-conductivity field using Gaussian correlation function
will perform much better than exponential correlation case, which is more
fitted to the PINNs model and the modified neural architecture search based
PINNs model shows a great potential in approximating solutions to PDEs.
Moreover, a three dimensional stochastic flow model is built to provide a
benchmark to the simulation of groundwater flow in highly heterogeneous
aquifers. The NAS model based deep collocation method is verified to be
effective and accurate through numerical examples in different dimensions using
different manufactured solutions.",['cs.LG'],265,146
A Review and Comparative Study on Probabilistic Object Detection in Autonomous Driving,"Capturing uncertainty in object detection is indispensable for safe
autonomous driving. In recent years, deep learning has become the de-facto
approach for object detection, and many probabilistic object detectors have
been proposed. However, there is no summary on uncertainty estimation in deep
object detection, and existing methods are not only built with different
network architectures and uncertainty estimation methods, but also evaluated on
different datasets with a wide range of evaluation metrics. As a result, a
comparison among methods remains challenging, as does the selection of a model
that best suits a particular application. This paper aims to alleviate this
problem by providing a review and comparative study on existing probabilistic
object detection methods for autonomous driving applications. First, we provide
an overview of generic uncertainty estimation in deep learning, and then
systematically survey existing methods and evaluation metrics for probabilistic
object detection. Next, we present a strict comparative study for probabilistic
object detection based on an image detector and three public autonomous driving
datasets. Finally, we present a discussion of the remaining challenges and
future works. Code has been made available at
https://github.com/asharakeh/pod_compare.git","['cs.CV', 'cs.RO']",190,116
Trading via Image Classification,"The art of systematic financial trading evolved with an array of approaches,
ranging from simple strategies to complex algorithms all relying, primary, on
aspects of time-series analysis. Recently, after visiting the trading floor of
a leading financial institution, we noticed that traders always execute their
trade orders while observing images of financial time-series on their screens.
In this work, we built upon the success in image recognition and examine the
value in transforming the traditional time-series analysis to that of image
classification. We create a large sample of financial time-series images
encoded as candlestick (Box and Whisker) charts and label the samples following
three algebraically-defined binary trade strategies. Using the images, we train
over a dozen machine-learning classification models and find that the
algorithms are very efficient in recovering the complicated, multiscale
label-generating rules when the data is represented visually. We suggest that
the transformation of continuous numeric time-series classification problem to
a vision problem is useful for recovering signals typical of technical
analysis.","['cs.CV', 'q-fin.CP', 'q-fin.TR']",172,111
Hybrid Model for Anomaly Detection on Call Detail Records by Time Series Forecasting,"Mobile network operators store an enormous amount of information like log
files that describe various events and users' activities. Analysis of these
logs might be used in many critical applications such as detecting
cyber-attacks, finding behavioral patterns of users, security incident
response, network forensics, etc. In a cellular network Call Detail Records
(CDR) is one type of such logs containing metadata of calls and usually
includes valuable information about contact such as the phone numbers of
originating and receiving subscribers, call duration, the area of activity,
type of call (SMS or voice call) and a timestamp. With anomaly detection, it is
possible to determine abnormal reduction or increment of network traffic in an
area or for a particular person. This paper's primary goal is to study
subscribers' behavior in a cellular network, mainly predicting the number of
calls in a region and detecting anomalies in the network traffic. In this
paper, a new hybrid method is proposed based on various anomaly detection
methods such as GARCH, K-means, and Neural Network to determine the anomalous
data. Moreover, we have discussed the possible causes of such anomalies.","['cs.LG', 'stat.ML']",188,120
Dual Contrastive Loss and Attention for GANs,"Generative Adversarial Networks (GANs) produce impressive results on
unconditional image generation when powered with large-scale image datasets.
Yet generated images are still easy to spot especially on datasets with high
variance (e.g. bedroom, church). In this paper, we propose various improvements
to further push the boundaries in image generation. Specifically, we propose a
novel dual contrastive loss and show that, with this loss, discriminator learns
more generalized and distinguishable representations to incentivize generation.
In addition, we revisit attention and extensively experiment with different
attention blocks in the generator. We find attention to be still an important
module for successful image generation even though it was not used in the
recent state-of-the-art models. Lastly, we study different attention
architectures in the discriminator, and propose a reference attention
mechanism. By combining the strengths of these remedies, we improve the
compelling state-of-the-art Fr\'{e}chet Inception Distance (FID) by at least
17.5% on several benchmark datasets. We obtain even more significant
improvements on compositional synthetic scenes (up to 47.5% in FID).","['cs.CV', 'cs.GR']",178,118
Are conditional GANs explicitly conditional?,"This paper proposes two important contributions for conditional Generative
Adversarial Networks (cGANs) to improve the wide variety of applications that
exploit this architecture. The first main contribution is an analysis of cGANs
to show that they are not explicitly conditional. In particular, it will be
shown that the discriminator and subsequently the cGAN does not automatically
learn the conditionality between inputs. The second contribution is a new
method, called acontrario, that explicitly models conditionality for both parts
of the adversarial architecture via a novel acontrario loss that involves
training the discriminator to learn unconditional (adverse) examples. This
leads to a novel type of data augmentation approach for GANs (acontrario
learning) which allows to restrict the search space of the generator to
conditional outputs using adverse examples. Extensive experimentation is
carried out to evaluate the conditionality of the discriminator by proposing a
probability distribution analysis. Comparisons with the cGAN architecture for
different applications show significant improvements in performance on well
known datasets including, semantic image synthesis, image segmentation and
monocular depth prediction using different metrics including Fr\'echet
Inception Distance(FID), mean Intersection over Union (mIoU), Root Mean Square
Error log (RMSE log) and Number of statistically-Different Bins (NDB)","['cs.CV', 'cs.AI']",199,134
An Attention-Based System for Damage Assessment Using Satellite Imagery,"When disaster strikes, accurate situational information and a fast, effective
response are critical to save lives. Widely available, high resolution
satellite images enable emergency responders to estimate locations, causes, and
severity of damage. Quickly and accurately analyzing the extensive amount of
satellite imagery available, though, requires an automatic approach. In this
paper, we present Siam-U-Net-Attn model - a multi-class deep learning model
with an attention mechanism - to assess damage levels of buildings given a pair
of satellite images depicting a scene before and after a disaster. We evaluate
the proposed method on xView2, a large-scale building damage assessment
dataset, and demonstrate that the proposed approach achieves accurate damage
scale classification and building segmentation results simultaneously.",['cs.CV'],119,87
Analysis of Vision-based Abnormal Red Blood Cell Classification,"Identification of abnormalities in red blood cells (RBC) is key to diagnosing
a range of medical conditions from anaemia to liver disease. Currently this is
done manually, a time-consuming and subjective process. This paper presents an
automated process utilising the advantages of machine learning to increase
capacity and standardisation of cell abnormality detection, and its performance
is analysed. Three different machine learning technologies were used: a Support
Vector Machine (SVM), a classical machine learning technology; TabNet, a deep
learning architecture for tabular data; U-Net, a semantic segmentation network
designed for medical image segmentation. A critical issue was the highly
imbalanced nature of the dataset which impacts the efficacy of machine
learning. To address this, synthesising minority class samples in feature space
was investigated via Synthetic Minority Over-sampling Technique (SMOTE) and
cost-sensitive learning. A combination of these two methods is investigated to
improve the overall performance. These strategies were found to increase
sensitivity to minority classes. The impact of unknown cells on semantic
segmentation is demonstrated, with some evidence of the model applying learning
of labelled cells to these anonymous cells. These findings indicate both
classical models and new deep learning networks as promising methods in
automating RBC abnormality detection.",['cs.CV'],203,133
ColorUNet: A convolutional classification approach to colorization,"This paper tackles the challenge of colorizing grayscale images. We take a
deep convolutional neural network approach, and choose to take the angle of
classification, working on a finite set of possible colors. Similarly to a
recent paper, we implement a loss and a prediction function that favor
realistic, colorful images rather than ""true"" ones.
  We show that a rather lightweight architecture inspired by the U-Net, and
trained on a reasonable amount of pictures of landscapes, achieves satisfactory
results on this specific subset of pictures. We show that data augmentation
significantly improves the performance and robustness of the model, and provide
visual analysis of the prediction confidence.
  We show an application of our model, extending the task to video
colorization. We suggest a way to smooth color predictions across frames,
without the need to train a recurrent network designed for sequential inputs.",['cs.CV'],143,94
Flip Learning: Erase to Segment,"Nodule segmentation from breast ultrasound images is challenging yet
essential for the diagnosis. Weakly-supervised segmentation (WSS) can help
reduce time-consuming and cumbersome manual annotation. Unlike existing
weakly-supervised approaches, in this study, we propose a novel and general WSS
framework called Flip Learning, which only needs the box annotation.
Specifically, the target in the label box will be erased gradually to flip the
classification tag, and the erased region will be considered as the
segmentation result finally. Our contribution is three-fold. First, our
proposed approach erases on superpixel level using a Multi-agent Reinforcement
Learning framework to exploit the prior boundary knowledge and accelerate the
learning process. Second, we design two rewards: classification score and
intensity distribution reward, to avoid under- and over-segmentation,
respectively. Third, we adopt a coarse-to-fine learning strategy to reduce the
residual errors and improve the segmentation performance. Extensively validated
on a large dataset, our proposed approach achieves competitive performance and
shows great potential to narrow the gap between fully-supervised and
weakly-supervised learning.","['cs.CV', 'cs.AI', 'cs.LG', 'cs.MA']",174,118
Large Hole Image Inpainting With Compress-Decompression Network,"Image inpainting technology can patch images with missing pixels. Existing
methods propose convolutional neural networks to repair corrupted images. The
networks focus on the valid pixels around the missing pixels, use the
encoder-decoder structure to extract valuable information, and use the
information to fix the vacancy. However, if the missing part is too large to
provide useful information, the result will exist blur, color mixing, and
object confusion. In order to patch the large hole image, we study the existing
approaches and propose a new network, the compression-decompression network.
The compression network takes responsibility for inpainting and generating a
down-sample image. The decompression network takes responsibility for extending
the down-sample image into the original resolution. We construct the
compression network with the residual network and propose a similar texture
selection algorithm to extend the image that is better than using the
super-resolution network. We evaluate our model over Places2 and CelebA data
set and use the similarity ratio as the metric. The result shows that our model
has better performance when the inpainting task has many conflicts.","['cs.CV', 'eess.IV']",182,105
Im2Avatar: Colorful 3D Reconstruction from a Single Image,"Existing works on single-image 3D reconstruction mainly focus on shape
recovery. In this work, we study a new problem, that is, simultaneously
recovering 3D shape and surface color from a single image, namely ""colorful 3D
reconstruction"". This problem is both challenging and intriguing because the
ability to infer textured 3D model from a single image is at the core of visual
understanding. Here, we propose an end-to-end trainable framework, Colorful
Voxel Network (CVN), to tackle this problem. Conditioned on a single 2D input,
CVN learns to decompose shape and surface color information of a 3D object into
a 3D shape branch and a surface color branch, respectively. Specifically, for
the shape recovery, we generate a shape volume with the state of its voxels
indicating occupancy. For the surface color recovery, we combine the strength
of appearance hallucination and geometric projection by concurrently learning a
regressed color volume and a 2D-to-3D flow volume, which are then fused into a
blended color volume. The final textured 3D model is obtained by sampling color
from the blended color volume at the positions of occupied voxels in the shape
volume. To handle the severe sparse volume representations, a novel loss
function, Mean Squared False Cross-Entropy Loss (MSFCEL), is designed.
Extensive experiments demonstrate that our approach achieves significant
improvement over baselines, and shows great generalization across diverse
object categories and arbitrary viewpoints.",['cs.CV'],233,134
Extreme Precipitation Seasonal Forecast Using a Transformer Neural Network,"An impact of climate change is the increase in frequency and intensity of
extreme precipitation events. However, confidently predicting the likelihood of
extreme precipitation at seasonal scales remains an outstanding challenge.
Here, we present an approach to forecasting the quantiles of the maximum daily
precipitation in each week up to six months ahead using the temporal fusion
transformer (TFT) model. Through experiments in two regions, we compare TFT
predictions with those of two baselines: climatology and a calibrated ECMWF
SEAS5 ensemble forecast (S5). Our results show that, in terms of quantile risk
at six month lead time, the TFT predictions significantly outperform those from
S5 and show an overall small improvement compared to climatology. The TFT also
responds positively to departures from normal that climatology cannot.","['cs.LG', 'cs.AI', 'physics.ao-ph']",126,88
An approach to image denoising using manifold approximation without clean images,"Image restoration has been an extensively researched topic in numerous
fields. With the advent of deep learning, a lot of the current algorithms were
replaced by algorithms that are more flexible and robust. Deep networks have
demonstrated impressive performance in a variety of tasks like blind denoising,
image enhancement, deblurring, super-resolution, inpainting, among others. Most
of these learning-based algorithms use a large amount of clean data during the
training process. However, in certain applications in medical image processing,
one may not have access to a large amount of clean data. In this paper, we
propose a method for denoising that attempts to learn the denoising process by
pushing the noisy data close to the clean data manifold, using only noisy
images during training. Furthermore, we use perceptual loss terms and an
iterative refinement step to further refine the clean images without losing
important features.","['cs.CV', 'eess.IV']",146,99
TAFSSL: Task-Adaptive Feature Sub-Space Learning for few-shot classification,"The field of Few-Shot Learning (FSL), or learning from very few (typically
$1$ or $5$) examples per novel class (unseen during training), has received a
lot of attention and significant performance advances in the recent literature.
While number of techniques have been proposed for FSL, several factors have
emerged as most important for FSL performance, awarding SOTA even to the
simplest of techniques. These are: the backbone architecture (bigger is
better), type of pre-training on the base classes (meta-training vs regular
multi-class, currently regular wins), quantity and diversity of the base
classes set (the more the merrier, resulting in richer and better adaptive
features), and the use of self-supervised tasks during pre-training (serving as
a proxy for increasing the diversity of the base set). In this paper we propose
yet another simple technique that is important for the few shot learning
performance - a search for a compact feature sub-space that is discriminative
for a given few-shot test task. We show that the Task-Adaptive Feature
Sub-Space Learning (TAFSSL) can significantly boost the performance in FSL
scenarios when some additional unlabeled data accompanies the novel few-shot
task, be it either the set of unlabeled queries (transductive FSL) or some
additional set of unlabeled data samples (semi-supervised FSL). Specifically,
we show that on the challenging miniImageNet and tieredImageNet benchmarks,
TAFSSL can improve the current state-of-the-art in both transductive and
semi-supervised FSL settings by more than $5\%$, while increasing the benefit
of using unlabeled data in FSL to above $10\%$ performance gain.",['cs.CV'],264,151
PAC-Bayesian Policy Evaluation for Reinforcement Learning,"Bayesian priors offer a compact yet general means of incorporating domain
knowledge into many learning tasks. The correctness of the Bayesian analysis
and inference, however, largely depends on accuracy and correctness of these
priors. PAC-Bayesian methods overcome this problem by providing bounds that
hold regardless of the correctness of the prior distribution. This paper
introduces the first PAC-Bayesian bound for the batch reinforcement learning
problem with function approximation. We show how this bound can be used to
perform model-selection in a transfer learning scenario. Our empirical results
confirm that PAC-Bayesian policy evaluation is able to leverage prior
distributions when they are informative and, unlike standard Bayesian RL
approaches, ignore them when they are misleading.","['cs.LG', 'stat.ML']",118,86
Explaining Away Attacks Against Neural Networks,"We investigate the problem of identifying adversarial attacks on image-based
neural networks. We present intriguing experimental results showing significant
discrepancies between the explanations generated for the predictions of a model
on clean and adversarial data. Utilizing this intuition, we propose a framework
which can identify whether a given input is adversarial based on the
explanations given by the model. Code for our experiments can be found here:
https://github.com/seansaito/Explaining-Away-Attacks-Against-Neural-Networks.","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",78,60
Object Detection from Video Tubelets with Convolutional Neural Networks,"Deep Convolution Neural Networks (CNNs) have shown impressive performance in
various vision tasks such as image classification, object detection and
semantic segmentation. For object detection, particularly in still images, the
performance has been significantly increased last year thanks to powerful deep
networks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNN
features (R-CNN)). The lately introduced ImageNet task on object detection from
video (VID) brings the object detection task into the video domain, in which
objects' locations at each frame are required to be annotated with bounding
boxes. In this work, we introduce a complete framework for the VID task based
on still-image object detection and general object tracking. Their relations
and contributions in the VID task are thoroughly studied and evaluated. In
addition, a temporal convolution network is proposed to incorporate temporal
information to regularize the detection results and shows its effectiveness for
the task.",['cs.CV'],150,102
Self-EMD: Self-Supervised Object Detection without ImageNet,"In this paper, we propose a novel self-supervised representation learning
method, Self-EMD, for object detection. Our method directly trained on
unlabeled non-iconic image dataset like COCO, instead of commonly used
iconic-object image dataset like ImageNet. We keep the convolutional feature
maps as the image embedding to preserve spatial structures and adopt Earth
Mover's Distance (EMD) to compute the similarity between two embeddings. Our
Faster R-CNN (ResNet50-FPN) baseline achieves 39.8% mAP on COCO, which is on
par with the state of the art self-supervised methods pre-trained on ImageNet.
More importantly, it can be further improved to 40.4% mAP with more unlabeled
images, showing its great potential for leveraging more easily obtained
unlabeled data. Code will be made available.",['cs.CV'],128,97
Bias-Variance Reduced Local SGD for Less Heterogeneous Federated Learning,"Recently, local SGD has got much attention and been extensively studied in
the distributed learning community to overcome the communication bottleneck
problem. However, the superiority of local SGD to minibatch SGD only holds in
quite limited situations. In this paper, we study a new local algorithm called
Bias-Variance Reduced Local SGD (BVR-L-SGD) for nonconvex distributed
optimization. Algorithmically, our proposed bias and variance reduced local
gradient estimator fully utilizes small second-order heterogeneity of local
objectives and suggests randomly picking up one of the local models instead of
taking the average of them when workers are synchronized. Theoretically, under
small heterogeneity of local objectives, we show that BVR-L-SGD achieves better
communication complexity than both the previous non-local and local methods
under mild conditions, and particularly BVR-L-SGD is the first method that
breaks the barrier of communication complexity $\Theta(1/\varepsilon)$ for
general nonconvex smooth objectives when the heterogeneity is small and the
local computation budget is large. Numerical results are given to verify the
theoretical findings and give empirical evidence of the superiority of our
method.","['cs.LG', 'math.OC']",183,114
Surrogate-assisted parallel tempering for Bayesian neural learning,"Due to the need for robust uncertainty quantification, Bayesian neural
learning has gained attention in the era of deep learning and big data. Markov
Chain Monte-Carlo (MCMC) methods typically implement Bayesian inference which
faces several challenges given a large number of parameters, complex and
multimodal posterior distributions, and computational complexity of large
neural network models. Parallel tempering MCMC addresses some of these
limitations given that they can sample multimodal posterior distributions and
utilize high-performance computing. However, certain challenges remain given
large neural network models and big data. Surrogate-assisted optimization
features the estimation of an objective function for models which are
computationally expensive. In this paper, we address the inefficiency of
parallel tempering MCMC for large-scale problems by combining parallel
computing features with surrogate assisted likelihood estimation that describes
the plausibility of a model parameter value, given specific observed data.
Hence, we present surrogate-assisted parallel tempering for Bayesian neural
learning for simple to computationally expensive models. Our results
demonstrate that the methodology significantly lowers the computational cost
while maintaining quality in decision making with Bayesian neural networks. The
method has applications for a Bayesian inversion and uncertainty quantification
for a broad range of numerical models.","['cs.LG', 'cs.AI', 'stat.ML']",199,119
Algorithmic Discrimination: Formulation and Exploration in Deep Learning-based Face Biometrics,"The most popular face recognition benchmarks assume a distribution of
subjects without much attention to their demographic attributes. In this work,
we perform a comprehensive discrimination-aware experimentation of deep
learning-based face recognition. The main aim of this study is focused on a
better understanding of the feature space generated by deep models, and the
performance achieved over different demographic groups. We also propose a
general formulation of algorithmic discrimination with application to face
biometrics. The experiments are conducted over the new DiveFace database
composed of 24K identities from six different demographic groups. Two popular
face recognition models are considered in the experimental framework: ResNet-50
and VGG-Face. We experimentally show that demographic groups highly represented
in popular face databases have led to popular pre-trained deep face models
presenting strong algorithmic discrimination. That discrimination can be
observed both qualitatively at the feature space of the deep models and
quantitatively in large performance differences when applying those models in
different demographic groups, e.g. for face biometrics.","['cs.CV', 'cs.CY']",169,107
Dynamic multi-object Gaussian process models: A framework for data-driven functional modelling of human joints,"Statistical shape models (SSMs) are state-of-the-art medical image analysis
tools for extracting and explaining features across a set of biological
structures. However, a principled and robust way to combine shape and pose
features has been illusive due to three main issues: 1) Non-homogeneity of the
data (data with linear and non-linear natural variation across features), 2)
non-optimal representation of the $3D$ motion (rigid transformation
representations that are not proportional to the kinetic energy that move an
object from one position to the other), and 3) artificial discretization of the
models. In this paper, we propose a new framework for dynamic multi-object
statistical modelling framework for the analysis of human joints in a
continuous domain. Specifically, we propose to normalise shape and dynamic
spatial features in the same linearized statistical space permitting the use of
linear statistics; we adopt an optimal 3D motion representation for more
accurate rigid transformation comparisons; and we provide a 3D shape and pose
prediction protocol using a Markov chain Monte Carlo sampling-based fitting.
The framework affords an efficient generative dynamic multi-object modelling
platform for biological joints. We validate the framework using a controlled
synthetic data. Finally, the framework is applied to an analysis of the human
shoulder joint to compare its performance with standard SSM approaches in
prediction of shape while adding the advantage of determining relative pose
between bones in a complex. Excellent validity is observed and the shoulder
joint shape-pose prediction results suggest that the novel framework may have
utility for a range of medical image analysis applications. Furthermore, the
framework is generic and can be extended to n$>$2 objects, making it suitable
for clinical and diagnostic methods for the management of joint disorders.",['cs.CV'],291,163
GenDICE: Generalized Offline Estimation of Stationary Values,"An important problem that arises in reinforcement learning and Monte Carlo
methods is estimating quantities defined by the stationary distribution of a
Markov chain. In many real-world applications, access to the underlying
transition operator is limited to a fixed set of data that has already been
collected, without additional interaction with the environment being available.
We show that consistent estimation remains possible in this challenging
scenario, and that effective estimation can still be achieved in important
applications. Our approach is based on estimating a ratio that corrects for the
discrepancy between the stationary and empirical distributions, derived from
fundamental properties of the stationary distribution, and exploiting
constraint reformulations based on variational divergence minimization. The
resulting algorithm, GenDICE, is straightforward and effective. We prove its
consistency under general conditions, provide an error analysis, and
demonstrate strong empirical performance on benchmark problems, including
off-line PageRank and off-policy policy evaluation.","['stat.ML', 'cs.LG']",150,110
Accounting for Human Learning when Inferring Human Preferences,"Inverse reinforcement learning (IRL) is a common technique for inferring
human preferences from data. Standard IRL techniques tend to assume that the
human demonstrator is stationary, that is that their policy $\pi$ doesn't
change over time. In practice, humans interacting with a novel environment or
performing well on a novel task will change their demonstrations as they learn
more about the environment or task. We investigate the consequences of relaxing
this assumption of stationarity, in particular by modelling the human as
learning. Surprisingly, we find in some small examples that this can lead to
better inference than if the human was stationary. That is, by observing a
demonstrator who is themselves learning, a machine can infer more than by
observing a demonstrator who is noisily rational. In addition, we find evidence
that misspecification can lead to poor inference, suggesting that modelling
human learning is important, especially when the human is facing an unfamiliar
environment.","['cs.LG', 'cs.AI']",155,94
A Background-Agnostic Framework with Adversarial Training for Abnormal Event Detection in Video,"Abnormal event detection in video is a complex computer vision problem that
has attracted significant attention in recent years. The complexity of the task
arises from the commonly-adopted definition of an abnormal event, that is, a
rarely occurring event that typically depends on the surrounding context.
Following the standard formulation of abnormal event detection as outlier
detection, we propose a background-agnostic framework that learns from training
videos containing only normal events. Our framework is composed of an object
detector, a set of appearance and motion auto-encoders, and a set of
classifiers. Since our framework only looks at object detections, it can be
applied to different scenes, provided that normal events are defined
identically across scenes and that the single main factor of variation is the
background. To overcome the lack of abnormal data during training, we propose
an adversarial learning strategy for the auto-encoders. We create a
scene-agnostic set of out-of-domain pseudo-abnormal examples, which are
correctly reconstructed by the auto-encoders before applying gradient ascent on
the pseudo-abnormal examples. We further utilize the pseudo-abnormal examples
to serve as abnormal examples when training appearance-based and motion-based
binary classifiers to discriminate between normal and abnormal latent features
and reconstructions. We compare our framework with the state-of-the-art methods
on four benchmark data sets, using various evaluation metrics. Compared to
existing methods, the empirical results indicate that our approach achieves
favorable performance on all data sets. In addition, we provide region-based
and track-based annotations for two large-scale abnormal event detection data
sets from the literature, namely ShanghaiTech and Subway.","['cs.CV', 'eess.IV']",273,156
Spatio-Temporal Convolutional LSTMs for Tumor Growth Prediction by Learning 4D Longitudinal Patient Data,"Prognostic tumor growth modeling via volumetric medical imaging observations
can potentially lead to better outcomes of tumor treatment and surgical
planning. Recent advances of convolutional networks have demonstrated higher
accuracy than traditional mathematical models in predicting future tumor
volumes. This indicates that deep learning-based techniques may have great
potentials on addressing such problem. However, current 2D patch-based modeling
approaches cannot make full use of the spatio-temporal imaging context of the
tumor's longitudinal 4D (3D + time) data. Moreover, they are incapable to
predict clinically-relevant tumor properties, other than volumes. In this
paper, we exploit to formulate the tumor growth process through convolutional
Long Short-Term Memory (ConvLSTM) that extract tumor's static imaging
appearances and capture its temporal dynamic changes within a single network.
We extend ConvLSTM into the spatio-temporal domain (ST-ConvLSTM) by jointly
learning the inter-slice 3D contexts and the longitudinal or temporal dynamics
from multiple patient studies. Our approach can incorporate other non-imaging
patient information in an end-to-end trainable manner. Experiments are
conducted on the largest 4D longitudinal tumor dataset of 33 patients to date.
Results validate that the ST-ConvLSTM produces a Dice score of 83.2%+-5.1% and
a RVD of 11.2%+-10.8%, both significantly outperforming (p<0.05) other compared
methods of linear model, ConvLSTM, and generative adversarial network (GAN)
under the metric of predicting future tumor volumes. Additionally, our new
method enables the prediction of both cell density and CT intensity numbers.
Last, we demonstrate the generalizability of ST-ConvLSTM by employing it in 4D
medical image segmentation task, which achieves an averaged Dice score of
86.3+-1.2% for left-ventricle segmentation in 4D ultrasound with 3 seconds per
patient.",['cs.CV'],291,193
Latent Variable Time-varying Network Inference,"In many applications of finance, biology and sociology, complex systems
involve entities interacting with each other. These processes have the
peculiarity of evolving over time and of comprising latent factors, which
influence the system without being explicitly measured. In this work we present
latent variable time-varying graphical lasso (LTGL), a method for multivariate
time-series graphical modelling that considers the influence of hidden or
unmeasurable factors. The estimation of the contribution of the latent factors
is embedded in the model which produces both sparse and low-rank components for
each time point. In particular, the first component represents the connectivity
structure of observable variables of the system, while the second represents
the influence of hidden factors, assumed to be few with respect to the observed
variables. Our model includes temporal consistency on both components,
providing an accurate evolutionary pattern of the system. We derive a tractable
optimisation algorithm based on alternating direction method of multipliers,
and develop a scalable and efficient implementation which exploits proximity
operators in closed form. LTGL is extensively validated on synthetic data,
achieving optimal performance in terms of accuracy, structure learning and
scalability with respect to ground truth and state-of-the-art methods for
graphical inference. We conclude with the application of LTGL to real case
studies, from biology and finance, to illustrate how our method can be
successfully employed to gain insights on multivariate time-series data.","['stat.ML', 'cs.LG']",234,143
Unsupervised Deep Context Prediction for Background Foreground Separation,"In many advanced video based applications background modeling is a
pre-processing step to eliminate redundant data, for instance in tracking or
video surveillance applications. Over the past years background subtraction is
usually based on low level or hand-crafted features such as raw color
components, gradients, or local binary patterns. The background subtraction
algorithms performance suffer in the presence of various challenges such as
dynamic backgrounds, photometric variations, camera jitters, and shadows. To
handle these challenges for the purpose of accurate background modeling we
propose a unified framework based on the algorithm of image inpainting. It is
an unsupervised visual feature learning hybrid Generative Adversarial algorithm
based on context prediction. We have also presented the solution of random
region inpainting by the fusion of center region inpaiting and random region
inpainting with the help of poisson blending technique. Furthermore we also
evaluated foreground object detection with the fusion of our proposed method
and morphological operations. The comparison of our proposed method with 12
state-of-the-art methods shows its stability in the application of background
estimation and foreground detection.",['cs.CV'],181,119
Deep Reinforcement Learning With Macro-Actions,"Deep reinforcement learning has been shown to be a powerful framework for
learning policies from complex high-dimensional sensory inputs to actions in
complex tasks, such as the Atari domain. In this paper, we explore output
representation modeling in the form of temporal abstraction to improve
convergence and reliability of deep reinforcement learning approaches. We
concentrate on macro-actions, and evaluate these on different Atari 2600 games,
where we show that they yield significant improvements in learning speed.
Additionally, we show that they can even achieve better scores than DQN. We
offer analysis and explanation for both convergence and final results,
revealing a problem deep RL approaches have with sparse reward signals.","['cs.LG', 'cs.AI', 'cs.NE']",112,84
Point Cloud Completion by Learning Shape Priors,"In view of the difficulty in reconstructing object details in point cloud
completion, we propose a shape prior learning method for object completion. The
shape priors include geometric information in both complete and the partial
point clouds. We design a feature alignment strategy to learn the shape prior
from complete points, and a coarse to fine strategy to incorporate partial
prior in the fine stage. To learn the complete objects prior, we first train a
point cloud auto-encoder to extract the latent embeddings from complete points.
Then we learn a mapping to transfer the point features from partial points to
that of the complete points by optimizing feature alignment losses. The feature
alignment losses consist of a L2 distance and an adversarial loss obtained by
Maximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2
distance optimizes the partial features towards the complete ones in the
feature space, and MMD-GAN decreases the statistical distance of two point
features in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art
performances on the point cloud completion task. Our code is available at
https://github.com/xiaogangw/point-cloud-completion-shape-prior.","['cs.CV', 'cs.LG', 'eess.IV']",193,101
IDA: Improved Data Augmentation Applied to Salient Object Detection,"In this paper, we present an Improved Data Augmentation (IDA) technique
focused on Salient Object Detection (SOD). Standard data augmentation
techniques proposed in the literature, such as image cropping, rotation,
flipping, and resizing, only generate variations of the existing examples,
providing a limited generalization. Our method combines image inpainting,
affine transformations, and the linear combination of different generated
background images with salient objects extracted from labeled data. Our
proposed technique enables more precise control of the object's position and
size while preserving background information. The background choice is based on
an inter-image optimization, while object size follows a uniform random
distribution within a specified interval, and the object position is
intra-image optimal. We show that our method improves the segmentation quality
when used for training state-of-the-art neural networks on several famous
datasets of the SOD field. Combining our method with others surpasses
traditional techniques such as horizontal-flip in 0.52% for F-measure and 1.19%
for Precision. We also provide an evaluation in 7 different SOD datasets, with
9 distinct evaluation metrics and an average ranking of the evaluated methods.","['cs.CV', 'cs.LG', 'eess.IV']",188,130
High-Fidelity Pluralistic Image Completion with Transformers,"Image completion has made tremendous progress with convolutional neural
networks (CNNs), because of their powerful texture modeling capacity. However,
due to some inherent properties (e.g., local inductive prior, spatial-invariant
kernels), CNNs do not perform well in understanding global structures or
naturally support pluralistic completion. Recently, transformers demonstrate
their power in modeling the long-term relationship and generating diverse
results, but their computation complexity is quadratic to input length, thus
hampering the application in processing high-resolution images. This paper
brings the best of both worlds to pluralistic image completion: appearance
prior reconstruction with transformer and texture replenishment with CNN. The
former transformer recovers pluralistic coherent structures together with some
coarse textures, while the latter CNN enhances the local texture details of
coarse priors guided by the high-resolution masked images. The proposed method
vastly outperforms state-of-the-art methods in terms of three aspects: 1) large
performance boost on image fidelity even compared to deterministic completion
methods; 2) better diversity and higher fidelity for pluralistic completion; 3)
exceptional generalization ability on large masks and generic dataset, like
ImageNet.","['cs.CV', 'cs.GR']",181,130
LU-Net: An Efficient Network for 3D LiDAR Point Cloud Semantic Segmentation Based on End-to-End-Learned 3D Features and U-Net,"We propose LU-Net -- for LiDAR U-Net, a new method for the semantic
segmentation of a 3D LiDAR point cloud. Instead of applying some global 3D
segmentation method such as PointNet, we propose an end-to-end architecture for
LiDAR point cloud semantic segmentation that efficiently solves the problem as
an image processing problem. We first extract high-level 3D features for each
point given its 3D neighbors. Then, these features are projected into a 2D
multichannel range-image by considering the topology of the sensor. Thanks to
these learned features and this projection, we can finally perform the
segmentation using a simple U-Net segmentation network, which performs very
well while being very efficient. In this way, we can exploit both the 3D nature
of the data and the specificity of the LiDAR sensor. This approach outperforms
the state-of-the-art by a large margin on the KITTI dataset, as our experiments
show. Moreover, this approach operates at 24fps on a single GPU. This is above
the acquisition rate of common LiDAR sensors which makes it suitable for
real-time applications.",['cs.CV'],184,112
Res2Net: A New Multi-scale Backbone Architecture,"Representing features at multiple scales is of great importance for numerous
vision tasks. Recent advances in backbone convolutional neural networks (CNNs)
continually demonstrate stronger multi-scale representation ability, leading to
consistent performance gains on a wide range of applications. However, most
existing methods represent the multi-scale features in a layer-wise manner. In
this paper, we propose a novel building block for CNNs, namely Res2Net, by
constructing hierarchical residual-like connections within one single residual
block. The Res2Net represents multi-scale features at a granular level and
increases the range of receptive fields for each network layer. The proposed
Res2Net block can be plugged into the state-of-the-art backbone CNN models,
e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these
models and demonstrate consistent performance gains over baseline models on
widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies
and experimental results on representative computer vision tasks, i.e., object
detection, class activation mapping, and salient object detection, further
verify the superiority of the Res2Net over the state-of-the-art baseline
methods. The source code and trained models are available on
https://mmcheng.net/res2net/.",['cs.CV'],197,128
Face Images as Jigsaw Puzzles: Compositional Perception of Human Faces for Machines Using Generative Adversarial Networks,"An important goal in human-robot-interaction (HRI) is for machines to achieve
a close to human level of face perception. One of the important differences
between machine learning and human intelligence is the lack of
compositionality. This paper introduces a new scheme to enable generative
adversarial networks to learn the distribution of face images composed of
smaller parts. This results in a more flexible machine face perception and
easier generalization to outside training examples. We demonstrate that this
model is able to produce realistic high-quality face images by generating and
piecing together the parts. Additionally, we demonstrate that this model learns
the relations between the facial parts and their distributions. Therefore, the
specific facial parts are interchangeable between generated face images.","['cs.CV', 'cs.LG']",123,78
A Simple Baseline Algorithm for Graph Classification,"Graph classification has recently received a lot of attention from various
fields of machine learning e.g. kernel methods, sequential modeling or graph
embedding. All these approaches offer promising results with different
respective strengths and weaknesses. However, most of them rely on complex
mathematics and require heavy computational power to achieve their best
performance. We propose a simple and fast algorithm based on the spectral
decomposition of graph Laplacian to perform graph classification and get a
first reference score for a dataset. We show that this method obtains
competitive results compared to state-of-the-art algorithms.","['cs.LG', 'stat.ML']",97,78
MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis,"In this paper, we propose MINE to perform novel view synthesis and depth
estimation via dense 3D reconstruction from a single image. Our approach is a
continuous depth generalization of the Multiplane Images (MPI) by introducing
the NEural radiance fields (NeRF). Given a single image as input, MINE predicts
a 4-channel image (RGB and volume density) at arbitrary depth values to jointly
reconstruct the camera frustum and fill in occluded contents. The reconstructed
and inpainted frustum can then be easily rendered into novel RGB or depth views
using differentiable rendering. Extensive experiments on RealEstate10K, KITTI
and Flowers Light Fields show that our MINE outperforms state-of-the-art by a
large margin in novel view synthesis. We also achieve competitive results in
depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our
source code is available at https://github.com/vincentfung13/MINE","['cs.CV', 'cs.GR', 'cs.LG']",146,107
Improving Graph Attention Networks with Large Margin-based Constraints,"Graph Attention Networks (GATs) are the state-of-the-art neural architecture
for representation learning with graphs. GATs learn attention functions that
assign weights to nodes so that different nodes have different influences in
the feature aggregation steps. In practice, however, induced attention
functions are prone to over-fitting due to the increasing number of parameters
and the lack of direct supervision on attention weights. GATs also suffer from
over-smoothing at the decision boundary of nodes. Here we propose a framework
to address their weaknesses via margin-based constraints on attention during
training. We first theoretically demonstrate the over-smoothing behavior of
GATs and then develop an approach using constraint on the attention weights
according to the class boundary and feature aggregation pattern. Furthermore,
to alleviate the over-fitting problem, we propose additional constraints on the
graph structure. Extensive experiments and ablation studies on common benchmark
datasets demonstrate the effectiveness of our method, which leads to
significant improvements over the previous state-of-the-art graph attention
methods on all datasets.","['cs.LG', 'stat.ML']",172,106
Conclusive Local Interpretation Rules for Random Forests,"In critical situations involving discrimination, gender inequality, economic
damage, and even the possibility of casualties, machine learning models must be
able to provide clear interpretations for their decisions. Otherwise, their
obscure decision-making processes can lead to socioethical issues as they
interfere with people's lives. In the aforementioned sectors, random forest
algorithms strive, thus their ability to explain themselves is an obvious
requirement. In this paper, we present LionForests, which relies on a
preliminary work of ours. LionForests is a random forest-specific
interpretation technique, which provides rules as explanations. It is
applicable from binary classification tasks to multi-class classification and
regression tasks, and it is supported by a stable theoretical background.
Experimentation, including sensitivity analysis and comparison with
state-of-the-art techniques, is also performed to demonstrate the efficacy of
our contribution. Finally, we highlight a unique property of LionForests,
called conclusiveness, that provides interpretation validity and distinguishes
it from previous techniques.","['cs.LG', 'cs.AI', 'I.2.0; I.2.6']",156,115
On Improving Adversarial Transferability of Vision Transformers,"Vision transformers (ViTs) process input images as sequences of patches via
self-attention; a radically different architecture than convolutional neural
networks (CNNs). This makes it interesting to study the adversarial feature
space of ViT models and their transferability. In particular, we observe that
adversarial patterns found via conventional adversarial attacks show very low
black-box transferability even for large ViT models. However, we show that this
phenomenon is only due to the sub-optimal attack procedures that do not
leverage the true representation potential of ViTs. A deep ViT is composed of
multiple blocks, with a consistent architecture comprising of self-attention
and feed-forward layers, where each block is capable of independently producing
a class token. Formulating an attack using only the last class token
(conventional approach) does not directly leverage the discriminative
information stored in the earlier tokens, leading to poor adversarial
transferability of ViTs. Using the compositional nature of ViT models, we
enhance the transferability of existing attacks by introducing two novel
strategies specific to the architecture of ViT models. (i) Self-Ensemble: We
propose a method to find multiple discriminative pathways by dissecting a
single ViT model into an ensemble of networks. This allows explicitly utilizing
class-specific information at each ViT block. (ii) Token Refinement: We then
propose to refine the tokens to further enhance the discriminative capacity at
each block of ViT. Our token refinement systematically combines the class
tokens with structural information preserved within the patch tokens. An
adversarial attack, when applied to such refined tokens within the ensemble of
classifiers found in a single vision transformer, has significantly higher
transferability.","['cs.CV', 'cs.AI', 'cs.LG']",268,158
Zero-Assignment Constraint for Graph Matching with Outliers,"Graph matching (GM), as a longstanding problem in computer vision and pattern
recognition, still suffers from numerous cluttered outliers in practical
applications. To address this issue, we present the zero-assignment constraint
(ZAC) for approaching the graph matching problem in the presence of outliers.
The underlying idea is to suppress the matchings of outliers by assigning
zero-valued vectors to the potential outliers in the obtained optimal
correspondence matrix. We provide elaborate theoretical analysis to the
problem, i.e., GM with ZAC, and figure out that the GM problem with and without
outliers are intrinsically different, which enables us to put forward a
sufficient condition to construct valid and reasonable objective function.
Consequently, we design an efficient outlier-robust algorithm to significantly
reduce the incorrect or redundant matchings caused by numerous outliers.
Extensive experiments demonstrate that our method can achieve the
state-of-the-art performance in terms of accuracy and efficiency, especially in
the presence of numerous outliers.",['cs.CV'],159,107
A new semi-supervised inductive transfer learning framework: Co-Transfer,"In many practical data mining scenarios, such as network intrusion detection,
Twitter spam detection, and computer-aided diagnosis, a source domain that is
different from but related to a target domain is very common. In addition, a
large amount of unlabeled data is available in both source and target domains,
but labeling each of them is difficult, expensive, time-consuming, and sometime
unnecessary. Therefore, it is very important and worthwhile to fully explore
the labeled and unlabeled data in source and target domains to settle the task
in target domain. In this paper, a new semi-supervised inductive transfer
learning framework, named Co-Transfer is proposed. Co-Transfer first generates
three TrAdaBoost classifiers for transfer learning from the source domain to
the target domain, and meanwhile another three TrAdaBoost classifiers are
generated for transfer learning from the target domain to the source domain,
using bootstraped samples from the original labeled data. In each round of
co-transfer, each group of TrAdaBoost classifiers are refined using the
carefully labeled data. Finally, the group of TrAdaBoost classifiers learned to
transfer from the source domain to the target domain produce the final
hypothesis. Experiments results illustrate Co-Transfer can effectively exploit
and reuse the labeled and unlabeled data in source and target domains.",['cs.LG'],210,102
Eliminating Search Intent Bias in Learning to Rank,"Click-through data has proven to be a valuable resource for improving
search-ranking quality. Search engines can easily collect click data, but
biases introduced in the data can make it difficult to use the data
effectively. In order to measure the effects of biases, many click models have
been proposed in the literature. However, none of the models can explain the
observation that users with different search intent (e.g., informational,
navigational, etc.) have different click behaviors. In this paper, we study how
differences in user search intent can influence click activities and determined
that there exists a bias between user search intent and the relevance of the
document relevance. Based on this observation, we propose a search intent bias
hypothesis that can be applied to most existing click models to improve their
ability to learn unbiased relevance. Experimental results demonstrate that
after adopting the search intent hypothesis, click models can better interpret
user clicks and substantially improve retrieval performance.","['cs.LG', 'cs.IR', 'stat.ML']",161,97
Learning Multimodal Graph-to-Graph Translation for Molecular Optimization,"We view molecular optimization as a graph-to-graph translation problem. The
goal is to learn to map from one molecular graph to another with better
properties based on an available corpus of paired molecules. Since molecules
can be optimized in different ways, there are multiple viable translations for
each input graph. A key challenge is therefore to model diverse translation
outputs. Our primary contributions include a junction tree encoder-decoder for
learning diverse graph translations along with a novel adversarial training
method for aligning distributions of molecules. Diverse output distributions in
our model are explicitly realized by low-dimensional latent vectors that
modulate the translation process. We evaluate our model on multiple molecular
optimization tasks and show that our model outperforms previous
state-of-the-art baselines.","['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",128,90
Actor-Critic Algorithm for High-dimensional Partial Differential Equations,"We develop a deep learning model to effectively solve high-dimensional
nonlinear parabolic partial differential equations (PDE). We follow Feynman-Kac
formula to reformulate PDE into the equivalent stochastic control problem
governed by a Backward Stochastic Differential Equation (BSDE) system. The
Markovian property of the BSDE is utilized in designing our neural network
architecture, which is inspired by the Actor-Critic algorithm usually applied
for deep Reinforcement Learning. Compared to the State-of-the-Art model, we
make several improvements including 1) largely reduced trainable parameters, 2)
faster convergence rate and 3) fewer hyperparameters to tune. We demonstrate
those improvements by solving a few well-known classes of PDEs such as
Hamilton-Jacobian-Bellman equation, Allen-Cahn equation and Black-Scholes
equation with dimensions on the order of 100.","['cs.LG', 'math.OC', 'stat.ML']",129,103
Context-Aware Single-Shot Detector,"SSD is one of the state-of-the-art object detection algorithms, and it
combines high detection accuracy with real-time speed. However, it is widely
recognized that SSD is less accurate in detecting small objects compared to
large objects, because it ignores the context from outside the proposal boxes.
In this paper, we present CSSD--a shorthand for context-aware single-shot
multibox object detector. CSSD is built on top of SSD, with additional layers
modeling multi-scale contexts. We describe two variants of CSSD, which differ
in their context layers, using dilated convolution layers (DiCSSD) and
deconvolution layers (DeCSSD) respectively. The experimental results show that
the multi-scale context modeling significantly improves the detection accuracy.
In addition, we study the relationship between effective receptive fields
(ERFs) and the theoretical receptive fields (TRFs), particularly on a VGGNet.
The empirical results further strengthen our conclusion that SSD coupled with
context layers achieves better detection results especially for small objects
($+3.2\% {\rm AP}_{@0.5}$ on MS-COCO compared to the newest SSD), while
maintaining comparable runtime performance.",['cs.CV'],179,118
Classifying sequences by the optimized dissimilarity space embedding approach: a case study on the solubility analysis of the E. coli proteome,"We evaluate a version of the recently-proposed classification system named
Optimized Dissimilarity Space Embedding (ODSE) that operates in the input space
of sequences of generic objects. The ODSE system has been originally presented
as a classification system for patterns represented as labeled graphs. However,
since ODSE is founded on the dissimilarity space representation of the input
data, the classifier can be easily adapted to any input domain where it is
possible to define a meaningful dissimilarity measure. Here we demonstrate the
effectiveness of the ODSE classifier for sequences by considering an
application dealing with the recognition of the solubility degree of the
Escherichia coli proteome. Solubility, or analogously aggregation propensity,
is an important property of protein molecules, which is intimately related to
the mechanisms underlying the chemico-physical process of folding. Each protein
of our dataset is initially associated with a solubility degree and it is
represented as a sequence of symbols, denoting the 20 amino acid residues. The
herein obtained computational results, which we stress that have been achieved
with no context-dependent tuning of the ODSE system, confirm the validity and
generality of the ODSE-based approach for structured data classification.","['cs.CV', 'cs.AI', 'physics.bio-ph', 'q-bio.BM', 'I.5']",194,121
Dilated Fully Convolutional Neural Network for Depth Estimation from a Single Image,"Depth prediction plays a key role in understanding a 3D scene. Several
techniques have been developed throughout the years, among which Convolutional
Neural Network has recently achieved state-of-the-art performance on estimating
depth from a single image. However, traditional CNNs suffer from the lower
resolution and information loss caused by the pooling layers. And oversized
parameters generated from fully connected layers often lead to a exploded
memory usage problem. In this paper, we present an advanced Dilated Fully
Convolutional Neural Network to address the deficiencies. Taking advantages of
the exponential expansion of the receptive field in dilated convolutions, our
model can minimize the loss of resolution. It also reduces the amount of
parameters significantly by replacing the fully connected layers with the fully
convolutional layers. We show experimentally on NYU Depth V2 datasets that the
depth prediction obtained from our model is considerably closer to ground truth
than that from traditional CNNs techniques.",['cs.CV'],155,107
Unsupervised and Supervised Principal Component Analysis: Tutorial,"This is a detailed tutorial paper which explains the Principal Component
Analysis (PCA), Supervised PCA (SPCA), kernel PCA, and kernel SPCA. We start
with projection, PCA with eigen-decomposition, PCA with one and multiple
projection directions, properties of the projection matrix, reconstruction
error minimization, and we connect to auto-encoder. Then, PCA with singular
value decomposition, dual PCA, and kernel PCA are covered. SPCA using both
scoring and Hilbert-Schmidt independence criterion are explained. Kernel SPCA
using both direct and dual approaches are then introduced. We cover all cases
of projection and reconstruction of training and out-of-sample data. Finally,
some simulations are provided on Frey and AT&T face datasets for verifying the
theory in practice.","['stat.ML', 'cs.LG']",118,78
Focal and Efficient IOU Loss for Accurate Bounding Box Regression,"In object detection, bounding box regression (BBR) is a crucial step that
determines the object localization performance. However, we find that most
previous loss functions for BBR have two main drawbacks: (i) Both $\ell_n$-norm
and IOU-based loss functions are inefficient to depict the objective of BBR,
which leads to slow convergence and inaccurate regression results. (ii) Most of
the loss functions ignore the imbalance problem in BBR that the large number of
anchor boxes which have small overlaps with the target boxes contribute most to
the optimization of BBR. To mitigate the adverse effects caused thereby, we
perform thorough studies to exploit the potential of BBR losses in this paper.
Firstly, an Efficient Intersection over Union (EIOU) loss is proposed, which
explicitly measures the discrepancies of three geometric factors in BBR, i.e.,
the overlap area, the central point and the side length. After that, we state
the Effective Example Mining (EEM) problem and propose a regression version of
focal loss to make the regression process focus on high-quality anchor boxes.
Finally, the above two parts are combined to obtain a new loss function, namely
Focal-EIOU loss. Extensive experiments on both synthetic and real datasets are
performed. Notable superiorities on both the convergence speed and the
localization accuracy can be achieved over other BBR losses.",['cs.CV'],219,137
3D RoI-aware U-Net for Accurate and Efficient Colorectal Tumor Segmentation,"Segmentation of colorectal cancerous regions from 3D Magnetic Resonance (MR)
images is a crucial procedure for radiotherapy which conventionally requires
accurate delineation of tumour boundaries at an expense of labor, time and
reproducibility. While deep learning based methods serve good baselines in 3D
image segmentation tasks, small applicable patch size limits effective
receptive field and degrades segmentation performance. In addition, Regions of
interest (RoIs) localization from large whole volume 3D images serves as a
preceding operation that brings about multiple benefits in terms of speed,
target completeness, reduction of false positives. Distinct from sliding window
or non-joint localization-segmentation based models, we propose a novel
multitask framework referred to as 3D RoI-aware U-Net (3D RU-Net), for RoI
localization and in-region segmentation where the two tasks share one backbone
encoder network. With the region proposals from the encoder, we crop
multi-level RoI in-region features from the encoder to form a GPU
memory-efficient decoder for detailpreserving segmentation and therefore
enlarged applicable volume size and effective receptive field. To effectively
train the model, we designed a Dice formulated loss function for the
global-to-local multi-task learning procedure. Based on the efficiency gains,
we went on to ensemble models with different receptive fields to achieve even
higher performance costing minor extra computational expensiveness. Extensive
experiments were conducted on 64 cancerous cases with a four-fold
cross-validation, and the results showed significant superiority in terms of
accuracy and efficiency over conventional frameworks. In conclusion, the
proposed method has a huge potential for extension to other 3D object
segmentation tasks from medical images due to its inherent generalizability.
The code for the proposed method is publicly available.",['cs.CV'],282,185
Deep learning for time series classification,"Time series analysis is a field of data science which is interested in
analyzing sequences of numerical values ordered in time. Time series are
particularly interesting because they allow us to visualize and understand the
evolution of a process over time. Their analysis can reveal trends,
relationships and similarities across the data. There exists numerous fields
containing data in the form of time series: health care (electrocardiogram,
blood sugar, etc.), activity recognition, remote sensing, finance (stock market
price), industry (sensors), etc. Time series classification consists of
constructing algorithms dedicated to automatically label time series data. The
sequential aspect of time series data requires the development of algorithms
that are able to harness this temporal property, thus making the existing
off-the-shelf machine learning models for traditional tabular data suboptimal
for solving the underlying task. In this context, deep learning has emerged in
recent years as one of the most effective methods for tackling the supervised
classification task, particularly in the field of computer vision. The main
objective of this thesis was to study and develop deep neural networks
specifically constructed for the classification of time series data. We thus
carried out the first large scale experimental study allowing us to compare the
existing deep methods and to position them compared other non-deep learning
based state-of-the-art methods. Subsequently, we made numerous contributions in
this area, notably in the context of transfer learning, data augmentation,
ensembling and adversarial attacks. Finally, we have also proposed a novel
architecture, based on the famous Inception network (Google), which ranks among
the most efficient to date.","['cs.LG', 'cs.AI', 'stat.ML']",265,164
Fairness Through Counterfactual Utilities,"Group fairness definitions such as Demographic Parity and Equal Opportunity
make assumptions about the underlying decision-problem that restrict them to
classification problems. Prior work has translated these definitions to other
machine learning environments, such as unsupervised learning and reinforcement
learning, by implementing their closest mathematical equivalent. As a result,
there are numerous bespoke interpretations of these definitions. Instead, we
provide a generalized set of group fairness definitions that unambiguously
extend to all machine learning environments while still retaining their
original fairness notions. We derive two fairness principles that enable such a
generalized framework. First, our framework measures outcomes in terms of
utilities, rather than predictions, and does so for both the decision-algorithm
and the individual. Second, our framework considers counterfactual outcomes,
rather than just observed outcomes, thus preventing loopholes where fairness
criteria are satisfied through self-fulfilling prophecies. We provide concrete
examples of how our counterfactual utility fairness framework resolves known
fairness issues in classification, clustering, and reinforcement learning
problems. We also show that many of the bespoke interpretations of Demographic
Parity and Equal Opportunity fit nicely as special cases of our framework.","['cs.LG', 'cs.CY']",185,115
Graph Regularized Nonnegative Tensor Ring Decomposition for Multiway Representation Learning,"Tensor ring (TR) decomposition is a powerful tool for exploiting the low-rank
nature of multiway data and has demonstrated great potential in a variety of
important applications. In this paper, nonnegative tensor ring (NTR)
decomposition and graph regularized NTR (GNTR) decomposition are proposed,
where the former equips TR decomposition with local feature extraction by
imposing nonnegativity on the core tensors and the latter is additionally able
to capture manifold geometry information of tensor data, both significantly
extend the applications of TR decomposition for nonnegative multiway
representation learning. Accelerated proximal gradient based methods are
derived for NTR and GNTR. The experimental result demonstrate that the proposed
algorithms can extract parts-based basis with rich colors and rich lines from
tensor objects that provide more interpretable and meaningful representation,
and hence yield better performance than the state-of-the-art tensor based
methods in clustering and classification tasks.",['cs.CV'],147,98
"Rotation, Translation, and Cropping for Zero-Shot Generalization","Deep Reinforcement Learning (DRL) has shown impressive performance on domains
with visual inputs, in particular various games. However, the agent is usually
trained on a fixed environment, e.g. a fixed number of levels. A growing mass
of evidence suggests that these trained models fail to generalize to even
slight variations of the environments they were trained on. This paper advances
the hypothesis that the lack of generalization is partly due to the input
representation, and explores how rotation, cropping and translation could
increase generality. We show that a cropped, translated and rotated observation
can get better generalization on unseen levels of two-dimensional arcade games
from the GVGAI framework. The generality of the agents is evaluated on both
human-designed and procedurally generated levels.","['cs.LG', 'cs.CV', 'stat.ML']",125,91
Machine Learning on Graphs: A Model and Comprehensive Taxonomy,"There has been a surge of recent interest in learning representations for
graph-structured data. Graph representation learning methods have generally
fallen into three main categories, based on the availability of labeled data.
The first, network embedding (such as shallow graph embedding or graph
auto-encoders), focuses on learning unsupervised representations of relational
structure. The second, graph regularized neural networks, leverages graphs to
augment neural network losses with a regularization objective for
semi-supervised learning. The third, graph neural networks, aims to learn
differentiable functions over discrete topologies with arbitrary structure.
However, despite the popularity of these areas there has been surprisingly
little work on unifying the three paradigms. Here, we aim to bridge the gap
between graph neural networks, network embedding and graph regularization
models. We propose a comprehensive taxonomy of representation learning methods
for graph-structured data, aiming to unify several disparate bodies of work.
Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which
generalizes popular algorithms for semi-supervised learning on graphs (e.g.
GraphSage, Graph Convolutional Networks, Graph Attention Networks), and
unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc)
into a single consistent approach. To illustrate the generality of this
approach, we fit over thirty existing methods into this framework. We believe
that this unifying view both provides a solid foundation for understanding the
intuition behind these methods, and enables future research in the area.","['cs.LG', 'cs.NE', 'cs.SI', 'stat.ML']",232,138
Cross-View Exocentric to Egocentric Video Synthesis,"Cross-view video synthesis task seeks to generate video sequences of one view
from another dramatically different view. In this paper, we investigate the
exocentric (third-person) view to egocentric (first-person) view video
generation task. This is challenging because egocentric view sometimes is
remarkably different from the exocentric view. Thus, transforming the
appearances across the two different views is a non-trivial task. Particularly,
we propose a novel Bi-directional Spatial Temporal Attention Fusion Generative
Adversarial Network (STA-GAN) to learn both spatial and temporal information to
generate egocentric video sequences from the exocentric view. The proposed
STA-GAN consists of three parts: temporal branch, spatial branch, and attention
fusion. First, the temporal and spatial branches generate a sequence of fake
frames and their corresponding features. The fake frames are generated in both
downstream and upstream directions for both temporal and spatial branches.
Next, the generated four different fake frames and their corresponding features
(spatial and temporal branches in two directions) are fed into a novel
multi-generation attention fusion module to produce the final video sequence.
Meanwhile, we also propose a novel temporal and spatial dual-discriminator for
more robust network optimization. Extensive experiments on the Side2Ego and
Top2Ego datasets show that the proposed STA-GAN significantly outperforms the
existing methods.","['cs.CV', 'cs.MM']",213,113
Low-Rank Bottleneck in Multi-head Attention Models,"Attention based Transformer architecture has enabled significant advances in
the field of natural language processing. In addition to new pre-training
techniques, recent improvements crucially rely on working with a relatively
larger embedding dimension for tokens. Unfortunately, this leads to models that
are prohibitively large to be employed in the downstream tasks. In this paper
we identify one of the important factors contributing to the large embedding
size requirement. In particular, our analysis highlights that the scaling
between the number of heads and the size of each head in the current
architecture gives rise to a low-rank bottleneck in attention heads, causing
this limitation. We further validate this in our experiments. As a solution we
propose to set the head size of an attention unit to input sequence length, and
independent of the number of heads, resulting in multi-head attention layers
with provably more expressive power. We empirically show that this allows us to
train models with a relatively smaller embedding dimension and with better
performance scaling.","['cs.LG', 'stat.ML']",169,106
Learning a Mixture of Deep Networks for Single Image Super-Resolution,"Single image super-resolution (SR) is an ill-posed problem which aims to
recover high-resolution (HR) images from their low-resolution (LR)
observations. The crux of this problem lies in learning the complex mapping
between low-resolution patches and the corresponding high-resolution patches.
Prior arts have used either a mixture of simple regression models or a single
non-linear neural network for this propose. This paper proposes the method of
learning a mixture of SR inference modules in a unified framework to tackle
this problem. Specifically, a number of SR inference modules specialized in
different image local patterns are first independently applied on the LR image
to obtain various HR estimates, and the resultant HR estimates are adaptively
aggregated to form the final HR image. By selecting neural networks as the SR
inference module, the whole procedure can be incorporated into a unified
network and be optimized jointly. Extensive experiments are conducted to
investigate the relation between restoration performance and different network
architectures. Compared with other current image SR approaches, our proposed
method achieves state-of-the-arts restoration results on a wide range of images
consistently while allowing more flexible design choices. The source codes are
available in http://www.ifp.illinois.edu/~dingliu2/accv2016.",['cs.CV'],208,132
A Novel GAN-based Fault Diagnosis Approach for Imbalanced Industrial Time Series,"This paper proposes a novel fault diagnosis approach based on generative
adversarial networks (GAN) for imbalanced industrial time series where normal
samples are much larger than failure cases. We combine a well-designed feature
extractor with GAN to help train the whole network. Aimed at obtaining data
distribution and hidden pattern in both original distinguishing features and
latent space, the encoder-decoder-encoder three-sub-network is employed in GAN,
based on Deep Convolution Generative Adversarial Networks (DCGAN) but without
Tanh activation layer and only trained on normal samples. In order to verify
the validity and feasibility of our approach, we test it on rolling bearing
data from Case Western Reserve University and further verify it on data
collected from our laboratory. The results show that our proposed approach can
achieve excellent performance in detecting faulty by outputting much larger
evaluation scores.","['cs.LG', 'stat.ML']",142,110
A New Ratio Image Based CNN Algorithm For SAR Despeckling,"In SAR domain many application like classification, detection and
segmentation are impaired by speckle. Hence, despeckling of SAR images is the
key for scene understanding. Usually despeckling filters face the trade-off of
speckle suppression and information preservation. In the last years deep
learning solutions for speckle reduction have been proposed. One the biggest
issue for these methods is how to train a network given the lack of a
reference. In this work we proposed a convolutional neural network based
solution trained on simulated data. We propose the use of a cost function
taking into account both spatial and statistical properties. The aim is two
fold: overcome the trade-off between speckle suppression and details
suppression; find a suitable cost function for despeckling in unsupervised
learning. The algorithm is validated on both real and simulated data, showing
interesting performances.",['cs.CV'],139,94
Robust Generative Adversarial Network,"Generative adversarial networks (GANs) are powerful generative models, but
usually suffer from instability and generalization problem which may lead to
poor generations. Most existing works focus on stabilizing the training of the
discriminator while ignoring the generalization properties. In this work, we
aim to improve the generalization capability of GANs by promoting the local
robustness within the small neighborhood of the training samples. We also prove
that the robustness in small neighborhood of training sets can lead to better
generalization. Particularly, we design a robust optimization framework where
the generator and discriminator compete with each other in a
\textit{worst-case} setting within a small Wasserstein ball. The generator
tries to map \textit{the worst input distribution} (rather than a Gaussian
distribution used in most GANs) to the real data distribution, while the
discriminator attempts to distinguish the real and fake distribution
\textit{with the worst perturbation}. We have proved that our robust method can
obtain a tighter generalization upper bound than traditional GANs under mild
assumptions, ensuring a theoretical superiority of RGAN over GANs. A series of
experiments on CIFAR-10, STL-10 and CelebA datasets indicate that our proposed
robust framework can improve on five baseline GAN models substantially and
consistently.","['cs.LG', 'stat.ML']",203,124
Representing Point Clouds with Generative Conditional Invertible Flow Networks,"In this paper, we propose a simple yet effective method to represent point
clouds as sets of samples drawn from a cloud-specific probability distribution.
This interpretation matches intrinsic characteristics of point clouds: the
number of points and their ordering within a cloud is not important as all
points are drawn from the proximity of the object boundary. We postulate to
represent each cloud as a parameterized probability distribution defined by a
generative neural network. Once trained, such a model provides a natural
framework for point cloud manipulation operations, such as aligning a new cloud
into a default spatial orientation. To exploit similarities between same-class
objects and to improve model performance, we turn to weight sharing: networks
that model densities of points belonging to objects in the same family share
all parameters with the exception of a small, object-specific embedding vector.
We show that these embedding vectors capture semantic relationships between
objects. Our method leverages generative invertible flow networks to learn
embeddings as well as to generate point clouds. Thanks to this formulation and
contrary to similar approaches, we are able to train our model in an end-to-end
fashion. As a result, our model offers competitive or superior quantitative
results on benchmark datasets, while enabling unprecedented capabilities to
perform cloud manipulation tasks, such as point cloud registration and
regeneration, by a generative network.","['cs.CV', 'cs.LG']",227,138
Dilated Spatial Generative Adversarial Networks for Ergodic Image Generation,"Generative models have recently received renewed attention as a result of
adversarial learning. Generative adversarial networks consist of samples
generation model and a discrimination model able to distinguish between genuine
and synthetic samples. In combination with convolutional (for the
discriminator) and de-convolutional (for the generator) layers, they are
particularly suitable for image generation, especially of natural scenes.
However, the presence of fully connected layers adds global dependencies in the
generated images. This may lead to high and global variations in the generated
sample for small local variations in the input noise. In this work we propose
to use architec-tures based on fully convolutional networks (including among
others dilated layers), architectures specifically designed to generate
globally ergodic images, that is images without global dependencies. Conducted
experiments reveal that these architectures are well suited for generating
natural textures such as geologic structures .","['cs.CV', 'cs.LG', 'eess.IV']",142,97
3D Spatial Recognition without Spatially Labeled 3D,"We introduce WyPR, a Weakly-supervised framework for Point cloud Recognition,
requiring only scene-level class tags as supervision. WyPR jointly addresses
three core 3D recognition tasks: point-level semantic segmentation, 3D proposal
generation, and 3D object detection, coupling their predictions through self
and cross-task consistency losses. We show that in conjunction with standard
multiple-instance learning objectives, WyPR can detect and segment objects in
point cloud data without access to any spatial labels at training time. We
demonstrate its efficacy using the ScanNet and S3DIS datasets, outperforming
prior state of the art on weakly-supervised segmentation by more than 6% mIoU.
In addition, we set up the first benchmark for weakly-supervised 3D object
detection on both datasets, where WyPR outperforms standard approaches and
establishes strong baselines for future work.","['cs.CV', 'cs.AI', 'cs.LG', 'cs.MM']",132,103
"Towards Understanding Adversarial Examples Systematically: Exploring Data Size, Task and Model Factors","Most previous works usually explained adversarial examples from several
specific perspectives, lacking relatively integral comprehension about this
problem. In this paper, we present a systematic study on adversarial examples
from three aspects: the amount of training data, task-dependent and
model-specific factors. Particularly, we show that adversarial generalization
(i.e. test accuracy on adversarial examples) for standard training requires
more data than standard generalization (i.e. test accuracy on clean examples);
and uncover the global relationship between generalization and robustness with
respect to the data size especially when data is augmented by generative
models. This reveals the trade-off correlation between standard generalization
and robustness in limited training data regime and their consistency when data
size is large enough. Furthermore, we explore how different task-dependent and
model-specific factors influence the vulnerability of deep neural networks by
extensive empirical analysis. Relevant recommendations on defense against
adversarial attacks are provided as well. Our results outline a potential path
towards the luminous and systematic understanding of adversarial examples.","['cs.LG', 'stat.ML']",168,109
Vector Learning for Cross Domain Representations,"Recently, generative adversarial networks have gained a lot of popularity for
image generation tasks. However, such models are associated with complex
learning mechanisms and demand very large relevant datasets. This work borrows
concepts from image and video captioning models to form an image generative
framework. The model is trained in a similar fashion as recurrent captioning
model and uses the learned weights for image generation. This is done in an
inverse direction, where the input is a caption and the output is an image. The
vector representation of the sentence and frames are extracted from an
encoder-decoder model which is initially trained on similar sentence and image
pairs. Our model conditions image generation on a natural language caption. We
leverage a sequence-to-sequence model to generate synthetic captions that have
the same meaning for having a robust image generation. One key advantage of our
method is that the traditional image captioning datasets can be used for
synthetic sentence paraphrases. Results indicate that images generated through
multiple captions are better at capturing the semantic meaning of the family of
captions.","['cs.LG', 'stat.ML']",181,107
Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization,"Recent years have seen a rise in the development of representational learning
methods for graph data. Most of these methods, however, focus on node-level
representation learning at various scales (e.g., microscopic, mesoscopic, and
macroscopic node embedding). In comparison, methods for representation learning
on whole graphs are currently relatively sparse. In this paper, we propose a
novel unsupervised whole graph embedding method. Our method uses spectral graph
wavelets to capture topological similarities on each k-hop sub-graph between
nodes and uses them to learn embeddings for the whole graph. We evaluate our
method against 12 well-known baselines on 4 real-world datasets and show that
our method achieves the best performance across all experiments, outperforming
the current state-of-the-art by a considerable margin.","['cs.LG', 'cs.AI', 'cs.SI']",128,93
DeepTopPush: Simple and Scalable Method for Accuracy at the Top,"Accuracy at the top is a special class of binary classification problems
where the performance is evaluated only on a small number of relevant (top)
samples. Applications include information retrieval systems or processes with
manual (expensive) postprocessing. This leads to the minimization of irrelevant
samples above a threshold. We consider classifiers in the form of an arbitrary
(deep) network and propose a new method DeepTopPush for minimizing the top loss
function. Since the threshold depends on all samples, the problem is
non-decomposable. We modify the stochastic gradient descent to handle the
non-decomposability in an end-to-end training manner and propose a way to
estimate the threshold only from values on the current minibatch. We
demonstrate the good performance of DeepTopPush on visual recognition datasets
and on a real-world application of selecting a small number of molecules for
further drug testing.","['cs.LG', 'math.OC', 'stat.ML']",144,91
Detecting abnormalities in resting-state dynamics: An unsupervised learning approach,"Resting-state functional MRI (rs-fMRI) is a rich imaging modality that
captures spontaneous brain activity patterns, revealing clues about the
connectomic organization of the human brain. While many rs-fMRI studies have
focused on static measures of functional connectivity, there has been a recent
surge in examining the temporal patterns in these data. In this paper, we
explore two strategies for capturing the normal variability in resting-state
activity across a healthy population: (a) an autoencoder approach on the
rs-fMRI sequence, and (b) a next frame prediction strategy. We show that both
approaches can learn useful representations of rs-fMRI data and demonstrate
their novel application for abnormality detection in the context of
discriminating autism patients from healthy controls.","['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML']",121,89
H3DNet: 3D Object Detection Using Hybrid Geometric Primitives,"We introduce H3DNet, which takes a colorless 3D point cloud as input and
outputs a collection of oriented object bounding boxes (or BB) and their
semantic labels. The critical idea of H3DNet is to predict a hybrid set of
geometric primitives, i.e., BB centers, BB face centers, and BB edge centers.
We show how to convert the predicted geometric primitives into object proposals
by defining a distance function between an object and the geometric primitives.
This distance function enables continuous optimization of object proposals, and
its local minimums provide high-fidelity object proposals. H3DNet then utilizes
a matching and refinement module to classify object proposals into detected
objects and fine-tune the geometric parameters of the detected objects. The
hybrid set of geometric primitives not only provides more accurate signals for
object detection than using a single type of geometric primitives, but it also
provides an overcomplete set of constraints on the resulting 3D layout.
Therefore, H3DNet can tolerate outliers in predicted geometric primitives. Our
model achieves state-of-the-art 3D detection results on two large datasets with
real 3D scans, ScanNet and SUN RGB-D.",['cs.CV'],188,115
Munchausen Reinforcement Learning,"Bootstrapping is a core mechanism in Reinforcement Learning (RL). Most
algorithms, based on temporal differences, replace the true value of a
transiting state by their current estimate of this value. Yet, another estimate
could be leveraged to bootstrap RL: the current policy. Our core contribution
stands in a very simple idea: adding the scaled log-policy to the immediate
reward. We show that slightly modifying Deep Q-Network (DQN) in that way
provides an agent that is competitive with distributional methods on Atari
games, without making use of distributional RL, n-step returns or prioritized
replay. To demonstrate the versatility of this idea, we also use it together
with an Implicit Quantile Network (IQN). The resulting agent outperforms
Rainbow on Atari, installing a new State of the Art with very little
modifications to the original algorithm. To add to this empirical study, we
provide strong theoretical insights on what happens under the hood -- implicit
Kullback-Leibler regularization and increase of the action-gap.","['cs.LG', 'stat.ML']",163,115
Attribute-Induced Bias Eliminating for Transductive Zero-Shot Learning,"Transductive Zero-shot learning (ZSL) targets to recognize the unseen
categories by aligning the visual and semantic information in a joint embedding
space. There exist four kinds of domain biases in Transductive ZSL, i.e.,
visual bias and semantic bias between two domains and two visual-semantic
biases in respective seen and unseen domains, but existing work only focuses on
the part of them, which leads to severe semantic ambiguity during the knowledge
transfer. To solve the above problem, we propose a novel Attribute-Induced Bias
Eliminating (AIBE) module for Transductive ZSL. Specifically, for the visual
bias between two domains, the Mean-Teacher module is first leveraged to bridge
the visual representation discrepancy between two domains with unsupervised
learning and unlabelled images. Then, an attentional graph attribute embedding
is proposed to reduce the semantic bias between seen and unseen categories,
which utilizes the graph operation to capture the semantic relationship between
categories. Besides, to reduce the semantic-visual bias in the seen domain, we
align the visual center of each category, instead of the individual visual data
point, with the corresponding semantic attributes, which further preserves the
semantic relationship in the embedding space. Finally, for the semantic-visual
bias in the unseen domain, an unseen semantic alignment constraint is designed
to align visual and semantic space in an unsupervised manner. The evaluations
on several benchmarks demonstrate the effectiveness of the proposed method,
e.g., obtaining the 82.8%/75.5%, 97.1%/82.5%, and 73.2%/52.1% for
Conventional/Generalized ZSL settings for CUB, AwA2, and SUN datasets,
respectively.",['cs.CV'],261,134
Learning to Design Circuits,"Analog IC design relies on human experts to search for parameters that
satisfy circuit specifications with their experience and intuitions, which is
highly labor intensive, time consuming and suboptimal. Machine learning is a
promising tool to automate this process. However, supervised learning is
difficult for this task due to the low availability of training data: 1)
Circuit simulation is slow, thus generating large-scale dataset is
time-consuming; 2) Most circuit designs are propitiatory IPs within individual
IC companies, making it expensive to collect large-scale datasets. We propose
Learning to Design Circuits (L2DC) to leverage reinforcement learning that
learns to efficiently generate new circuits data and to optimize circuits. We
fix the schematic, and optimize the parameters of the transistors automatically
by training an RL agent with no prior knowledge about optimizing circuits.
After iteratively getting observations, generating a new set of transistor
parameters, getting a reward, and adjusting the model, L2DC is able to optimize
circuits. We evaluate L2DC on two transimpedance amplifiers. Trained for a day,
our RL agent can achieve comparable or better performance than human experts
trained for a quarter. It first learns to meet hard-constraints (eg. gain,
bandwidth), and then learns to optimize good-to-have targets (eg. area, power).
Compared with grid search-aided human design, L2DC can achieve
$\mathbf{250}\boldsymbol{\times}$ higher sample efficiency with comparable
performance. Under the same runtime constraint, the performance of L2DC is also
better than Bayesian Optimization.",['cs.LG'],242,154
Explainable Online Validation of Machine Learning Models for Practical Applications,"We present a reformulation of the regression and classification, which aims
to validate the result of a machine learning algorithm. Our reformulation
simplifies the original problem and validates the result of the machine
learning algorithm using the training data. Since the validation of machine
learning algorithms must always be explainable, we perform our experiments with
the kNN algorithm as well as with an algorithm based on conditional
probabilities, which is proposed in this work. For the evaluation of our
approach, three publicly available data sets were used and three classification
and two regression problems were evaluated. The presented algorithm based on
conditional probabilities is also online capable and requires only a fraction
of memory compared to the kNN algorithm.","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",119,72
CDGAN: Cyclic Discriminative Generative Adversarial Networks for Image-to-Image Transformation,"Image-to-image transformation is a kind of problem, where the input image
from one visual representation is transformed into the output image of another
visual representation. Since 2014, Generative Adversarial Networks (GANs) have
facilitated a new direction to tackle this problem by introducing the generator
and the discriminator networks in its architecture. Many recent works, like
Pix2Pix, CycleGAN, DualGAN, PS2MAN and CSGAN handled this problem with the
required generator and discriminator networks and choice of the different
losses that are used in the objective functions. In spite of these works, still
there is a gap to fill in terms of both the quality of the images generated
that should look more realistic and as much as close to the ground truth
images. In this work, we introduce a new Image-to-Image Transformation network
named Cyclic Discriminative Generative Adversarial Networks (CDGAN) that fills
the above mentioned gaps. The proposed CDGAN generates high quality and more
realistic images by incorporating the additional discriminator networks for
cycled images in addition to the original architecture of the CycleGAN. To
demonstrate the performance of the proposed CDGAN, it is tested over three
different baseline image-to-image transformation datasets. The quantitative
metrics such as pixel-wise similarity, structural level similarity and
perceptual level similarity are used to judge the performance. Moreover, the
qualitative results are also analyzed and compared with the state-of-the-art
methods. The proposed CDGAN method clearly outperformed all the
state-of-the-art methods when compared over the three baseline Image-to-Image
transformation datasets.","['cs.CV', 'cs.LG', 'eess.IV']",257,138
Weighing Counts: Sequential Crowd Counting by Reinforcement Learning,"We formulate counting as a sequential decision problem and present a novel
crowd counting model solvable by deep reinforcement learning. In contrast to
existing counting models that directly output count values, we divide one-step
estimation into a sequence of much easier and more tractable sub-decision
problems. Such sequential decision nature corresponds exactly to a physical
process in reality scale weighing. Inspired by scale weighing, we propose a
novel 'counting scale' termed LibraNet where the count value is analogized by
weight. By virtually placing a crowd image on one side of a scale, LibraNet
(agent) sequentially learns to place appropriate weights on the other side to
match the crowd count. At each step, LibraNet chooses one weight (action) from
the weight box (the pre-defined action pool) according to the current crowd
image features and weights placed on the scale pan (state). LibraNet is
required to learn to balance the scale according to the feedback of the needle
(Q values). We show that LibraNet exactly implements scale weighing by
visualizing the decision process how LibraNet chooses actions. Extensive
experiments demonstrate the effectiveness of our design choices and report
state-of-the-art results on a few crowd counting benchmarks. We also
demonstrate good cross-dataset generalization of LibraNet. Code and models are
made available at: https://git.io/libranet",['cs.CV'],220,128
Deep Learning Based Steel Pipe Weld Defect Detection,"Steel pipes are widely used in high-risk and high-pressure scenarios such as
oil, chemical, natural gas, shale gas, etc. If there is some defect in steel
pipes, it will lead to serious adverse consequences. Applying object detection
in the field of deep learning to pipe weld defect detection and identification
can effectively improve inspection efficiency and promote the development of
industrial automation. Most predecessors used traditional computer vision
methods applied to detect defects of steel pipe weld seams. However,
traditional computer vision methods rely on prior knowledge and can only detect
defects with a single feature, so it is difficult to complete the task of
multi-defect classification, while deep learning is end-to-end. In this paper,
the state-of-the-art single-stage object detection algorithm YOLOv5 is proposed
to be applied to the field of steel pipe weld defect detection, and compared
with the two-stage representative object detection algorithm Faster R-CNN. The
experimental results show that applying YOLOv5 to steel pipe weld defect
detection can greatly improve the accuracy, complete the multi-classification
task, and meet the criteria of real-time detection.","['cs.CV', 'cs.AI', '68T07, 65D19', 'I.4.0; I.2.10']",189,108
Cross-Task Representation Learning for Anatomical Landmark Detection,"Recently, there is an increasing demand for automatically detecting
anatomical landmarks which provide rich structural information to facilitate
subsequent medical image analysis. Current methods related to this task often
leverage the power of deep neural networks, while a major challenge in fine
tuning such models in medical applications arises from insufficient number of
labeled samples. To address this, we propose to regularize the knowledge
transfer across source and target tasks through cross-task representation
learning. The proposed method is demonstrated for extracting facial anatomical
landmarks which facilitate the diagnosis of fetal alcohol syndrome. The source
and target tasks in this work are face recognition and landmark detection,
respectively. The main idea of the proposed method is to retain the feature
representations of the source model on the target task data, and to leverage
them as an additional source of supervisory signals for regularizing the target
model learning, thereby improving its performance under limited training
samples. Concretely, we present two approaches for the proposed representation
learning by constraining either final or intermediate model features on the
target model. Experimental results on a clinical face image dataset demonstrate
that the proposed approach works well with few labeled data, and outperforms
other compared approaches.","['cs.CV', 'cs.LG', 'eess.IV']",201,130
Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment,"Reinforcement learning from large-scale offline datasets provides us with the
ability to learn policies without potentially unsafe or impractical
exploration. Significant progress has been made in the past few years in
dealing with the challenge of correcting for differing behavior between the
data collection and learned policies. However, little attention has been paid
to potentially changing dynamics when transferring a policy to the online
setting, where performance can be up to 90% reduced for existing methods. In
this paper we address this problem with Augmented World Models (AugWM). We
augment a learned dynamics model with simple transformations that seek to
capture potential changes in physical properties of the robot, leading to more
robust policies. We not only train our policy in this new setting, but also
provide it with the sampled augmentation as a context, allowing it to adapt to
changes in the environment. At test time we learn the context in a
self-supervised fashion by approximating the augmentation which corresponds to
the new environment. We rigorously evaluate our approach on over 100 different
changed dynamics settings, and show that this simple approach can significantly
improve the zero-shot generalization of a recent state-of-the-art baseline,
often achieving successful policies where the baseline fails.","['cs.LG', 'cs.AI']",208,138
Food Classification with Convolutional Neural Networks and Multi-Class Linear Discernment Analysis,"Convolutional neural networks (CNNs) have been successful in representing the
fully-connected inferencing ability perceived to be seen in the human brain:
they take full advantage of the hierarchy-style patterns commonly seen in
complex data and develop more patterns using simple features. Countless
implementations of CNNs have shown how strong their ability is to learn these
complex patterns, particularly in the realm of image classification. However,
the cost of getting a high performance CNN to a so-called ""state of the art""
level is computationally costly. Even when using transfer learning, which
utilize the very deep layers from models such as MobileNetV2, CNNs still take a
great amount of time and resources. Linear discriminant analysis (LDA), a
generalization of Fisher's linear discriminant, can be implemented in a
multi-class classification method to increase separability of class features
while not needing a high performance system to do so for image classification.
Similarly, we also believe LDA has great promise in performing well. In this
paper, we discuss our process of developing a robust CNN for food
classification as well as our effective implementation of multi-class LDA and
prove that (1) CNN is superior to LDA for image classification and (2) why LDA
should not be left out of the races for image classification, particularly for
binary cases.","['cs.CV', 'cs.AI']",219,135
Explainable cardiac pathology classification on cine MRI with motion characterization by semi-supervised learning of apparent flow,"We propose a method to classify cardiac pathology based on a novel approach
to extract image derived features to characterize the shape and motion of the
heart. An original semi-supervised learning procedure, which makes efficient
use of a large amount of non-segmented images and a small amount of images
segmented manually by experts, is developed to generate pixel-wise apparent
flow between two time points of a 2D+t cine MRI image sequence. Combining the
apparent flow maps and cardiac segmentation masks, we obtain a local apparent
flow corresponding to the 2D motion of myocardium and ventricular cavities.
This leads to the generation of time series of the radius and thickness of
myocardial segments to represent cardiac motion. These time series of motion
features are reliable and explainable characteristics of pathological cardiac
motion. Furthermore, they are combined with shape-related features to classify
cardiac pathologies. Using only nine feature values as input, we propose an
explainable, simple and flexible model for pathology classification. On ACDC
training set and testing set, the model achieves 95% and 94% respectively as
classification accuracy. Its performance is hence comparable to that of the
state-of-the-art. Comparison with various other models is performed to outline
some advantages of our model.","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']",209,129
Unsupervised Monocular Depth Learning with Integrated Intrinsics and Spatio-Temporal Constraints,"Monocular depth inference has gained tremendous attention from researchers in
recent years and remains as a promising replacement for expensive
time-of-flight sensors, but issues with scale acquisition and implementation
overhead still plague these systems. To this end, this work presents an
unsupervised learning framework that is able to predict at-scale depth maps and
egomotion, in addition to camera intrinsics, from a sequence of monocular
images via a single network. Our method incorporates both spatial and temporal
geometric constraints to resolve depth and pose scale factors, which are
enforced within the supervisory reconstruction loss functions at training time.
Only unlabeled stereo sequences are required for training the weights of our
single-network architecture, which reduces overall implementation overhead as
compared to previous methods. Our results demonstrate strong performance when
compared to the current state-of-the-art on multiple sequences of the KITTI
driving dataset and can provide faster training times with its reduced network
complexity.","['cs.CV', 'cs.LG', 'cs.RO']",158,115
On Finding Gray Pixels,"We propose a novel grayness index for finding gray pixels and demonstrate its
effectiveness and efficiency in illumination estimation. The grayness index, GI
in short, is derived using the Dichromatic Reflection Model and is
learning-free. GI allows to estimate one or multiple illumination sources in
color-biased images. On standard single-illumination and multiple-illumination
estimation benchmarks, GI outperforms state-of-the-art statistical methods and
many recent deep methods. GI is simple and fast, written in a few dozen lines
of code, processing a 1080p image in ~0.4 seconds with a non-optimized Matlab
code.",['cs.CV'],98,70
Feature Selection for Learning to Predict Outcomes of Compute Cluster Jobs with Application to Decision Support,"We present a machine learning framework and a new test bed for data mining
from the Slurm Workload Manager for high-performance computing (HPC) clusters.
The focus was to find a method for selecting features to support decisions:
helping users decide whether to resubmit failed jobs with boosted CPU and
memory allocations or migrate them to a computing cloud. This task was cast as
both supervised classification and regression learning, specifically,
sequential problem solving suitable for reinforcement learning. Selecting
relevant features can improve training accuracy, reduce training time, and
produce a more comprehensible model, with an intelligent system that can
explain predictions and inferences. We present a supervised learning model
trained on a Simple Linux Utility for Resource Management (Slurm) data set of
HPC jobs using three different techniques for selecting features: linear
regression, lasso, and ridge regression. Our data set represented both HPC jobs
that failed and those that succeeded, so our model was reliable, less likely to
overfit, and generalizable. Our model achieved an R^2 of 95\% with 99\%
accuracy. We identified five predictors for both CPU and memory properties.","['cs.LG', 'cs.AI', 'I.2.6, I.2.11', 'I.2.6; I.2.11']",183,118
Self-Calibrating Neural Radiance Fields,"In this work, we propose a camera self-calibration algorithm for generic
cameras with arbitrary non-linear distortions. We jointly learn the geometry of
the scene and the accurate camera parameters without any calibration objects.
Our camera model consists of a pinhole model, a fourth order radial distortion,
and a generic noise model that can learn arbitrary non-linear camera
distortions. While traditional self-calibration algorithms mostly rely on
geometric constraints, we additionally incorporate photometric consistency.
This requires learning the geometry of the scene, and we use Neural Radiance
Fields (NeRF). We also propose a new geometric loss function, viz., projected
ray distance loss, to incorporate geometric consistency for complex non-linear
camera models. We validate our approach on standard real image datasets and
demonstrate that our model can learn the camera intrinsics and extrinsics
(pose) from scratch without COLMAP initialization. Also, we show that learning
accurate camera models in a differentiable manner allows us to improve PSNR
over baselines. Our module is an easy-to-use plugin that can be applied to NeRF
variants to improve performance. The code and data are currently available at
https://github.com/POSTECH-CVLab/SCNeRF.",['cs.CV'],192,121
Small Data Challenges in Big Data Era: A Survey of Recent Progress on Unsupervised and Semi-Supervised Methods,"Representation learning with small labeled data have emerged in many
problems, since the success of deep neural networks often relies on the
availability of a huge amount of labeled data that is expensive to collect. To
address it, many efforts have been made on training sophisticated models with
few labeled data in an unsupervised and semi-supervised fashion. In this paper,
we will review the recent progresses on these two major categories of methods.
A wide spectrum of models will be categorized in a big picture, where we will
show how they interplay with each other to motivate explorations of new ideas.
We will review the principles of learning the transformation equivariant,
disentangled, self-supervised and semi-supervised representations, all of which
underpin the foundation of recent progresses. Many implementations of
unsupervised and semi-supervised generative models have been developed on the
basis of these criteria, greatly expanding the territory of existing
autoencoders, generative adversarial nets (GANs) and other deep networks by
exploring the distribution of unlabeled data for more powerful representations.
We will discuss emerging topics by revealing the intrinsic connections between
unsupervised and semi-supervised learning, and propose in future directions to
bridge the algorithmic and theoretical gap between transformation equivariance
for unsupervised learning and supervised invariance for supervised learning,
and unify unsupervised pretraining and supervised finetuning. We will also
provide a broader outlook of future directions to unify transformation and
instance equivariances for representation learning, connect unsupervised and
semi-supervised augmentations, and explore the role of the self-supervised
regularization for many learning problems.",['cs.CV'],257,140
Improved Network Robustness with Adversary Critic,"Ideally, what confuses neural network should be confusing to humans. However,
recent experiments have shown that small, imperceptible perturbations can
change the network prediction. To address this gap in perception, we propose a
novel approach for learning robust classifier. Our main idea is: adversarial
examples for the robust classifier should be indistinguishable from the regular
data of the adversarial target. We formulate a problem of learning robust
classifier in the framework of Generative Adversarial Networks (GAN), where the
adversarial attack on classifier acts as a generator, and the critic network
learns to distinguish between regular and adversarial images. The classifier
cost is augmented with the objective that its adversarial examples should
confuse the adversary critic. To improve the stability of the adversarial
mapping, we introduce adversarial cycle-consistency constraint which ensures
that the adversarial mapping of the adversarial examples is close to the
original. In the experiments, we show the effectiveness of our defense. Our
method surpasses in terms of robustness networks trained with adversarial
training. Additionally, we verify in the experiments with human annotators on
MTurk that adversarial examples are indeed visually confusing. Codes for the
project are available at https://github.com/aam-at/adversary_critic.","['cs.LG', 'cs.CV', 'stat.ML']",197,118
Linformer: Self-Attention with Linear Complexity,"Large transformer models have shown extraordinary success in achieving
state-of-the-art results in many natural language processing applications.
However, training and deploying these models can be prohibitively costly for
long sequences, as the standard self-attention mechanism of the Transformer
uses $O(n^2)$ time and space with respect to sequence length. In this paper, we
demonstrate that the self-attention mechanism can be approximated by a low-rank
matrix. We further exploit this finding to propose a new self-attention
mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to
$O(n)$ in both time and space. The resulting linear transformer, the
\textit{Linformer}, performs on par with standard Transformer models, while
being much more memory- and time-efficient.","['cs.LG', 'stat.ML']",125,86
Free Energy and the Generalized Optimality Equations for Sequential Decision Making,"The free energy functional has recently been proposed as a variational
principle for bounded rational decision-making, since it instantiates a natural
trade-off between utility gains and information processing costs that can be
axiomatically derived. Here we apply the free energy principle to general
decision trees that include both adversarial and stochastic environments. We
derive generalized sequential optimality equations that not only include the
Bellman optimality equations as a limit case, but also lead to well-known
decision-rules such as Expectimax, Minimax and Expectiminimax. We show how
these decision-rules can be derived from a single free energy principle that
assigns a resource parameter to each node in the decision tree. These resource
parameters express a concrete computational cost that can be measured as the
amount of samples that are needed from the distribution that belongs to each
node. The free energy principle therefore provides the normative basis for
generalized optimality equations that account for both adversarial and
stochastic environments.","['stat.ML', 'cs.AI', 'cs.GT', 'cs.SY']",162,99
When and why PINNs fail to train: A neural tangent kernel perspective,"Physics-informed neural networks (PINNs) have lately received great attention
thanks to their flexibility in tackling a wide range of forward and inverse
problems involving partial differential equations. However, despite their
noticeable empirical success, little is known about how such constrained neural
networks behave during their training via gradient descent. More importantly,
even less is known about why such models sometimes fail to train at all. In
this work, we aim to investigate these questions through the lens of the Neural
Tangent Kernel (NTK); a kernel that captures the behavior of fully-connected
neural networks in the infinite width limit during training via gradient
descent. Specifically, we derive the NTK of PINNs and prove that, under
appropriate conditions, it converges to a deterministic kernel that stays
constant during training in the infinite-width limit. This allows us to analyze
the training dynamics of PINNs through the lens of their limiting NTK and find
a remarkable discrepancy in the convergence rate of the different loss
components contributing to the total training error. To address this
fundamental pathology, we propose a novel gradient descent algorithm that
utilizes the eigenvalues of the NTK to adaptively calibrate the convergence
rate of the total training error. Finally, we perform a series of numerical
experiments to verify the correctness of our theory and the practical
effectiveness of the proposed algorithms. The data and code accompanying this
manuscript are publicly available at
\url{https://github.com/PredictiveIntelligenceLab/PINNsNTK}.","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",241,147
Actor-critic versus direct policy search: a comparison based on sample complexity,"Sample efficiency is a critical property when optimizing policy parameters
for the controller of a robot. In this paper, we evaluate two state-of-the-art
policy optimization algorithms. One is a recent deep reinforcement learning
method based on an actor-critic algorithm, Deep Deterministic Policy Gradient
(DDPG), that has been shown to perform well on various control benchmarks. The
other one is a direct policy search method, Covariance Matrix Adaptation
Evolution Strategy (CMA-ES), a black-box optimization method that is widely
used for robot learning. The algorithms are evaluated on a continuous version
of the mountain car benchmark problem, so as to compare their sample
complexity. From a preliminary analysis, we expect DDPG to be more sample
efficient than CMA-ES, which is confirmed by our experimental results.",['cs.LG'],130,96
XVFI: eXtreme Video Frame Interpolation,"In this paper, we firstly present a dataset (X4K1000FPS) of 4K videos of 1000
fps with the extreme motion to the research community for video frame
interpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that
first handles the VFI for 4K videos with large motion. The XVFI-Net is based on
a recursive multi-scale shared structure that consists of two cascaded modules
for bidirectional optical flow learning between two input frames (BiOF-I) and
for bidirectional optical flow learning from target to input frames (BiOF-T).
The optical flows are stably approximated by a complementary flow reversal
(CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start
at any scale of input while the BiOF-T module only operates at the original
input scale so that the inference can be accelerated while maintaining highly
accurate VFI performance. Extensive experimental results show that our XVFI-Net
can successfully capture the essential information of objects with extremely
large motions and complex textures while the state-of-the-art methods exhibit
poor performance. Furthermore, our XVFI-Net framework also performs comparably
on the previous lower resolution benchmark dataset, which shows a robustness of
our algorithm as well. All source codes, pre-trained models, and proposed
X4K1000FPS datasets are publicly available at
https://github.com/JihyongOh/XVFI.",['cs.CV'],221,137
ECG-DelNet: Delineation of Ambulatory Electrocardiograms with Mixed Quality Labeling Using Neural Networks,"Electrocardiogram (ECG) detection and delineation are key steps for numerous
tasks in clinical practice, as ECG is the most performed non-invasive test for
assessing cardiac condition. State-of-the-art algorithms employ digital signal
processing (DSP), which require laborious rule adaptation to new morphologies.
In contrast, deep learning (DL) algorithms, especially for classification, are
gaining weight in academic and industrial settings. However, the lack of model
explainability and small databases hinder their applicability. We demonstrate
DL can be successfully applied to low interpretative tasks by embedding ECG
detection and delineation onto a segmentation framework. For this purpose, we
adapted and validated the most used neural network architecture for image
segmentation, the U-Net, to one-dimensional data. The model was trained using
PhysioNet's QT database, comprised of 105 ambulatory ECG recordings, for
single- and multi-lead scenarios. To alleviate data scarcity, data
regularization techniques such as pre-training with low-quality data labels,
performing ECG-based data augmentation and applying strong model regularizers
to the architecture were attempted. Other variations in the model's capacity
(U-Net's depth and width), alongside the application of state-of-the-art
additions, were evaluated. These variations were exhaustively validated in a
5-fold cross-validation manner. The best performing configuration reached
precisions of 90.12%, 99.14% and 98.25% and recalls of 98.73%, 99.94% and
99.88% for the P, QRS and T waves, respectively, on par with DSP-based
approaches. Despite being a data-hungry technique trained on a small dataset,
DL-based approaches demonstrate to be a viable alternative to traditional
DSP-based ECG processing techniques.","['cs.LG', 'eess.SP', 'stat.ML']",271,175
3D Dense Geometry-Guided Facial Expression Synthesis by Adversarial Learning,"Manipulating facial expressions is a challenging task due to fine-grained
shape changes produced by facial muscles and the lack of input-output pairs for
supervised learning. Unlike previous methods using Generative Adversarial
Networks (GAN), which rely on cycle-consistency loss or sparse geometry
(landmarks) loss for expression synthesis, we propose a novel GAN framework to
exploit 3D dense (depth and surface normals) information for expression
manipulation. However, a large-scale dataset containing RGB images with
expression annotations and their corresponding depth maps is not available. To
this end, we propose to use an off-the-shelf state-of-the-art 3D reconstruction
model to estimate the depth and create a large-scale RGB-Depth dataset after a
manual data clean-up process. We utilise this dataset to minimise the novel
depth consistency loss via adversarial learning (note we do not have ground
truth depth maps for generated face images) and the depth categorical loss of
synthetic data on the discriminator. In addition, to improve the generalisation
and lower the bias of the depth parameters, we propose to use a novel
confidence regulariser on the discriminator side of the framework. We
extensively performed both quantitative and qualitative evaluations on two
publicly available challenging facial expression benchmarks: AffectNet and
RaFD. Our experiments demonstrate that the proposed method outperforms the
competitive baseline and existing arts by a large margin.",['cs.CV'],227,140
OmniDRL: Robust Pedestrian Detection using Deep Reinforcement Learning on Omnidirectional Cameras,"Pedestrian detection is one of the most explored topics in computer vision
and robotics. The use of deep learning methods allowed the development of new
and highly competitive algorithms. Deep Reinforcement Learning has proved to be
within the state-of-the-art in terms of both detection in perspective cameras
and robotics applications. However, for detection in omnidirectional cameras,
the literature is still scarce, mostly because of their high levels of
distortion. This paper presents a novel and efficient technique for robust
pedestrian detection in omnidirectional images. The proposed method uses deep
Reinforcement Learning that takes advantage of the distortion in the image. By
considering the 3D bounding boxes and their distorted projections into the
image, our method is able to provide the pedestrian's position in the world, in
contrast to the image positions provided by most state-of-the-art methods for
perspective cameras. Our method avoids the need of pre-processing steps to
remove the distortion, which is computationally expensive. Beyond the novel
solution, our method compares favorably with the state-of-the-art methodologies
that do not consider the underlying distortion for the detection task.","['cs.CV', 'cs.RO']",189,108
Otimizacao de pesos e funcoes de ativacao de redes neurais aplicadas na previsao de series temporais,"Neural Networks have been applied for time series prediction with good
experimental results that indicate the high capacity to approximate functions
with good precision. Most neural models used in these applications use
activation functions with fixed parameters. However, it is known that the
choice of activation function strongly influences the complexity and
performance of the neural network and that a limited number of activation
functions have been used. In this work, we propose the use of a family of free
parameter asymmetric activation functions for neural networks and show that
this family of defined activation functions satisfies the requirements of the
universal approximation theorem. A methodology for the global optimization of
this family of activation functions with free parameter and the weights of the
connections between the processing units of the neural network is used. The
central idea of the proposed methodology is to simultaneously optimize the
weights and the activation function used in a multilayer perceptron network
(MLP), through an approach that combines the advantages of simulated annealing,
tabu search and a local learning algorithm, with the purpose of improving
performance in the adjustment and forecasting of time series. We chose two
learning algorithms: backpropagation with the term momentum (BPM) and
LevenbergMarquardt (LM).",['cs.LG'],204,110
Deep Appearance Maps,"We propose a deep representation of appearance, i. e., the relation of color,
surface orientation, viewer position, material and illumination. Previous
approaches have useddeep learning to extract classic appearance
representationsrelating to reflectance model parameters (e. g., Phong)
orillumination (e. g., HDR environment maps). We suggest todirectly represent
appearance itself as a network we call aDeep Appearance Map (DAM). This is a 4D
generalizationover 2D reflectance maps, which held the view direction fixed.
First, we show how a DAM can be learned from images or video frames and later
be used to synthesize appearance, given new surface orientations and viewer
positions. Second, we demonstrate how another network can be used to map from
an image or video frames to a DAM network to reproduce this appearance, without
using a lengthy optimization such as stochastic gradient descent
(learning-to-learn). Finally, we show the example of an appearance
estimation-and-segmentation task, mapping from an image showingmultiple
materials to multiple deep appearance maps.","['cs.CV', 'cs.GR']",161,103
Beyond Point Estimate: Inferring Ensemble Prediction Variation from Neuron Activation Strength in Recommender Systems,"Despite deep neural network (DNN)'s impressive prediction performance in
various domains, it is well known now that a set of DNN models trained with the
same model specification and the same data can produce very different
prediction results. Ensemble method is one state-of-the-art benchmark for
prediction uncertainty estimation. However, ensembles are expensive to train
and serve for web-scale traffic.
  In this paper, we seek to advance the understanding of prediction variation
estimated by the ensemble method. Through empirical experiments on two widely
used benchmark datasets MovieLens and Criteo in recommender systems, we observe
that prediction variations come from various randomness sources, including
training data shuffling, and parameter random initialization. By introducing
more randomness into model training, we notice that ensemble's mean predictions
tend to be more accurate while the prediction variations tend to be higher.
Moreover, we propose to infer prediction variation from neuron activation
strength and demonstrate the strong prediction power from activation strength
features. Our experiment results show that the average R squared on MovieLens
is as high as 0.56 and on Criteo is 0.81. Our method performs especially well
when detecting the lowest and highest variation buckets, with 0.92 AUC and 0.89
AUC respectively. Our approach provides a simple way for prediction variation
estimation, which opens up new opportunities for future work in many
interesting areas (e.g.,model-based reinforcement learning) without relying on
serving expensive ensemble models.","['cs.LG', 'stat.ML']",242,155
Neural Scene Graphs for Dynamic Scenes,"Recent implicit neural rendering methods have demonstrated that it is
possible to learn accurate view synthesis for complex scenes by predicting
their volumetric density and color supervised solely by a set of RGB images.
However, existing methods are restricted to learning efficient representations
of static scenes that encode all scene objects into a single neural network,
and lack the ability to represent dynamic scenes and decompositions into
individual scene objects. In this work, we present the first neural rendering
method that decomposes dynamic scenes into scene graphs. We propose a learned
scene graph representation, which encodes object transformation and radiance,
to efficiently render novel arrangements and views of the scene. To this end,
we learn implicitly encoded scenes, combined with a jointly learned latent
representation to describe objects with a single implicit function. We assess
the proposed method on synthetic and real automotive data, validating that our
approach learns dynamic scenes -- only by observing a video of this scene --
and allows for rendering novel photo-realistic views of novel scene
compositions with unseen sets of objects at unseen poses.","['cs.CV', 'cs.GR']",179,111
Detecting Malicious Accounts in Permissionless Blockchains using Temporal Graph Properties,"The temporal nature of modeling accounts as nodes and transactions as
directed edges in a directed graph -- for a blockchain, enables us to
understand the behavior (malicious or benign) of the accounts. Predictive
classification of accounts as malicious or benign could help users of the
permissionless blockchain platforms to operate in a secure manner. Motivated by
this, we introduce temporal features such as burst and attractiveness on top of
several already used graph properties such as the node degree and clustering
coefficient. Using identified features, we train various Machine Learning (ML)
algorithms and identify the algorithm that performs the best in detecting which
accounts are malicious. We then study the behavior of the accounts over
different temporal granularities of the dataset before assigning them malicious
tags. For Ethereum blockchain, we identify that for the entire dataset - the
ExtraTreesClassifier performs the best among supervised ML algorithms. On the
other hand, using cosine similarity on top of the results provided by
unsupervised ML algorithms such as K-Means on the entire dataset, we were able
to detect 554 more suspicious accounts. Further, using behavior change analysis
for accounts, we identify 814 unique suspicious accounts across different
temporal granularities.","['cs.LG', 'cs.SI', 'stat.ML']",196,111
FleXOR: Trainable Fractional Quantization,"Quantization based on the binary codes is gaining attention because each
quantized bit can be directly utilized for computations without dequantization
using look-up tables. Previous attempts, however, only allow for integer
numbers of quantization bits, which ends up restricting the search space for
compression ratio and accuracy. In this paper, we propose an encryption
algorithm/architecture to compress quantized weights so as to achieve
fractional numbers of bits per weight. Decryption during inference is
implemented by digital XOR-gate networks added into the neural network model
while XOR gates are described by utilizing $\tanh(x)$ for backward propagation
to enable gradient calculations. We perform experiments using MNIST, CIFAR-10,
and ImageNet to show that inserting XOR gates learns quantization/encrypted bit
decisions through training and obtains high accuracy even for fractional sub
1-bit weights. As a result, our proposed method yields smaller size and higher
model accuracy compared to binary neural networks.","['cs.LG', 'stat.ML']",154,119
Junction Tree Variational Autoencoder for Molecular Graph Generation,"We seek to automate the design of molecules based on specific chemical
properties. In computational terms, this task involves continuous embedding and
generation of molecular graphs. Our primary contribution is the direct
realization of molecular graphs, a task previously approached by generating
linear SMILES strings instead of graphs. Our junction tree variational
autoencoder generates molecular graphs in two phases, by first generating a
tree-structured scaffold over chemical substructures, and then combining them
into a molecule with a graph message passing network. This approach allows us
to incrementally expand molecules while maintaining chemical validity at every
step. We evaluate our model on multiple tasks ranging from molecular generation
to optimization. Across these tasks, our model outperforms previous
state-of-the-art baselines by a significant margin.","['cs.LG', 'cs.NE', 'stat.ML']",126,92
K-Net: Towards Unified Image Segmentation,"Semantic, instance, and panoptic segmentations have been addressed using
different and specialized frameworks despite their underlying connections. This
paper presents a unified, simple, and effective framework for these essentially
similar tasks. The framework, named K-Net, segments both instances and semantic
categories consistently by a group of learnable kernels, where each kernel is
responsible for generating a mask for either a potential instance or a stuff
class. To remedy the difficulties of distinguishing various instances, we
propose a kernel update strategy that enables each kernel dynamic and
conditional on its meaningful group in the input image. K-Net can be trained in
an end-to-end manner with bipartite matching, and its training and inference
are naturally NMS-free and box-free. Without bells and whistles, K-Net
surpasses all previous state-of-the-art single-model results of panoptic
segmentation on MS COCO and semantic segmentation on ADE20K with 52.1% PQ and
54.3% mIoU, respectively. Its instance segmentation performance is also on par
with Cascade Mask R-CNNon MS COCO with 60%-90% faster inference speeds. Code
and models will be released at https://github.com/open-mmlab/mmdetection.","['cs.CV', 'cs.AI']",192,138
Multimodal Classification of Urban Micro-Events,"In this paper we seek methods to effectively detect urban micro-events. Urban
micro-events are events which occur in cities, have limited geographical
coverage and typically affect only a small group of citizens. Because of their
scale these are difficult to identify in most data sources. However, by using
citizen sensing to gather data, detecting them becomes feasible. The data
gathered by citizen sensing is often multimodal and, as a consequence, the
information required to detect urban micro-events is distributed over multiple
modalities. This makes it essential to have a classifier capable of combining
them. In this paper we explore several methods of creating such a classifier,
including early, late, hybrid fusion and representation learning using
multimodal graphs. We evaluate performance on a real world dataset obtained
from a live citizen reporting system. We show that a multimodal approach yields
higher performance than unimodal alternatives. Furthermore, we demonstrate that
our hybrid combination of early and late fusion with multimodal embeddings
performs best in classification of urban micro-events.","['cs.LG', 'stat.ML']",170,112
Embodied Question Answering,"We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where
an agent is spawned at a random location in a 3D environment and asked a
question (""What color is the car?""). In order to answer, the agent must first
intelligently navigate to explore the environment, gather information through
first-person (egocentric) vision, and then answer the question (""orange"").
  This challenging task requires a range of AI skills -- active perception,
language understanding, goal-driven navigation, commonsense reasoning, and
grounding of language into actions. In this work, we develop the environments,
end-to-end-trained reinforcement learning agents, and evaluation protocols for
EmbodiedQA.","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']",102,76
Exploration by Random Network Distillation,"We introduce an exploration bonus for deep reinforcement learning methods
that is easy to implement and adds minimal overhead to the computation
performed. The bonus is the error of a neural network predicting features of
the observations given by a fixed randomly initialized neural network. We also
introduce a method to flexibly combine intrinsic and extrinsic rewards. We find
that the random network distillation (RND) bonus combined with this increased
flexibility enables significant progress on several hard exploration Atari
games. In particular we establish state of the art performance on Montezuma's
Revenge, a game famously difficult for deep reinforcement learning methods. To
the best of our knowledge, this is the first method that achieves better than
average human performance on this game without using demonstrations or having
access to the underlying state of the game, and occasionally completes the
first level.","['cs.LG', 'cs.AI', 'stat.ML']",142,93
Distributed Deep Reinforcement Learning: An Overview,"Deep reinforcement learning (DRL) is a very active research area. However,
several technical and scientific issues require to be addressed, amongst which
we can mention data inefficiency, exploration-exploitation trade-off, and
multi-task learning. Therefore, distributed modifications of DRL were
introduced; agents that could be run on many machines simultaneously. In this
article, we provide a survey of the role of the distributed approaches in DRL.
We overview the state of the field, by studying the key research works that
have a significant impact on how we can use distributed methods in DRL. We
choose to overview these papers, from the perspective of distributed learning,
and not the aspect of innovations in reinforcement learning algorithms. Also,
we evaluate these methods on different tasks and compare their performance with
each other and with single actor and learner agents.","['cs.LG', 'cs.DC']",138,92
State-Regularized Recurrent Neural Networks,"Recurrent neural networks are a widely used class of neural architectures.
They have, however, two shortcomings. First, it is difficult to understand what
exactly they learn. Second, they tend to work poorly on sequences requiring
long-term memorization, despite having this capacity in principle. We aim to
address both shortcomings with a class of recurrent networks that use a
stochastic state transition mechanism between cell applications. This
mechanism, which we term state-regularization, makes RNNs transition between a
finite set of learnable states. We evaluate state-regularized RNNs on (1)
regular languages for the purpose of automata extraction; (2) nonregular
languages such as balanced parentheses, palindromes, and the copy task where
external memory is required; and (3) real-word sequence learning tasks for
sentiment analysis, visual object recognition, and language modeling. We show
that state-regularization (a) simplifies the extraction of finite state
automata modeling an RNN's state transition dynamics; (b) forces RNNs to
operate more like automata with external memory and less like finite state
machines; (c) makes RNNs have better interpretability and explainability.","['cs.LG', 'stat.ML']",176,120
Blackbox Attacks on Reinforcement Learning Agents Using Approximated Temporal Information,"Recent research on reinforcement learning (RL) has suggested that trained
agents are vulnerable to maliciously crafted adversarial samples. In this work,
we show how such samples can be generalised from White-box and Grey-box attacks
to a strong Black-box case, where the attacker has no knowledge of the agents,
their training parameters and their training methods. We use
sequence-to-sequence models to predict a single action or a sequence of future
actions that a trained agent will make. First, we show our approximation model,
based on time-series information from the agent, consistently predicts RL
agents' future actions with high accuracy in a Black-box setup on a wide range
of games and RL algorithms. Second, we find that although adversarial samples
are transferable from the target model to our RL agents, they often outperform
random Gaussian noise only marginally. This highlights a serious methodological
deficiency in previous work on such agents; random jamming should have been
taken as the baseline for evaluation. Third, we propose a novel use for
adversarial samplesin Black-box attacks of RL agents: they can be used to
trigger a trained agent to misbehave after a specific time delay. This appears
to be a genuinely new type of attack. It potentially enables an attacker to use
devices controlled by RL agents as time bombs.","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",222,131
Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs,"Transformer neural networks have achieved state-of-the-art results for
unstructured data such as text and images but their adoption for
graph-structured data has been limited. This is partly due to the difficulty of
incorporating complex structural information in the basic transformer
framework. We propose a simple yet powerful extension to the transformer -
residual edge channels. The resultant framework, which we call Edge-augmented
Graph Transformer (EGT), can directly accept, process and output structural
information as well as node information. It allows us to use global
self-attention, the key element of transformers, directly for graphs and comes
with the benefit of long-range interaction among nodes. Moreover, the edge
channels allow the structural information to evolve from layer to layer, and
prediction tasks on edges/links can be performed directly from the output
embeddings of these channels. In addition, we introduce a generalized
positional encoding scheme for graphs based on Singular Value Decomposition
which can improve the performance of EGT. Our framework, which relies on global
node feature aggregation, achieves better performance compared to
Convolutional/Message-Passing Graph Neural Networks, which rely on local
feature aggregation within a neighborhood. We verify the performance of EGT in
a supervised learning setting on a wide range of experiments on benchmark
datasets. Our findings indicate that convolutional aggregation is not an
essential inductive bias for graphs and global self-attention can serve as a
flexible and adaptive alternative.",['cs.LG'],238,149
Self-supervised Auxiliary Learning for Graph Neural Networks via Meta-Learning,"In recent years, graph neural networks (GNNs) have been widely adopted in the
representation learning of graph-structured data and provided state-of-the-art
performance in various applications such as link prediction, node
classification, and recommendation. Motivated by recent advances of
self-supervision for representation learning in natural language processing and
computer vision, self-supervised learning has been recently studied to leverage
unlabeled graph-structured data. However, employing self-supervision tasks as
auxiliary tasks to assist a primary task has been less explored in the
literature on graphs. In this paper, we propose a novel self-supervised
auxiliary learning framework to effectively learn graph neural networks.
Moreover, this work is the first study showing that a meta-path prediction is
beneficial as a self-supervised auxiliary task for heterogeneous graphs. Our
method is learning to learn a primary task with various auxiliary tasks to
improve generalization performance. The proposed method identifies an effective
combination of auxiliary tasks and automatically balances them to improve the
primary task. Our methods can be applied to any graph neural network in a
plug-in manner without manual labeling or additional data. Also, it can be
extended to any other auxiliary tasks. Our experiments demonstrate that the
proposed method consistently improves the performance of node classification
and link prediction.",['cs.LG'],215,117
Disaggregation of SMAP L3 Brightness Temperatures to 9km using Kernel Machines,"In this study, a machine learning algorithm is used for disaggregation of
SMAP brightness temperatures (T$_{\textrm{B}}$) from 36km to 9km. It uses image
segmentation to cluster the study region based on meteorological and land cover
similarity, followed by a support vector machine based regression that computes
the value of the disaggregated T$_{\textrm{B}}$ at all pixels. High resolution
remote sensing products such as land surface temperature, normalized difference
vegetation index, enhanced vegetation index, precipitation, soil texture, and
land-cover were used for disaggregation. The algorithm was implemented in Iowa,
United States, from April to July 2015, and compared with the SMAP L3_SM_AP
T$_{\textrm{B}}$ product at 9km. It was found that the disaggregated
T$_{\textrm{B}}$ were very similar to the SMAP-T$_{\textrm{B}}$ product, even
for vegetated areas with a mean difference $\leq$ 5K. However, the standard
deviation of the disaggregation was lower by 7K than that of the AP product.
The probability density functions of the disaggregated T$_{\textrm{B}}$ were
similar to the SMAP-T$_{\textrm{B}}$. The results indicate that this algorithm
may be used for disaggregating T$_{\textrm{B}}$ using complex non-linear
correlations on a grid.",['cs.CV'],205,111
Ensemble of Convolutional Neural Networks for Dermoscopic Images Classification,"In this report, we are presenting our automated prediction system for disease
classification within dermoscopic images. The proposed solution is based on
deep learning, where we employed transfer learning strategy on VGG16 and
GoogLeNet architectures. The key feature of our solution is preprocessing based
primarily on image augmentation and colour normalization. The solution was
evaluated on Task 3: Lesion Diagnosis of the ISIC 2018: Skin Lesion Analysis
Towards Melanoma Detection.",['cs.CV'],70,55
Relational Graph Neural Networks for Fraud Detection in a Super-App environment,"Large digital platforms create environments where different types of user
interactions are captured, these relationships offer a novel source of
information for fraud detection problems. In this paper we propose a framework
of relational graph convolutional networks methods for fraudulent behaviour
prevention in the financial services of a Super-App. To this end, we apply the
framework on different heterogeneous graphs of users, devices, and credit
cards; and finally use an interpretability algorithm for graph neural networks
to determine the most important relations to the classification task of the
users. Our results show that there is an added value when considering models
that take advantage of the alternative data of the Super-App and the
interactions found in their high connectivity, further proofing how they can
leverage that into better decisions and fraud detection strategies.","['cs.LG', 'q-fin.GN']",135,97
Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks,"Attention mechanisms, especially self-attention, have played an increasingly
important role in deep feature representation for visual tasks. Self-attention
updates the feature at each position by computing a weighted sum of features
using pair-wise affinities across all positions to capture the long-range
dependency within a single sample. However, self-attention has quadratic
complexity and ignores potential correlation between different samples. This
paper proposes a novel attention mechanism which we call external attention,
based on two external, small, learnable, shared memories, which can be
implemented easily by simply using two cascaded linear layers and two
normalization layers; it conveniently replaces self-attention in existing
popular architectures. External attention has linear complexity and implicitly
considers the correlations between all data samples. We further incorporate the
multi-head mechanism into external attention to provide an all-MLP
architecture, external attention MLP (EAMLP), for image classification.
Extensive experiments on image classification, object detection, semantic
segmentation, instance segmentation, image generation, and point cloud analysis
reveal that our method provides results comparable or superior to the
self-attention mechanism and some of its variants, with much lower
computational and memory costs.",['cs.CV'],188,134
Informative Dropout for Robust Representation Learning: A Shape-bias Perspective,"Convolutional Neural Networks (CNNs) are known to rely more on local texture
rather than global shape when making decisions. Recent work also indicates a
close relationship between CNN's texture-bias and its robustness against
distribution shift, adversarial perturbation, random corruption, etc. In this
work, we attempt at improving various kinds of robustness universally by
alleviating CNN's texture bias. With inspiration from the human visual system,
we propose a light-weight model-agnostic method, namely Informative Dropout
(InfoDrop), to improve interpretability and reduce texture bias. Specifically,
we discriminate texture from shape based on local self-information in an image,
and adopt a Dropout-like algorithm to decorrelate the model output from the
local texture. Through extensive experiments, we observe enhanced robustness
under various scenarios (domain generalization, few-shot classification, image
corruption, and adversarial perturbation). To the best of our knowledge, this
work is one of the earliest attempts to improve different kinds of robustness
in a unified model, shedding new light on the relationship between shape-bias
and robustness, also on new approaches to trustworthy machine learning
algorithms. Code is available at https://github.com/bfshi/InfoDrop.","['cs.LG', 'cs.CV', 'stat.ML']",188,121
Image Captioning with Semantic Attention,"Automatically generating a natural language description of an image has
attracted interests recently both because of its importance in practical
applications and because it connects two major artificial intelligence fields:
computer vision and natural language processing. Existing approaches are either
top-down, which start from a gist of an image and convert it into words, or
bottom-up, which come up with words describing various aspects of an image and
then combine them. In this paper, we propose a new algorithm that combines both
approaches through a model of semantic attention. Our algorithm learns to
selectively attend to semantic concept proposals and fuse them into hidden
states and outputs of recurrent neural networks. The selection and fusion form
a feedback connecting the top-down and bottom-up computation. We evaluate our
algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental
results show that our algorithm significantly outperforms the state-of-the-art
approaches consistently across different evaluation metrics.",['cs.CV'],159,111
Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport,"Successful quantitative investment usually relies on precise predictions of
the future movement of the stock price. Recently, machine learning based
solutions have shown their capacity to give more accurate stock prediction and
become indispensable components in modern quantitative investment systems.
However, the i.i.d. assumption behind existing methods is inconsistent with the
existence of diverse trading patterns in the stock market, which inevitably
limits their ability to achieve better stock prediction performance. In this
paper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to
empower existing stock prediction models with the ability to model multiple
stock trading patterns. Essentially, TRA is a lightweight module that consists
of a set of independent predictors for learning multiple patterns as well as a
router to dispatch samples to different predictors. Nevertheless, the lack of
explicit pattern identifiers makes it quite challenging to train an effective
TRA-based model. To tackle this challenge, we further design a learning
algorithm based on Optimal Transport (OT) to obtain the optimal sample to
predictor assignment and effectively optimize the router with such assignment
through an auxiliary loss term. Experiments on the real-world stock ranking
task show that compared to the state-of-the-art baselines, e.g., Attention LSTM
and Transformer, the proposed method can improve information coefficient (IC)
from 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used
in this work are publicly available:
https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TRA.","['cs.LG', 'cs.CE', 'q-fin.ST']",249,167
EgoMap: Projective mapping and structured egocentric memory for Deep RL,"Tasks involving localization, memorization and planning in partially
observable 3D environments are an ongoing challenge in Deep Reinforcement
Learning. We present EgoMap, a spatially structured neural memory architecture.
EgoMap augments a deep reinforcement learning agent's performance in 3D
environments on challenging tasks with multi-step objectives. The EgoMap
architecture incorporates several inductive biases including a differentiable
inverse projection of CNN feature vectors onto a top-down spatially structured
map. The map is updated with ego-motion measurements through a differentiable
affine transform. We show this architecture outperforms both standard recurrent
agents and state of the art agents with structured memory. We demonstrate that
incorporating these inductive biases into an agent's architecture allows for
stable training with reward alone, circumventing the expense of acquiring and
labelling expert trajectories. A detailed ablation study demonstrates the
impact of key aspects of the architecture and through extensive qualitative
analysis, we show how the agent exploits its structured internal memory to
achieve higher performance.","['cs.LG', 'cs.AI']",161,112
A Bayesian and Machine Learning approach to estimating Influence Model parameters for IM-RO,"The rise of Online Social Networks (OSNs) has caused an insurmountable amount
of interest from advertisers and researchers seeking to monopolize on its
features. Researchers aim to develop strategies for determining how information
is propagated among users within an OSN that is captured by diffusion or
influence models. We consider the influence models for the IM-RO problem, a
novel formulation to the Influence Maximization (IM) problem based on
implementing Stochastic Dynamic Programming (SDP). In contrast to existing
approaches involving influence spread and the theory of submodular functions,
the SDP method focuses on optimizing clicks and ultimately revenue to
advertisers in OSNs. Existing approaches to influence maximization have been
actively researched over the past decade, with applications to multiple fields,
however, our approach is a more practical variant to the original IM problem.
In this paper, we provide an analysis on the influence models of the IM-RO
problem by conducting experiments on synthetic and real-world datasets. We
propose a Bayesian and Machine Learning approach for estimating the parameters
of the influence models for the (Influence Maximization- Revenue Optimization)
IM-RO problem. We present a Bayesian hierarchical model and implement the
well-known Naive Bayes classifier (NBC), Decision Trees classifier (DTC) and
Random Forest classifier (RFC) on three real-world datasets. Compared to
previous approaches to estimating influence model parameters, our strategy has
the great advantage of being directly implementable in standard software
packages such as WinBUGS/OpenBUGS/JAGS and Apache Spark. We demonstrate the
efficiency and usability of our methods in terms of spreading information and
generating revenue for advertisers in the context of OSNs.","['stat.ML', 'cs.LG']",267,159
Identifying Unknown Instances for Autonomous Driving,"In the past few years, we have seen great progress in perception algorithms,
particular through the use of deep learning. However, most existing approaches
focus on a few categories of interest, which represent only a small fraction of
the potential categories that robots need to handle in the real-world. Thus,
identifying objects from unknown classes remains a challenging yet crucial
task. In this paper, we develop a novel open-set instance segmentation
algorithm for point clouds which can segment objects from both known and
unknown classes in a holistic way. Our method uses a deep convolutional neural
network to project points into a category-agnostic embedding space in which
they can be clustered into instances irrespective of their semantics.
Experiments on two large-scale self-driving datasets validate the effectiveness
of our proposed method.","['cs.CV', 'cs.LG', 'cs.RO']",135,102
ImageGCN: Multi-Relational Image Graph Convolutional Networks for Disease Identification with Chest X-rays,"Image representation is a fundamental task in computer vision. However, most
of the existing approaches for image representation ignore the relations
between images and consider each input image independently. Intuitively,
relations between images can help to understand the images and maintain model
consistency over related images. In this paper, we consider modeling the
image-level relations to generate more informative image representations, and
propose ImageGCN, an end-to-end graph convolutional network framework for
multi-relational image modeling. We also apply ImageGCN to chest X-ray (CXR)
images where rich relational information is available for disease
identification. Unlike previous image representation models, ImageGCN learns
the representation of an image using both its original pixel features and the
features of related images. Besides learning informative representations for
images, ImageGCN can also be used for object detection in a weakly supervised
manner. The Experimental results on ChestX-ray14 dataset demonstrate that
ImageGCN can outperform respective baselines in both disease identification and
localization tasks and can achieve comparable and often better results than the
state-of-the-art methods.","['cs.CV', 'cs.AI']",176,110
Predicting Sequences of Traversed Nodes in Graphs using Network Models with Multiple Higher Orders,"We propose a novel sequence prediction method for sequential data capturing
node traversals in graphs. Our method builds on a statistical modelling
framework that combines multiple higher-order network models into a single
multi-order model. We develop a technique to fit such multi-order models in
empirical sequential data and to select the optimal maximum order. Our
framework facilitates both next-element and full sequence prediction given a
sequence-prefix of any length. We evaluate our model based on six empirical
data sets containing sequences from website navigation as well as public
transport systems. The results show that our method out-performs
state-of-the-art algorithms for next-element prediction. We further demonstrate
the accuracy of our method during out-of-sample sequence prediction and
validate that our method can scale to data sets with millions of sequences.","['cs.LG', 'cs.IT', 'cs.SI', 'math.IT', 'physics.data-an', 'stat.ML']",140,86
Knowledge Generation -- Variational Bayes on Knowledge Graphs,"This thesis is a proof of concept for the potential of Variational
Auto-Encoder (VAE) on representation learning of real-world Knowledge Graphs
(KG). Inspired by successful approaches to the generation of molecular graphs,
we evaluate the capabilities of our model, the Relational Graph Variational
Auto-Encoder (RGVAE). The impact of the modular hyperparameter choices,
encoding through graph convolutions, graph matching and latent space prior, is
compared. The RGVAE is first evaluated on link prediction. The mean reciprocal
rank (MRR) scores on the two datasets FB15K-237 and WN18RR are compared to the
embedding-based model DistMult. A variational DistMult and a RGVAE without
latent space prior constraint are implemented as control models. The results
show that between different settings, the RGVAE with relaxed latent space,
scores highest on both datasets, yet does not outperform the DistMult. Further,
we investigate the latent space in a twofold experiment: first, linear
interpolation between the latent representation of two triples, then the
exploration of each latent dimension in a $95\%$ confidence interval. Both
interpolations show that the RGVAE learns to reconstruct the adjacency matrix
but fails to disentangle. For the last experiment we introduce a new validation
method for the FB15K-237 data set. The relation type-constrains of generated
triples are filtered and matched with entity types. The observed rate of valid
generated triples is insignificantly higher than the random threshold. All
generated and valid triples are unseen. A comparison between different latent
space priors, using the $\delta$-VAE method, reveals a decoder collapse.
Finally we analyze the limiting factors of our approach compared to molecule
generation and propose solutions for the decoder collapse and successful
representation learning of multi-relational KGs.","['cs.LG', 'cs.AI']",280,161
Unsupervised Learning for Large-Scale Fiber Detection and Tracking in Microscopic Material Images,"Constructing 3D structures from serial section data is a long standing
problem in microscopy. The structure of a fiber reinforced composite material
can be reconstructed using a tracking-by-detection model. Tracking-by-detection
algorithms rely heavily on detection accuracy, especially the recall
performance. The state-of-the-art fiber detection algorithms perform well under
ideal conditions, but are not accurate where there are local degradations of
image quality, due to contaminants on the material surface and/or defocus blur.
Convolutional Neural Networks (CNN) could be used for this problem, but would
require a large number of manual annotated fibers, which are not available. We
propose an unsupervised learning method to accurately detect fibers on the
large scale, that is robust against local degradations of image quality. The
proposed method does not require manual annotations, but uses fiber shape/size
priors and spatio-temporal consistency in tracking to simulate the supervision
in the training of the CNN. Experiments show significant improvements over
state-of-the-art fiber detection algorithms together with advanced tracking
performance.",['cs.CV'],174,114
LightSAL: Lightweight Sign Agnostic Learning for Implicit Surface Representation,"Recently, several works have addressed modeling of 3D shapes using deep
neural networks to learn implicit surface representations. Up to now, the
majority of works have concentrated on reconstruction quality, paying little or
no attention to model size or training time. This work proposes LightSAL, a
novel deep convolutional architecture for learning 3D shapes; the proposed work
concentrates on efficiency both in network training time and resulting model
size. We build on the recent concept of Sign Agnostic Learning for training the
proposed network, relying on signed distance fields, with unsigned distance as
ground truth. In the experimental section of the paper, we demonstrate that the
proposed architecture outperforms previous work in model size and number of
required training iterations, while achieving equivalent accuracy. Experiments
are based on the D-Faust dataset that contains 41k 3D scans of human shapes.
The proposed model has been implemented in PyTorch.",['cs.CV'],148,100
Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network,"Transformer-based architectures have shown great success in image captioning,
where object regions are encoded and then attended into the vectorial
representations to guide the caption decoding. However, such vectorial
representations only contain region-level information without considering the
global information reflecting the entire image, which fails to expand the
capability of complex multi-modal reasoning in image captioning. In this paper,
we introduce a Global Enhanced Transformer (termed GET) to enable the
extraction of a more comprehensive global representation, and then adaptively
guide the decoder to generate high-quality captions. In GET, a Global Enhanced
Encoder is designed for the embedding of the global feature, and a Global
Adaptive Decoder are designed for the guidance of the caption generation. The
former models intra- and inter-layer global representation by taking advantage
of the proposed Global Enhanced Attention and a layer-wise fusion module. The
latter contains a Global Adaptive Controller that can adaptively fuse the
global information into the decoder to guide the caption generation. Extensive
experiments on MS COCO dataset demonstrate the superiority of our GET over many
state-of-the-arts.",['cs.CV'],184,111
A sparse annotation strategy based on attention-guided active learning for 3D medical image segmentation,"3D image segmentation is one of the most important and ubiquitous problems in
medical image processing. It provides detailed quantitative analysis for
accurate disease diagnosis, abnormal detection, and classification. Currently
deep learning algorithms are widely used in medical image segmentation, most
algorithms trained models with full annotated datasets. However, obtaining
medical image datasets is very difficult and expensive, and full annotation of
3D medical image is a monotonous and time-consuming work. Partially labelling
informative slices in 3D images will be a great relief of manual annotation.
Sample selection strategies based on active learning have been proposed in the
field of 2D image, but few strategies focus on 3D images. In this paper, we
propose a sparse annotation strategy based on attention-guided active learning
for 3D medical image segmentation. Attention mechanism is used to improve
segmentation accuracy and estimate the segmentation accuracy of each slice. The
comparative experiments with three different strategies using datasets from the
developing human connectome project (dHCP) show that, our strategy only needs
15% to 20% annotated slices in brain extraction task and 30% to 35% annotated
slices in tissue segmentation task to achieve comparative results as full
annotation.","['cs.CV', 'eess.IV']",194,120
Sparse Transfer Learning via Winning Lottery Tickets,"The recently proposed Lottery Ticket Hypothesis of Frankle and Carbin (2019)
suggests that the performance of over-parameterized deep networks is due to the
random initialization seeding the network with a small fraction of favorable
weights. These weights retain their dominant status throughout training -- in a
very real sense, this sub-network ""won the lottery"" during initialization. The
authors find sub-networks via unstructured magnitude pruning with 85-95% of
parameters removed that train to the same accuracy as the original network at a
similar speed, which they call winning tickets. In this paper, we extend the
Lottery Ticket Hypothesis to a variety of transfer learning tasks. We show that
sparse sub-networks with approximately 90-95% of weights removed achieve (and
often exceed) the accuracy of the original dense network in several realistic
settings. We experimentally validate this by transferring the sparse
representation found via pruning on CIFAR-10 to SmallNORB and FashionMNIST for
object recognition tasks.","['cs.LG', 'cs.NE', 'stat.ML']",158,105
LRCN-RetailNet: A recurrent neural network architecture for accurate people counting,"Measuring and analyzing the flow of customers in retail stores is essential
for a retailer to better comprehend customers' behavior and support
decision-making. Nevertheless, not much attention has been given to the
development of novel technologies for automatic people counting. We introduce
LRCN-RetailNet: a recurrent neural network architecture capable of learning a
non-linear regression model and accurately predicting the people count from
videos captured by low-cost surveillance cameras. The input video format
follows the recently proposed RGBP image format, which is comprised of color
and people (foreground) information. Our architecture is capable of considering
two relevant aspects: spatial features extracted through convolutional layers
from the RGBP images; and the temporal coherence of the problem, which is
exploited by recurrent layers. We show that, through a supervised learning
approach, the trained models are capable of predicting the people count with
high accuracy. Additionally, we present and demonstrate that a straightforward
modification of the methodology is effective to exclude salespeople from the
people count. Comprehensive experiments were conducted to validate, evaluate
and compare the proposed architecture. Results corroborated that LRCN-RetailNet
remarkably outperforms both the previous RetailNet architecture, which was
limited to evaluating a single image per iteration; and a state-of-the-art
neural network for object detection. Finally, computational performance
experiments confirmed that the entire methodology is effective to estimate
people count in real-time.",['cs.CV'],229,141
An Introduction to Robust Graph Convolutional Networks,"Graph convolutional neural networks (GCNs) generalize tradition convolutional
neural networks (CNNs) from low-dimensional regular graphs (e.g., image) to
high dimensional irregular graphs (e.g., text documents on word embeddings).
Due to inevitable faulty data collection instruments, deceptive data
manipulation, or other system errors, the data might be error-contaminated.
Even a small amount of error such as noise can compromise the ability of GCNs
and render them inadmissible to a large extent. The key challenge is how to
effectively and efficiently employ GCNs in the presence of erroneous data. In
this paper, we propose a novel Robust Graph Convolutional Neural Networks for
possible erroneous single-view or multi-view data where data may come from
multiple sources. By incorporating an extra layers via Autoencoders into
traditional graph convolutional networks, we characterize and handle typical
error models explicitly. Experimental results on various real-world datasets
demonstrate the superiority of the proposed model over the baseline methods and
its robustness against different types of error.",['cs.LG'],165,122
Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging,"A key challenge in training neural networks for a given medical imaging task
is often the difficulty of obtaining a sufficient number of manually labeled
examples. In contrast, textual imaging reports, which are often readily
available in medical records, contain rich but unstructured interpretations
written by experts as part of standard clinical practice. We propose using
these textual reports as a form of weak supervision to improve the image
interpretation performance of a neural network without requiring additional
manually labeled examples. We use an image-text matching task to train a
feature extractor and then fine-tune it in a transfer learning setting for a
supervised task using a small labeled dataset. The end result is a neural
network that automatically interprets imagery without requiring textual reports
during inference. This approach can be applied to any task for which text-image
pairs are readily available. We evaluate our method on three classification
tasks and find consistent performance improvements, reducing the need for
labeled data by 67%-98%.","['cs.LG', 'cs.CL', 'cs.CV', 'eess.IV']",167,111
A Robust Local Binary Similarity Pattern for Foreground Object Detection,"Accurate and fast extraction of the foreground object is one of the most
significant issues to be solved due to its important meaning for object
tracking and recognition in video surveillance. Although many foreground object
detection methods have been proposed in the recent past, it is still regarded
as a tough problem due to illumination variations and dynamic backgrounds
challenges. In this paper, we propose a robust foreground object detection
method with two aspects of contributions. First, we propose a robust texture
operator named Robust Local Binary Similarity Pattern (RLBSP), which shows
strong robustness to illumination variations and dynamic backgrounds. Second, a
combination of color and texture features are used to characterize pixel
representations, which compensate each other to make full use of their own
advantages. Comprehensive experiments evaluated on the CDnet 2012 dataset
demonstrate that the proposed method performs favorably against
state-of-the-art methods.",['cs.CV'],147,104
Class-wise Dynamic Graph Convolution for Semantic Segmentation,"Recent works have made great progress in semantic segmentation by exploiting
contextual information in a local or global manner with dilated convolutions,
pyramid pooling or self-attention mechanism. In order to avoid potential
misleading contextual information aggregation in previous works, we propose a
class-wise dynamic graph convolution (CDGC) module to adaptively propagate
information. The graph reasoning is performed among pixels in the same class.
Based on the proposed CDGC module, we further introduce the Class-wise Dynamic
Graph Convolution Network(CDGCNet), which consists of two main parts including
the CDGC module and a basic segmentation network, forming a coarse-to-fine
paradigm. Specifically, the CDGC module takes the coarse segmentation result as
class mask to extract node features for graph construction and performs dynamic
graph convolutions on the constructed graph to learn the feature aggregation
and weight allocation. Then the refined feature and the original feature are
fused to get the final prediction. We conduct extensive experiments on three
popular semantic segmentation benchmarks including Cityscapes, PASCAL VOC 2012
and COCO Stuff, and achieve state-of-the-art performance on all three
benchmarks.",['cs.CV'],183,120
Dimensionality reduction to maximize prediction generalization capability,"This work develops an analytically solvable unsupervised learning scheme that
extracts the most informative components for predicting future inputs, termed
predictive principal component analysis (PredPCA). Our scheme can effectively
remove unpredictable observation noise and globally minimize the test
prediction error. Mathematical analyses demonstrate that, with sufficiently
high-dimensional observations that are generated by a linear or nonlinear
system, PredPCA can identify the optimal hidden state representation, true
system parameters, and true hidden state dimensionality, with a global
convergence guarantee. We demonstrate the performance of PredPCA by using
sequential visual inputs comprising hand-digits, rotating 3D objects, and
natural scenes. It reliably and accurately estimates distinct hidden states and
predicts future outcomes of previously unseen test input data, even in the
presence of considerable observation noise. The simple model structure and low
computational cost of PredPCA make it highly desirable as a learning scheme for
biological neural networks and neuromorphic chips.","['stat.ML', 'cs.CV', 'cs.LG']",150,112
Deep Reinforcement Learning: An Overview,"In recent years, a specific machine learning method called deep learning has
gained huge attraction, as it has obtained astonishing results in broad
applications such as pattern recognition, speech recognition, computer vision,
and natural language processing. Recent research has also been shown that deep
learning techniques can be combined with reinforcement learning methods to
learn useful representations for the problems with high dimensional raw data
input. This chapter reviews the recent advances in deep reinforcement learning
with a focus on the most used deep architectures such as autoencoders,
convolutional neural networks and recurrent neural networks which have
successfully been come together with the reinforcement learning framework.","['cs.LG', 'cs.AI', 'stat.ML']",106,77
DeGraF-Flow: Extending DeGraF Features for accurate and efficient sparse-to-dense optical flow estimation,"Modern optical flow methods make use of salient scene feature points detected
and matched within the scene as a basis for sparse-to-dense optical flow
estimation. Current feature detectors however either give sparse, non uniform
point clouds (resulting in flow inaccuracies) or lack the efficiency for
frame-rate real-time applications. In this work we use the novel Dense Gradient
Based Features (DeGraF) as the input to a sparse-to-dense optical flow scheme.
This consists of three stages: 1) efficient detection of uniformly distributed
Dense Gradient Based Features (DeGraF); 2) feature tracking via robust local
optical flow; and 3) edge preserving flow interpolation to recover overall
dense optical flow. The tunable density and uniformity of DeGraF features yield
superior dense optical flow estimation compared to other popular feature
detectors within this three stage pipeline. Furthermore, the comparable speed
of feature detection also lends itself well to the aim of real-time optical
flow recovery. Evaluation on established real-world benchmark datasets show
test performance in an autonomous vehicle setting where DeGraF-Flow shows
promising results in terms of accuracy with competitive computational
efficiency among non-GPU based methods, including a marked increase in speed
over the conceptually similar EpicFlow approach.","['cs.CV', 'cs.AI']",202,131
PT-ResNet: Perspective Transformation-Based Residual Network for Semantic Road Image Segmentation,"Semantic road region segmentation is a high-level task, which paves the way
towards road scene understanding. This paper presents a residual network
trained for semantic road segmentation. Firstly, we represent the projections
of road disparities in the v-disparity map as a linear model, which can be
estimated by optimizing the v-disparity map using dynamic programming. This
linear model is then utilized to reduce the redundant information in the left
and right road images. The right image is also transformed into the left
perspective view, which greatly enhances the road surface similarity between
the two images. Finally, the processed stereo images and their disparity maps
are concatenated to create a set of 3D images, which are then utilized to train
our neural network. The experimental results illustrate that our network
achieves a maximum F1-measure of approximately 91.19% when analyzing the images
from the KITTI road dataset.","['cs.CV', 'cs.LG', 'cs.RO', 'eess.IV']",150,97
"GAN ""Steerability"" without optimization","Recent research has shown remarkable success in revealing ""steering""
directions in the latent spaces of pre-trained GANs. These directions
correspond to semantically meaningful image transformations e.g., shift, zoom,
color manipulations), and have similar interpretable effects across all
categories that the GAN can generate. Some methods focus on user-specified
transformations, while others discover transformations in an unsupervised
manner. However, all existing techniques rely on an optimization procedure to
expose those directions, and offer no control over the degree of allowed
interaction between different transformations. In this paper, we show that
""steering"" trajectories can be computed in closed form directly from the
generator's weights without any form of training or optimization. This applies
to user-prescribed geometric transformations, as well as to unsupervised
discovery of more complex effects. Our approach allows determining both linear
and nonlinear trajectories, and has many advantages over previous methods. In
particular, we can control whether one transformation is allowed to come on the
expense of another (e.g. zoom-in with or without allowing translation to keep
the object centered). Moreover, we can determine the natural end-point of the
trajectory, which corresponds to the largest extent to which a transformation
can be applied without incurring degradation. Finally, we show how transferring
attributes between images can be achieved without optimization, even across
different categories.",['cs.CV'],221,144
Recurrent Rational Networks,"Latest insights from biology show that intelligence does not only emerge from
the connections between the neurons, but that individual neurons shoulder more
computational responsibility. Current Neural Network architecture design and
search are biased on fixed activation functions. Using more advanced learnable
activation functions provide Neural Networks with higher learning capacity.
However, general guidance for building such networks is still missing. In this
work, we first explain why rationals offer an optimal choice for activation
functions. We then show that they are closed under residual connections, and
inspired by recurrence for residual networks we derive a self-regularized
version of Rationals: Recurrent Rationals. We demonstrate that (Recurrent)
Rational Networks lead to high performance improvements on Image Classification
and Deep Reinforcement Learning.",['cs.LG'],121,94
Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection,"This paper focuses on Semi-Supervised Object Detection (SSOD). Knowledge
Distillation (KD) has been widely used for semi-supervised image
classification. However, adapting these methods for SSOD has the following
obstacles. (1) The teacher model serves a dual role as a teacher and a student,
such that the teacher predictions on unlabeled images may be very close to
those of student, which limits the upper-bound of the student. (2) The class
imbalance issue in SSOD hinders an efficient knowledge transfer from teacher to
student. To address these problems, we propose a novel method Temporal
Self-Ensembling Teacher (TSE-T) for SSOD. Differently from previous KD based
methods, we devise a temporally evolved teacher model. First, our teacher model
ensembles its temporal predictions for unlabeled images under stochastic
perturbations. Second, our teacher model ensembles its temporal model weights
with the student model weights by an exponential moving average (EMA) which
allows the teacher gradually learn from the student. These self-ensembling
strategies increase data and model diversity, thus improving teacher
predictions on unlabeled images. Finally, we use focal loss to formulate
consistency regularization term to handle the data imbalance problem, which is
a more efficient manner to utilize the useful information from unlabeled images
than a simple hard-thresholding method which solely preserves confident
predictions. Evaluated on the widely used VOC and COCO benchmarks, the mAP of
our method has achieved 80.73% and 40.52% on the VOC2007 test set and the
COCO2014 minval5k set respectively, which outperforms a strong fully-supervised
detector by 2.37% and 1.49%. Furthermore, our method sets the new
state-of-the-art in SSOD on VOC2007 test set which outperforms the baseline
SSOD method by 1.44%. The source code of this work is publicly available at
http://github.com/syangdong/tse-t.",['cs.CV'],301,178
Generic Outlier Detection in Multi-Armed Bandit,"In this paper, we study the problem of outlier arm detection in multi-armed
bandit settings, which finds plenty of applications in many high-impact domains
such as finance, healthcare, and online advertising. For this problem, a
learner aims to identify the arms whose expected rewards deviate significantly
from most of the other arms. Different from existing work, we target the
generic outlier arms or outlier arm groups whose expected rewards can be
larger, smaller, or even in between those of normal arms. To this end, we start
by providing a comprehensive definition of such generic outlier arms and
outlier arm groups. Then we propose a novel pulling algorithm named GOLD to
identify such generic outlier arms. It builds a real-time neighborhood graph
based on upper confidence bounds and catches the behavior pattern of outliers
from normal arms. We also analyze its performance from various aspects. In the
experiments conducted on both synthetic and real-world data sets, the proposed
algorithm achieves 98 % accuracy while saving 83 % exploration cost on average
compared with state-of-the-art techniques.","['cs.LG', 'cs.AI', 'stat.ML']",179,119
Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning,"The disparate experimental conditions in recent off-policy policy evaluation
(OPE) literature make it difficult both for practitioners to choose a reliable
estimator for their application domain, as well as for researchers to identify
fruitful research directions. In this work, we present the first detailed
empirical study of a broad suite of OPE methods. Based on thousands of
experiments and empirical analysis, we offer a summarized set of guidelines to
advance the understanding of OPE performance in practice, and suggest
directions for future research. Along the way, our empirical findings challenge
several commonly held beliefs about which class of approaches tends to perform
well. Our accompanying software implementation serves as a first comprehensive
benchmark for OPE.","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",116,84
"Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos","The task of video grounding, which temporally localizes a natural language
description in a video, plays an important role in understanding videos.
Existing studies have adopted strategies of sliding window over the entire
video or exhaustively ranking all possible clip-sentence pairs in a
pre-segmented video, which inevitably suffer from exhaustively enumerated
candidates. To alleviate this problem, we formulate this task as a problem of
sequential decision making by learning an agent which regulates the temporal
grounding boundaries progressively based on its policy. Specifically, we
propose a reinforcement learning based framework improved by multi-task
learning and it shows steady performance gains by considering additional
supervised boundary information during training. Our proposed framework
achieves state-of-the-art performance on ActivityNet'18 DenseCaption dataset
and Charades-STA dataset while observing only 10 or less clips per video.",['cs.CV'],138,102
Know What Your Neighbors Do: 3D Semantic Segmentation of Point Clouds,"In this paper, we present a deep learning architecture which addresses the
problem of 3D semantic segmentation of unstructured point clouds. Compared to
previous work, we introduce grouping techniques which define point
neighborhoods in the initial world space and the learned feature space.
Neighborhoods are important as they allow to compute local or global point
features depending on the spatial extend of the neighborhood. Additionally, we
incorporate dedicated loss functions to further structure the learned point
feature space: the pairwise distance loss and the centroid loss. We show how to
apply these mechanisms to the task of 3D semantic segmentation of point clouds
and report state-of-the-art performance on indoor and outdoor datasets.",['cs.CV'],115,76
Generalized Leverage Score Sampling for Neural Networks,"Leverage score sampling is a powerful technique that originates from
theoretical computer science, which can be used to speed up a large number of
fundamental questions, e.g. linear regression, linear programming,
semi-definite programming, cutting plane method, graph sparsification, maximum
matching and max-flow. Recently, it has been shown that leverage score sampling
helps to accelerate kernel methods [Avron, Kapralov, Musco, Musco, Velingker
and Zandieh 17].
  In this work, we generalize the results in [Avron, Kapralov, Musco, Musco,
Velingker and Zandieh 17] to a broader class of kernels. We further bring the
leverage score sampling into the field of deep learning theory.
  $\bullet$ We show the connection between the initialization for neural
network training and approximating the neural tangent kernel with random
features.
  $\bullet$ We prove the equivalence between regularized neural network and
neural tangent kernel ridge regression under the initialization of both
classical random Gaussian and leverage score sampling.","['cs.LG', 'stat.ML']",151,99
"LATTE: Accelerating LiDAR Point Cloud Annotation via Sensor Fusion, One-Click Annotation, and Tracking","LiDAR (Light Detection And Ranging) is an essential and widely adopted sensor
for autonomous vehicles, particularly for those vehicles operating at higher
levels (L4-L5) of autonomy. Recent work has demonstrated the promise of
deep-learning approaches for LiDAR-based detection. However, deep-learning
algorithms are extremely data hungry, requiring large amounts of labeled
point-cloud data for training and evaluation. Annotating LiDAR point cloud data
is challenging due to the following issues: 1) A LiDAR point cloud is usually
sparse and has low resolution, making it difficult for human annotators to
recognize objects. 2) Compared to annotation on 2D images, the operation of
drawing 3D bounding boxes or even point-wise labels on LiDAR point clouds is
more complex and time-consuming. 3) LiDAR data are usually collected in
sequences, so consecutive frames are highly correlated, leading to repeated
annotations. To tackle these challenges, we propose LATTE, an open-sourced
annotation tool for LiDAR point clouds. LATTE features the following
innovations: 1) Sensor fusion: We utilize image-based detection algorithms to
automatically pre-label a calibrated image, and transfer the labels to the
point cloud. 2) One-click annotation: Instead of drawing 3D bounding boxes or
point-wise labels, we simplify the annotation to just one click on the target
object, and automatically generate the bounding box for the target. 3)
Tracking: we integrate tracking into sequence annotation such that we can
transfer labels from one frame to subsequent ones and therefore significantly
reduce repeated labeling. Experiments show the proposed features accelerate the
annotation speed by 6.2x and significantly improve label quality with 23.6% and
2.2% higher instance-level precision and recall, and 2.0% higher bounding box
IoU. LATTE is open-sourced at https://github.com/bernwang/latte.",['cs.CV'],293,173
Asymmetric CNN for image super-resolution,"Deep convolutional neural networks (CNNs) have been widely applied for
low-level vision over the past five years. According to nature of different
applications, designing appropriate CNN architectures is developed. However,
customized architectures gather different features via treating all pixel
points as equal to improve the performance of given application, which ignores
the effects of local power pixel points and results in low training efficiency.
In this paper, we propose an asymmetric CNN (ACNet) comprising an asymmetric
block (AB), a memory enhancement block (MEB) and a high-frequency feature
enhancement block (HFFEB) for image super-resolution. The AB utilizes
one-dimensional asymmetric convolutions to intensify the square convolution
kernels in horizontal and vertical directions for promoting the influences of
local salient features for SISR. The MEB fuses all hierarchical low-frequency
features from the AB via residual learning (RL) technique to resolve the
long-term dependency problem and transforms obtained low-frequency features
into high-frequency features. The HFFEB exploits low- and high-frequency
features to obtain more robust super-resolution features and address excessive
feature enhancement problem. Addditionally, it also takes charge of
reconstructing a high-resolution (HR) image. Extensive experiments show that
our ACNet can effectively address single image super-resolution (SISR), blind
SISR and blind SISR of blind noise problems. The code of the ACNet is shown at
https://github.com/hellloxiaotian/ACNet.",['cs.CV'],226,139
Representing Closed Transformation Paths in Encoded Network Latent Space,"Deep generative networks have been widely used for learning mappings from a
low-dimensional latent space to a high-dimensional data space. In many cases,
data transformations are defined by linear paths in this latent space. However,
the Euclidean structure of the latent space may be a poor match for the
underlying latent structure in the data. In this work, we incorporate a
generative manifold model into the latent space of an autoencoder in order to
learn the low-dimensional manifold structure from the data and adapt the latent
space to accommodate this structure. In particular, we focus on applications in
which the data has closed transformation paths which extend from a starting
point and return to nearly the same point. Through experiments on data with
natural closed transformation paths, we show that this model introduces the
ability to learn the latent dynamics of complex systems, generate
transformation paths, and classify samples that belong on the same
transformation path.","['stat.ML', 'cs.LG']",159,83
Graph Node Embeddings using Domain-Aware Biased Random Walks,"The recent proliferation of publicly available graph-structured data has
sparked an interest in machine learning algorithms for graph data. Since most
traditional machine learning algorithms assume data to be tabular, embedding
algorithms for mapping graph data to real-valued vector spaces has become an
active area of research. Existing graph embedding approaches are based purely
on structural information and ignore any semantic information from the
underlying domain. In this paper, we demonstrate that semantic information can
play a useful role in computing graph embeddings. Specifically, we present a
framework for devising embedding strategies aware of domain-specific
interpretations of graph nodes and edges, and use knowledge of downstream
machine learning tasks to identify relevant graph substructures. Using two
real-life domains, we show that our framework yields embeddings that are simple
to implement and yet achieve equal or greater accuracy in machine learning
tasks compared to domain independent approaches.","['cs.LG', 'cs.SI', 'stat.ML']",150,98
Equalization Loss for Large Vocabulary Instance Segmentation,"Recent object detection and instance segmentation tasks mainly focus on
datasets with a relatively small set of categories, e.g. Pascal VOC with 20
classes and COCO with 80 classes. The new large vocabulary dataset LVIS brings
new challenges to conventional methods. In this work, we propose an
equalization loss to solve the long tail of rare categories problem. Combined
with exploiting the data from detection datasets to alleviate the effect of
missing-annotation problems during the training, our method achieves 5.1\%
overall AP gain and 11.4\% AP gain of rare categories on LVIS benchmark without
any bells and whistles compared to Mask R-CNN baseline. Finally we achieve 28.9
mask AP on the test-set of the LVIS and rank 1st place in LVIS Challenge 2019.",['cs.CV'],130,94
Better Understanding Hierarchical Visual Relationship for Image Caption,"The Convolutional Neural Network (CNN) has been the dominant image feature
extractor in computer vision for years. However, it fails to get the
relationship between images/objects and their hierarchical interactions which
can be helpful for representing and describing an image. In this paper, we
propose a new design for image caption under a general encoder-decoder
framework. It takes into account the hierarchical interactions between
different abstraction levels of visual information in the images and their
bounding-boxes. Specifically, we present CNN plus Graph Convolutional Network
(GCN) architecture that novelly integrates both semantic and spatial visual
relationships into image encoder. The representations of regions in an image
and the connections between images are refined by leveraging graph structure
through GCN. With the learned multi-level features, our model capitalizes on
the Transformer-based decoder for description generation. We conduct
experiments on the COCO image captioning dataset. Evaluations show that our
proposed model outperforms the previous state-of-the-art models in the task of
image caption, leading to a better performance in terms of all evaluation
metrics.",['cs.CV'],178,121
Thermal Infrared Colorization via Conditional Generative Adversarial Network,"Transforming a thermal infrared image into a realistic RGB image is a
challenging task. In this paper we propose a deep learning method to bridge
this gap. We propose learning the transformation mapping using a coarse-to-fine
generator that preserves the details. Since the standard mean squared loss
cannot penalize the distance between colorized and ground truth images well, we
propose a composite loss function that combines content, adversarial,
perceptual and total variation losses. The content loss is used to recover
global image information while the latter three losses are used to synthesize
local realistic textures. Quantitative and qualitative experiments demonstrate
that our approach significantly outperforms existing approaches.",['cs.CV'],109,79
Follow the Attention: Combining Partial Pose and Object Motion for Fine-Grained Action Detection,"Retailers have long been searching for ways to effectively understand their
customers' behaviour in order to provide a smooth and pleasant shopping
experience that attracts more customers everyday and maximises their revenue,
consequently. Humans can flawlessly understand others' behaviour by combining
different visual cues from activity to gestures and facial expressions.
Empowering the computer vision systems to do so, however, is still an open
problem due to its intrinsic challenges as well as extrinsic enforced
difficulties like lack of publicly available data and unique environment
conditions (wild). In this work, We emphasise on detecting the first and by far
the most crucial cue in behaviour analysis; that is human activity detection in
computer vision. To do so, we introduce a framework for integrating human pose
and object motion to both temporally detect and classify the activities in a
fine-grained manner (very short and similar activities). We incorporate partial
human pose and interaction with the objects in a multi-stream neural network
architecture to guide the spatiotemporal attention mechanism for more efficient
activity recognition. To this end, in the absence of pose supervision, we
propose to use the Generative Adversarial Network (GAN) to generate exact joint
locations from noisy probability heat maps. Additionally, based on the
intuition that complex actions demand more than one source of information to be
identified even by humans, we integrate the second stream of object motion to
our network as a prior knowledge that we quantitatively show improves the
recognition results. We empirically show the capability of our approach by
achieving state-of-the-art results on MERL shopping dataset. We further
investigate the effectiveness of this approach on a new shopping dataset that
we have collected to address existing shortcomings.","['cs.CV', 'cs.LG']",286,181
Passive attention in artificial neural networks predicts human visual selectivity,"Developments in machine learning interpretability techniques over the past
decade have provided new tools to observe the image regions that are most
informative for classification and localization in artificial neural networks
(ANNs). Are the same regions similarly informative to human observers? Using
data from 78 new experiments and 6,610 participants, we show that passive
attention techniques reveal a significant overlap with human visual selectivity
estimates derived from 6 distinct behavioral tasks including visual
discrimination, spatial localization, recognizability, free-viewing,
cued-object search, and saliency search fixations. We find that input
visualizations derived from relatively simple ANN architectures probed using
guided backpropagation methods are the best predictors of a shared component in
the joint variability of the human measures. We validate these correlational
results with causal manipulations using recognition experiments. We show that
images masked with ANN attention maps were easier for humans to classify than
control masks in a speeded recognition experiment. Similarly, we find that
recognition performance in the same ANN models was likewise influenced by
masking input images using human visual selectivity maps. This work contributes
a new approach to evaluating the biological and psychological validity of
leading ANNs as models of human vision: by examining their similarities and
differences in terms of their visual selectivity to the information contained
in images.",['cs.CV'],215,136
Using massive health insurance claims data to predict very high-cost claimants: a machine learning approach,"Due to escalating healthcare costs, accurately predicting which patients will
incur high costs is an important task for payers and providers of healthcare.
High-cost claimants (HiCCs) are patients who have annual costs above
$\$250,000$ and who represent just 0.16% of the insured population but
currently account for 9% of all healthcare costs. In this study, we aimed to
develop a high-performance algorithm to predict HiCCs to inform a novel care
management system. Using health insurance claims from 48 million people and
augmented with census data, we applied machine learning to train binary
classification models to calculate the personal risk of HiCC. To train the
models, we developed a platform starting with 6,006 variables across all
clinical and demographic dimensions and constructed over one hundred candidate
models. The best model achieved an area under the receiver operating
characteristic curve of 91.2%. The model exceeds the highest published
performance (84%) and remains high for patients with no prior history of
high-cost status (89%), who have less than a full year of enrollment (87%), or
lack pharmacy claims data (88%). It attains an area under the precision-recall
curve of 23.1%, and precision of 74% at a threshold of 0.99. A care management
program enrolling 500 people with the highest HiCC risk is expected to treat
199 true HiCCs and generate a net savings of $\$7.3$ million per year. Our
results demonstrate that high-performing predictive models can be constructed
using claims data and publicly available data alone, even for rare high-cost
claimants exceeding $\$250,000$. Our model demonstrates the transformational
power of machine learning and artificial intelligence in care management, which
would allow healthcare payers and providers to introduce the next generation of
care management programs.","['cs.LG', 'stat.ML', 'J.3, I.2.6', 'J.3; I.2.6']",295,181
CBNetV2: A Composite Backbone Network Architecture for Object Detection,"Modern top-performing object detectors depend heavily on backbone networks,
whose advances bring consistent performance gains through exploring more
effective network structures. In this paper, we propose a novel and flexible
backbone framework, namely CBNetV2, to construct high-performance detectors
using existing open-sourced pre-trained backbones under the pre-training
fine-tuning paradigm. In particular, CBNetV2 architecture groups multiple
identical backbones, which are connected through composite connections.
Specifically, it integrates the high- and low-level features of multiple
backbone networks and gradually expands the receptive field to more efficiently
perform object detection. We also propose a better training strategy with
assistant supervision for CBNet-based detectors. Without additional
pre-training of the composite backbone, CBNetV2 can be adapted to various
backbones (CNN-based vs. Transformer-based) and head designs of most mainstream
detectors (one-stage vs. two-stage, anchor-based vs. anchor-free-based).
Experiments provide strong evidence that, compared with simply increasing the
depth and width of the network, CBNetV2 introduces a more efficient, effective,
and resource-friendly way to build high-performance backbone networks.
Particularly, our Dual-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO
test-dev under the single-model and single-scale testing protocol, which is
significantly better than the state-of-the-art result (57.7% box AP and 50.2%
mask AP) achieved by Swin-L, while the training schedule is reduced by
6$\times$. With multi-scale testing, we push the current best single model
result to a new record of 60.1% box AP and 52.3% mask AP without using extra
training data. Code is available at https://github.com/VDIGPKU/CBNetV2.",['cs.CV'],278,176
Gradient Estimators for Implicit Models,"Implicit models, which allow for the generation of samples but not for
point-wise evaluation of probabilities, are omnipresent in real-world problems
tackled by machine learning and a hot topic of current research. Some examples
include data simulators that are widely used in engineering and scientific
research, generative adversarial networks (GANs) for image synthesis, and
hot-off-the-press approximate inference techniques relying on implicit
distributions. The majority of existing approaches to learning implicit models
rely on approximating the intractable distribution or optimisation objective
for gradient-based optimisation, which is liable to produce inaccurate updates
and thus poor models. This paper alleviates the need for such approximations by
proposing the Stein gradient estimator, which directly estimates the score
function of the implicitly defined distribution. The efficacy of the proposed
estimator is empirically demonstrated by examples that include meta-learning
for approximate inference, and entropy regularised GANs that provide improved
sample diversity.","['stat.ML', 'cs.LG']",152,103
3D Object Proposals using Stereo Imagery for Accurate Object Class Detection,"The goal of this paper is to perform 3D object detection in the context of
autonomous driving. Our method first aims at generating a set of high-quality
3D object proposals by exploiting stereo imagery. We formulate the problem as
minimizing an energy function that encodes object size priors, placement of
objects on the ground plane as well as several depth informed features that
reason about free space, point cloud densities and distance to the ground. We
then exploit a CNN on top of these proposals to perform object detection. In
particular, we employ a convolutional neural net (CNN) that exploits context
and depth information to jointly regress to 3D bounding box coordinates and
object pose. Our experiments show significant performance gains over existing
RGB and RGB-D object proposal methods on the challenging KITTI benchmark. When
combined with the CNN, our approach outperforms all existing results in object
detection and orientation estimation tasks for all three KITTI object classes.
Furthermore, we experiment also with the setting where LIDAR information is
available, and show that using both LIDAR and stereo leads to the best result.",['cs.CV'],185,122
Cross-modal registration using point clouds and graph-matching in the context of correlative microscopies,"Correlative microscopy aims at combining two or more modalities to gain more
information than the one provided by one modality on the same biological
structure. Registration is needed at different steps of correlative
microscopies workflows. Biologists want to select the image content used for
registration not to introduce bias in the correlation of unknown structures.
Intensity-based methods might not allow this selection and might be too slow
when the images are very large. We propose an approach based on point clouds
created from selected content by the biologist. These point clouds may be prone
to big differences in densities but also missing parts and outliers. In this
paper we present a method of registration for point clouds based on graph
building and graph matching, and compare the method to iterative closest point
based methods.","['cs.CV', 'q-bio.TO', '05C60', 'I.4; J.3']",135,95
Relatable Clothing: Detecting Visual Relationships between People and Clothing,"Detecting visual relationships between people and clothing in an image has
been a relatively unexplored problem in the field of computer vision and
biometrics. The lack readily available public dataset for ``worn'' and
``unworn'' classification has slowed the development of solutions for this
problem. We present the release of the Relatable Clothing Dataset which
contains 35287 person-clothing pairs and segmentation masks for the development
of ``worn'' and ``unworn'' classification models. Additionally, we propose a
novel soft attention unit for performing ``worn'' and ``unworn'' classification
using deep neural networks. The proposed soft attention models have an accuracy
of upward $98.55\% \pm 0.35\%$ on the Relatable Clothing Dataset and
demonstrate high generalizable, allowing us to classify unseen articles of
clothing such as high visibility vests as ``worn'' or ``unworn''.",['cs.CV'],130,86
RADIATE: A Radar Dataset for Automotive Perception in Bad Weather,"Datasets for autonomous cars are essential for the development and
benchmarking of perception systems. However, most existing datasets are
captured with camera and LiDAR sensors in good weather conditions. In this
paper, we present the RAdar Dataset In Adverse weaThEr (RADIATE), aiming to
facilitate research on object detection, tracking and scene understanding using
radar sensing for safe autonomous driving. RADIATE includes 3 hours of
annotated radar images with more than 200K labelled road actors in total, on
average about 4.6 instances per radar image. It covers 8 different categories
of actors in a variety of weather conditions (e.g., sun, night, rain, fog and
snow) and driving scenarios (e.g., parked, urban, motorway and suburban),
representing different levels of challenge. To the best of our knowledge, this
is the first public radar dataset which provides high-resolution radar images
on public roads with a large amount of road actors labelled. The data collected
in adverse weather, e.g., fog and snowfall, is unique. Some baseline results of
radar based object detection and recognition are given to show that the use of
radar data is promising for automotive applications in bad weather, where
vision and LiDAR can fail. RADIATE also has stereo images, 32-channel LiDAR and
GPS data, directed at other applications such as sensor fusion, localisation
and mapping. The public dataset can be accessed at
http://pro.hw.ac.uk/radiate/.","['cs.CV', 'cs.RO']",233,153
SharpGAN: Receptive Field Block Net for Dynamic Scene Deblurring,"When sailing at sea, the smart ship will inevitably produce swaying motion
due to the action of wind, wave and current, which makes the image collected by
the visual sensor appear motion blur. This will have an adverse effect on the
object detection algorithm based on the vision sensor, thereby affect the
navigation safety of the smart ship. In order to remove the motion blur in the
images during the navigation of the smart ship, we propose SharpGAN, a new
image deblurring method based on the generative adversarial network. First of
all, the Receptive Field Block Net (RFBNet) is introduced to the deblurring
network to strengthen the network's ability to extract the features of blurred
image. Secondly, we propose a feature loss that combines different levels of
image features to guide the network to perform higher-quality deblurring and
improve the feature similarity between the restored images and the sharp image.
Finally, we propose to use the lightweight RFB-s module to improve the
real-time performance of deblurring network. Compared with the existing
deblurring methods on large-scale real sea image datasets and large-scale
deblurring datasets, the proposed method not only has better deblurring
performance in visual perception and quantitative criteria, but also has higher
deblurring efficiency.","['cs.CV', 'cs.LG', 'eess.IV', 'I.2.10']",210,117
Automatic breast cancer grading in lymph nodes using a deep neural network,"The progression of breast cancer can be quantified in lymph node whole-slide
images (WSIs). We describe a novel method for effectively performing
classification of whole-slide images and patient level breast cancer grading.
Our method utilises a deep neural network. The method performs classification
on small patches and uses model averaging for boosting. In the first step,
region of interest patches are determined and cropped automatically by color
thresholding and then classified by the deep neural network. The classification
results are used to determine a slide level class and for further aggregation
to predict a patient level grade. Fast processing speed of our method enables
high throughput image analysis.","['cs.CV', 'cs.LG']",110,74
TediGAN: Text-Guided Diverse Face Image Generation and Manipulation,"In this work, we propose TediGAN, a novel framework for multi-modal image
generation and manipulation with textual descriptions. The proposed method
consists of three components: StyleGAN inversion module, visual-linguistic
similarity learning, and instance-level optimization. The inversion module maps
real images to the latent space of a well-trained StyleGAN. The
visual-linguistic similarity learns the text-image matching by mapping the
image and text into a common embedding space. The instance-level optimization
is for identity preservation in manipulation. Our model can produce diverse and
high-quality images with an unprecedented resolution at 1024. Using a control
mechanism based on style-mixing, our TediGAN inherently supports image
synthesis with multi-modal inputs, such as sketches or semantic labels, with or
without instance guidance. To facilitate text-guided multi-modal synthesis, we
propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real
face images and corresponding semantic segmentation map, sketch, and textual
descriptions. Extensive experiments on the introduced dataset demonstrate the
superior performance of our proposed method. Code and data are available at
https://github.com/weihaox/TediGAN.","['cs.CV', 'cs.AI', 'cs.MM']",183,119
TDprop: Does Jacobi Preconditioning Help Temporal Difference Learning?,"We investigate whether Jacobi preconditioning, accounting for the bootstrap
term in temporal difference (TD) learning, can help boost performance of
adaptive optimizers. Our method, TDprop, computes a per parameter learning rate
based on the diagonal preconditioning of the TD update rule. We show how this
can be used in both $n$-step returns and TD($\lambda$). Our theoretical
findings demonstrate that including this additional preconditioning information
is, surprisingly, comparable to normal semi-gradient TD if the optimal learning
rate is found for both via a hyperparameter search. In Deep RL experiments
using Expected SARSA, TDprop meets or exceeds the performance of Adam in all
tested games under near-optimal learning rates, but a well-tuned SGD can yield
similar improvements -- matching our theory. Our findings suggest that Jacobi
preconditioning may improve upon typical adaptive optimization methods in Deep
RL, but despite incorporating additional information from the TD bootstrap
term, may not always be better than SGD.","['cs.LG', 'stat.ML']",156,107
Stereo Waterdrop Removal with Row-wise Dilated Attention,"Existing vision systems for autonomous driving or robots are sensitive to
waterdrops adhered to windows or camera lenses. Most recent waterdrop removal
approaches take a single image as input and often fail to recover the missing
content behind waterdrops faithfully. Thus, we propose a learning-based model
for waterdrop removal with stereo images. To better detect and remove
waterdrops from stereo images, we propose a novel row-wise dilated attention
module to enlarge attention's receptive field for effective information
propagation between the two stereo images. In addition, we propose an attention
consistency loss between the ground-truth disparity map and attention scores to
enhance the left-right consistency in stereo images. Because of related
datasets' unavailability, we collect a real-world dataset that contains stereo
images with and without waterdrops. Extensive experiments on our dataset
suggest that our model outperforms state-of-the-art methods both quantitatively
and qualitatively. Our source code and the stereo waterdrop dataset are
available at
\href{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}","['cs.CV', 'cs.LG', 'cs.RO']",176,116
Syn2Real: A New Benchmark forSynthetic-to-Real Visual Domain Adaptation,"Unsupervised transfer of object recognition models from synthetic to real
data is an important problem with many potential applications. The challenge is
how to ""adapt"" a model trained on simulated images so that it performs well on
real-world data without any additional supervision. Unfortunately, current
benchmarks for this problem are limited in size and task diversity. In this
paper, we present a new large-scale benchmark called Syn2Real, which consists
of a synthetic domain rendered from 3D object models and two real-image domains
containing the same object categories. We define three related tasks on this
benchmark: closed-set object classification, open-set object classification,
and object detection. Our evaluation of multiple state-of-the-art methods
reveals a large gap in adaptation performance between the easier closed-set
classification task and the more difficult open-set and detection tasks. We
conclude that developing adaptation methods that work well across all three
tasks presents a significant future challenge for syn2real domain transfer.",['cs.CV'],163,106
An Optimization Framework for Semi-Supervised and Transfer Learning using Multiple Classifiers and Clusterers,"Unsupervised models can provide supplementary soft constraints to help
classify new, ""target"" data since similar instances in the target set are more
likely to share the same class label. Such models can also help detect possible
differences between training and target distributions, which is useful in
applications where concept drift may take place, as in transfer learning
settings. This paper describes a general optimization framework that takes as
input class membership estimates from existing classifiers learnt on previously
encountered ""source"" data, as well as a similarity matrix from a cluster
ensemble operating solely on the target data to be classified, and yields a
consensus labeling of the target data. This framework admits a wide range of
loss functions and classification/clustering methods. It exploits properties of
Bregman divergences in conjunction with Legendre duality to yield a principled
and scalable approach. A variety of experiments show that the proposed
framework can yield results substantially superior to those provided by popular
transductive learning techniques or by naively applying classifiers learnt on
the original task to the target data.","['cs.LG', 'I.5.2; I.5.3; I.5.4']",176,122
Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling,"Recent advances in deep reinforcement learning have made significant strides
in performance on applications such as Go and Atari games. However, developing
practical methods to balance exploration and exploitation in complex domains
remains largely unsolved. Thompson Sampling and its extension to reinforcement
learning provide an elegant approach to exploration that only requires access
to posterior samples of the model. At the same time, advances in approximate
Bayesian methods have made posterior approximation for flexible neural network
models practical. Thus, it is attractive to consider approximate Bayesian
neural networks in a Thompson Sampling framework. To understand the impact of
using an approximate posterior on Thompson Sampling, we benchmark
well-established and recently developed methods for approximate posterior
sampling combined with Thompson Sampling over a series of contextual bandit
problems. We found that many approaches that have been successful in the
supervised learning setting underperformed in the sequential decision-making
scenario. In particular, we highlight the challenge of adapting slowly
converging uncertainty estimates to the online setting.","['stat.ML', 'cs.LG']",165,109
Optical Flow Estimation using a Spatial Pyramid Network,"We learn to compute optical flow by combining a classical spatial-pyramid
formulation with deep learning. This estimates large motions in a
coarse-to-fine approach by warping one image of a pair at each pyramid level by
the current flow estimate and computing an update to the flow. Instead of the
standard minimization of an objective function at each pyramid level, we train
one deep network per level to compute the flow update. Unlike the recent
FlowNet approach, the networks do not need to deal with large motions; these
are dealt with by the pyramid. This has several advantages. First, our Spatial
Pyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms
of model parameters. This makes it more efficient and appropriate for embedded
applications. Second, since the flow at each pyramid level is small (< 1
pixel), a convolutional approach applied to pairs of warped images is
appropriate. Third, unlike FlowNet, the learned convolution filters appear
similar to classical spatio-temporal filters, giving insight into the method
and how to improve it. Our results are more accurate than FlowNet on most
standard benchmarks, suggesting a new direction of combining classical flow
methods with deep learning.",['cs.CV'],199,120
PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors,"We present PPF-FoldNet for unsupervised learning of 3D local descriptors on
pure point cloud geometry. Based on the folding-based auto-encoding of well
known point pair features, PPF-FoldNet offers many desirable properties: it
necessitates neither supervision, nor a sensitive local reference frame,
benefits from point-set sparsity, is end-to-end, fast, and can extract powerful
rotation invariant descriptors. Thanks to a novel feature visualization, its
evolution can be monitored to provide interpretable insights. Our extensive
experiments demonstrate that despite having six degree-of-freedom invariance
and lack of training labels, our network achieves state of the art results in
standard benchmark datasets and outperforms its competitors when rotations and
varying point densities are present. PPF-FoldNet achieves $9\%$ higher recall
on standard benchmarks, $23\%$ higher recall when rotations are introduced into
the same datasets and finally, a margin of $>35\%$ is attained when point
density is significantly decreased.","['cs.CV', 'cs.CG', 'cs.LG', 'cs.RO']",152,110
Evaluating Logical Generalization in Graph Neural Networks,"Recent research has highlighted the role of relational inductive biases in
building learning agents that can generalize and reason in a compositional
manner. However, while relational learning algorithms such as graph neural
networks (GNNs) show promise, we do not understand how effectively these
approaches can adapt to new tasks. In this work, we study the task of logical
generalization using GNNs by designing a benchmark suite grounded in
first-order logic. Our benchmark suite, GraphLog, requires that learning
algorithms perform rule induction in different synthetic logics, represented as
knowledge graphs. GraphLog consists of relation prediction tasks on 57 distinct
logical domains. We use GraphLog to evaluate GNNs in three different setups:
single-task supervised learning, multi-task pretraining, and continual
learning. Unlike previous benchmarks, our approach allows us to precisely
control the logical relationship between the different tasks. We find that the
ability for models to generalize and adapt is strongly determined by the
diversity of the logical rules they encounter during training, and our results
highlight new challenges for the design of GNN models. We publicly release the
dataset and code used to generate and interact with the dataset at
https://www.cs.mcgill.ca/~ksinha4/graphlog.","['cs.LG', 'stat.ML']",198,135
BoxNet: Deep Learning Based Biomedical Image Segmentation Using Boxes Only Annotation,"In recent years, deep learning (DL) methods have become powerful tools for
biomedical image segmentation. However, high annotation efforts and costs are
commonly needed to acquire sufficient biomedical training data for DL models.
To alleviate the burden of manual annotation, in this paper, we propose a new
weakly supervised DL approach for biomedical image segmentation using boxes
only annotation. First, we develop a method to combine graph search (GS) and DL
to generate fine object masks from box annotation, in which DL uses box
annotation to compute a rough segmentation for GS and then GS is applied to
locate the optimal object boundaries. During the mask generation process, we
carefully utilize information from box annotation to filter out potential
errors, and then use the generated masks to train an accurate DL segmentation
network. Extensive experiments on gland segmentation in histology images, lymph
node segmentation in ultrasound images, and fungus segmentation in electron
microscopy images show that our approach attains superior performance over the
best known state-of-the-art weakly supervised DL method and is able to achieve
(1) nearly the same accuracy compared to fully supervised DL methods with far
less annotation effort, (2) significantly better results with similar
annotation time, and (3) robust performance in various applications.",['cs.CV'],209,134
Learning to Observe: Approximating Human Perceptual Thresholds for Detection of Suprathreshold Image Transformations,"Many tasks in computer vision are often calibrated and evaluated relative to
human perception. In this paper, we propose to directly approximate the
perceptual function performed by human observers completing a visual detection
task. Specifically, we present a novel methodology for learning to detect image
transformations visible to human observers through approximating perceptual
thresholds. To do this, we carry out a subjective two-alternative forced-choice
study to estimate perceptual thresholds of human observers detecting local
exposure shifts in images. We then leverage transformation equivariant
representation learning to overcome issues of limited perceptual data. This
representation is then used to train a dense convolutional classifier capable
of detecting local suprathreshold exposure shifts - a distortion common to
image composites. In this context, our model can approximate perceptual
thresholds with an average error of 0.1148 exposure stops between empirical and
predicted thresholds. It can also be trained to detect a range of different
local transformations.",['cs.CV'],154,104
Neural Proximal/Trust Region Policy Optimization Attains Globally Optimal Policy,"Proximal policy optimization and trust region policy optimization (PPO and
TRPO) with actor and critic parametrized by neural networks achieve significant
empirical success in deep reinforcement learning. However, due to nonconvexity,
the global convergence of PPO and TRPO remains less understood, which separates
theory from practice. In this paper, we prove that a variant of PPO and TRPO
equipped with overparametrized neural networks converges to the globally
optimal policy at a sublinear rate. The key to our analysis is the global
convergence of infinite-dimensional mirror descent under a notion of one-point
monotonicity, where the gradient and iterate are instantiated by neural
networks. In particular, the desirable representation power and optimization
geometry induced by the overparametrization of such neural networks allow them
to accurately approximate the infinite-dimensional gradient and iterate.","['cs.LG', 'math.OC', 'stat.ML']",132,86
Effects of sparse rewards of different magnitudes in the speed of learning of model-based actor critic methods,"Actor critic methods with sparse rewards in model-based deep reinforcement
learning typically require a deterministic binary reward function that reflects
only two possible outcomes: if, for each step, the goal has been achieved or
not. Our hypothesis is that we can influence an agent to learn faster by
applying an external environmental pressure during training, which adversely
impacts its ability to get higher rewards. As such, we deviate from the
classical paradigm of sparse rewards and add a uniformly sampled reward value
to the baseline reward to show that (1) sample efficiency of the training
process can be correlated to the adversity experienced during training, (2) it
is possible to achieve higher performance in less time and with less resources,
(3) we can reduce the performance variability experienced seed over seed, (4)
there is a maximum point after which more pressure will not generate better
results, and (5) that random positive incentives have an adverse effect when
using a negative reward strategy, making an agent under those conditions learn
poorly and more slowly. These results have been shown to be valid for Deep
Deterministic Policy Gradients using Hindsight Experience Replay in a well
known Mujoco environment, but we argue that they could be generalized to other
methods and environments as well.","['cs.LG', 'cs.RO', 'stat.ML']",212,144
A Deep Primal-Dual Network for Guided Depth Super-Resolution,"In this paper we present a novel method to increase the spatial resolution of
depth images. We combine a deep fully convolutional network with a non-local
variational method in a deep primal-dual network. The joint network computes a
noise-free, high-resolution estimate from a noisy, low-resolution input depth
map. Additionally, a high-resolution intensity image is used to guide the
reconstruction in the network. By unrolling the optimization steps of a
first-order primal-dual algorithm and formulating it as a network, we can train
our joint method end-to-end. This not only enables us to learn the weights of
the fully convolutional network, but also to optimize all parameters of the
variational method and its optimization procedure. The training of such a deep
network requires a large dataset for supervision. Therefore, we generate
high-quality depth maps and corresponding color images with a physically based
renderer. In an exhaustive evaluation we show that our method outperforms the
state-of-the-art on multiple benchmarks.",['cs.CV'],170,104
MVF-Net: Multi-View 3D Face Morphable Model Regression,"We address the problem of recovering the 3D geometry of a human face from a
set of facial images in multiple views. While recent studies have shown
impressive progress in 3D Morphable Model (3DMM) based facial reconstruction,
the settings are mostly restricted to a single view. There is an inherent
drawback in the single-view setting: the lack of reliable 3D constraints can
cause unresolvable ambiguities. We in this paper explore 3DMM-based shape
recovery in a different setting, where a set of multi-view facial images are
given as input. A novel approach is proposed to regress 3DMM parameters from
multi-view inputs with an end-to-end trainable Convolutional Neural Network
(CNN). Multiview geometric constraints are incorporated into the network by
establishing dense correspondences between different views leveraging a novel
self-supervised view alignment loss. The main ingredient of the view alignment
loss is a differentiable dense optical flow estimator that can backpropagate
the alignment errors between an input view and a synthetic rendering from
another input view, which is projected to the target view through the 3D shape
to be inferred. Through minimizing the view alignment loss, better 3D shapes
can be recovered such that the synthetic projections from one view to another
can better align with the observed image. Extensive experiments demonstrate the
superiority of the proposed method over other 3DMM methods.",['cs.CV'],226,129
Sem-GAN: Semantically-Consistent Image-to-Image Translation,"Unpaired image-to-image translation is the problem of mapping an image in the
source domain to one in the target domain, without requiring corresponding
image pairs. To ensure the translated images are realistically plausible,
recent works, such as Cycle-GAN, demands this mapping to be invertible. While,
this requirement demonstrates promising results when the domains are unimodal,
its performance is unpredictable in a multi-modal scenario such as in an image
segmentation task. This is because, invertibility does not necessarily enforce
semantic correctness. To this end, we present a semantically-consistent GAN
framework, dubbed Sem-GAN, in which the semantics are defined by the class
identities of image segments in the source domain as produced by a semantic
segmentation algorithm. Our proposed framework includes consistency constraints
on the translation task that, together with the GAN loss and the
cycle-constraints, enforces that the images when translated will inherit the
appearances of the target domain, while (approximately) maintaining their
identities from the source domain. We present experiments on several
image-to-image translation tasks and demonstrate that Sem-GAN improves the
quality of the translated images significantly, sometimes by more than 20% on
the FCN score. Further, we show that semantic segmentation models, trained with
synthetic images translated via Sem-GAN, leads to significantly better
segmentation results than other variants.",['cs.CV'],220,124
Segmentation hirarchique faiblement supervise,"Image segmentation is the process of partitioning an image into a set of
meaningful regions according to some criteria. Hierarchical segmentation has
emerged as a major trend in this regard as it favors the emergence of important
regions at different scales. On the other hand, many methods allow us to have
prior information on the position of structures of interest in the images. In
this paper, we present a versatile hierarchical segmentation method that takes
into account any prior spatial information and outputs a hierarchical
segmentation that emphasizes the contours or regions of interest while
preserving the important structures in the image. An application of this method
to the weakly-supervised segmentation problem is presented.","['stat.ML', 'cs.CV', 'cs.LG', 'cs.NE']",115,74
Generalization of Reinforcement Learners with Working and Episodic Memory,"Memory is an important aspect of intelligence and plays a role in many deep
reinforcement learning models. However, little progress has been made in
understanding when specific memory systems help more than others and how well
they generalize. The field also has yet to see a prevalent consistent and
rigorous approach for evaluating agent performance on holdout data. In this
paper, we aim to develop a comprehensive methodology to test different kinds of
memory in an agent and assess how well the agent can apply what it learns in
training to a holdout set that differs from the training set along dimensions
that we suggest are relevant for evaluating memory-specific generalization. To
that end, we first construct a diverse set of memory tasks that allow us to
evaluate test-time generalization across multiple dimensions. Second, we
develop and perform multiple ablations on an agent architecture that combines
multiple memory systems, observe its baseline models, and investigate its
performance against the task suite.","['cs.LG', 'cs.AI', 'stat.ML']",163,106
Image Retrieval System Base on EMD Similarity Measure and S-Tree,"The paper approaches the binary signature for each image based on the
percentage of the pixels in each color images, at the same time the paper
builds a similar measure between images based on EMD (Earth Mover's Distance).
Besides, the paper proceeded to create the S-tree based on the similar measure
EMD to store the image's binary signatures to quickly query image signature
data. From there, the paper build an image retrieval algorithm and CBIR
(Content-Based Image Retrieval) based on a similar measure EMD and S-tree.
Based on this theory, the paper proceeded to build application and experimental
assessment of the process of querying image on the database system which have
over 10,000 images.","['cs.CV', 'cs.IR', 'H.2.8; H.3.3']",120,67
Urban flows prediction from spatial-temporal data using machine learning: A survey,"Urban spatial-temporal flows prediction is of great importance to traffic
management, land use, public safety, etc. Urban flows are affected by several
complex and dynamic factors, such as patterns of human activities, weather,
events and holidays. Datasets evaluated the flows come from various sources in
different domains, e.g. mobile phone data, taxi trajectories data, metro/bus
swiping data, bike-sharing data and so on. To summarize these methodologies of
urban flows prediction, in this paper, we first introduce four main factors
affecting urban flows. Second, in order to further analysis urban flows, a
preparation process of multi-sources spatial-temporal data related with urban
flows is partitioned into three groups. Third, we choose the spatial-temporal
dynamic data as a case study for the urban flows prediction task. Fourth, we
analyze and compare some well-known and state-of-the-art flows prediction
methods in detail, classifying them into five categories: statistics-based,
traditional machine learning-based, deep learning-based, reinforcement
learning-based and transfer learning-based methods. Finally, we give open
challenges of urban flows prediction and an outlook in the future of this
field. This paper will facilitate researchers find suitable methods and open
datasets for addressing urban spatial-temporal flows forecast problems.","['cs.LG', 'stat.ML']",207,130
Graph-based Reinforcement Learning for Active Learning in Real Time: An Application in Modeling River Networks,"Effective training of advanced ML models requires large amounts of labeled
data, which is often scarce in scientific problems given the substantial human
labor and material cost to collect labeled data. This poses a challenge on
determining when and where we should deploy measuring instruments (e.g.,
in-situ sensors) to collect labeled data efficiently. This problem differs from
traditional pool-based active learning settings in that the labeling decisions
have to be made immediately after we observe the input data that come in a time
series. In this paper, we develop a real-time active learning method that uses
the spatial and temporal contextual information to select representative query
samples in a reinforcement learning framework. To reduce the need for large
training data, we further propose to transfer the policy learned from
simulation data which is generated by existing physics-based models. We
demonstrate the effectiveness of the proposed method by predicting streamflow
and water temperature in the Delaware River Basin given a limited budget for
collecting labeled data. We further study the spatial and temporal distribution
of selected samples to verify the ability of this method in selecting
informative samples over space and time.","['cs.LG', 'cs.AI']",196,122
Introducing Symmetries to Black Box Meta Reinforcement Learning,"Meta reinforcement learning (RL) attempts to discover new RL algorithms
automatically from environment interaction. In so-called black-box approaches,
the policy and the learning algorithm are jointly represented by a single
neural network. These methods are very flexible, but they tend to underperform
in terms of generalisation to new, unseen environments. In this paper, we
explore the role of symmetries in meta-generalisation. We show that a recent
successful meta RL approach that meta-learns an objective for
backpropagation-based learning exhibits certain symmetries (specifically the
reuse of the learning rule, and invariance to input and output permutations)
that are not present in typical black-box meta RL systems. We hypothesise that
these symmetries can play an important role in meta-generalisation. Building
off recent work in black-box supervised meta learning, we develop a black-box
meta RL system that exhibits these same symmetries. We show through careful
experimentation that incorporating these symmetries can lead to algorithms with
a greater ability to generalise to unseen action & observation spaces, tasks,
and environments.","['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",173,102
Adversarial Attacks on Deep Models for Financial Transaction Records,"Machine learning models using transaction records as inputs are popular among
financial institutions. The most efficient models use deep-learning
architectures similar to those in the NLP community, posing a challenge due to
their tremendous number of parameters and limited robustness. In particular,
deep-learning models are vulnerable to adversarial attacks: a little change in
the input harms the model's output.
  In this work, we examine adversarial attacks on transaction records data and
defences from these attacks. The transaction records data have a different
structure than the canonical NLP or time series data, as neighbouring records
are less connected than words in sentences, and each record consists of both
discrete merchant code and continuous transaction amount. We consider a
black-box attack scenario, where the attack doesn't know the true decision
model, and pay special attention to adding transaction tokens to the end of a
sequence. These limitations provide more realistic scenario, previously
unexplored in NLP world.
  The proposed adversarial attacks and the respective defences demonstrate
remarkable performance using relevant datasets from the financial industry. Our
results show that a couple of generated transactions are sufficient to fool a
deep-learning model. Further, we improve model robustness via adversarial
training or separate adversarial examples detection. This work shows that
embedding protection from adversarial attacks improves model robustness,
allowing a wider adoption of deep models for transaction records in banking and
finance.","['cs.LG', 'cs.CR', 'q-fin.ST']",233,143
Weakly-supervised Graph Meta-learning for Few-shot Node Classification,"Graphs are widely used to model the relational structure of data, and the
research of graph machine learning (ML) has a wide spectrum of applications
ranging from drug design in molecular graphs to friendship recommendation in
social networks. Prevailing approaches for graph ML typically require abundant
labeled instances in achieving satisfactory results, which is commonly
infeasible in real-world scenarios since labeled data for newly emerged
concepts (e.g., new categorizations of nodes) on graphs is limited. Though
meta-learning has been applied to different few-shot graph learning problems,
most existing efforts predominately assume that all the data from those seen
classes is gold-labeled, while those methods may lose their efficacy when the
seen data is weakly-labeled with severe label noise. As such, we aim to
investigate a novel problem of weakly-supervised graph meta-learning for
improving the model robustness in terms of knowledge transfer. To achieve this
goal, we propose a new graph meta-learning framework -- Graph Hallucination
Networks (Meta-GHN) in this paper. Based on a new robustness-enhanced episodic
training, Meta-GHN is meta-learned to hallucinate clean node representations
from weakly-labeled data and extracts highly transferable meta-knowledge, which
enables the model to quickly adapt to unseen tasks with few labeled instances.
Extensive experiments demonstrate the superiority of Meta-GHN over existing
graph meta-learning studies on the task of weakly-supervised few-shot node
classification.",['cs.LG'],235,143
Gaze Estimation using Transformer,"Recent work has proven the effectiveness of transformers in many computer
vision tasks. However, the performance of transformers in gaze estimation is
still unexplored. In this paper, we employ transformers and assess their
effectiveness for gaze estimation. We consider two forms of vision transformer
which are pure transformers and hybrid transformers. We first follow the
popular ViT and employ a pure transformer to estimate gaze from images. On the
other hand, we preserve the convolutional layers and integrate CNNs as well as
transformers. The transformer serves as a component to complement CNNs. We
compare the performance of the two transformers in gaze estimation. The Hybrid
transformer significantly outperforms the pure transformer in all evaluation
datasets with less parameters. We further conduct experiments to assess the
effectiveness of the hybrid transformer and explore the advantage of
self-attention mechanism. Experiments show the hybrid transformer can achieve
state-of-the-art performance in all benchmarks with pre-training.To facilitate
further research, we release codes and models in
https://github.com/yihuacheng/GazeTR.",['cs.CV'],171,99
3G structure for image caption generation,"It is a big challenge of computer vision to make machine automatically
describe the content of an image with a natural language sentence. Previous
works have made great progress on this task, but they only use the global or
local image feature, which may lose some important subtle or global information
of an image. In this paper, we propose a model with 3-gated model which fuses
the global and local image features together for the task of image caption
generation. The model mainly has three gated structures. 1) Gate for the global
image feature, which can adaptively decide when and how much the global image
feature should be imported into the sentence generator. 2) The gated recurrent
neural network (RNN) is used as the sentence generator. 3) The gated feedback
method for stacking RNN is employed to increase the capability of nonlinearity
fitting. More specially, the global and local image features are combined
together in this paper, which makes full use of the image information. The
global image feature is controlled by the first gate and the local image
feature is selected by the attention mechanism. With the latter two gates, the
relationship between image and text can be well explored, which improves the
performance of the language part as well as the multi-modal embedding part.
Experimental results show that our proposed method outperforms the
state-of-the-art for image caption generation.",['cs.CV'],235,130
Fast Reinforcement Learning with Large Action Sets using Error-Correcting Output Codes for MDP Factorization,"The use of Reinforcement Learning in real-world scenarios is strongly limited
by issues of scale. Most RL learning algorithms are unable to deal with
problems composed of hundreds or sometimes even dozens of possible actions, and
therefore cannot be applied to many real-world problems. We consider the RL
problem in the supervised classification framework where the optimal policy is
obtained through a multiclass classifier, the set of classes being the set of
actions of the problem. We introduce error-correcting output codes (ECOCs) in
this setting and propose two new methods for reducing complexity when using
rollouts-based approaches. The first method consists in using an ECOC-based
classifier as the multiclass classifier, reducing the learning complexity from
O(A2) to O(Alog(A)). We then propose a novel method that profits from the
ECOC's coding dictionary to split the initial MDP into O(log(A)) seperate
two-action MDPs. This second method reduces learning complexity even further,
from O(A2) to O(log(A)), thus rendering problems with large action sets
tractable. We finish by experimentally demonstrating the advantages of our
approach on a set of benchmark problems, both in speed and performance.","['cs.LG', 'stat.ML', '68T05']",197,123
On the Effectiveness of Least Squares Generative Adversarial Networks,"Unsupervised learning with generative adversarial networks (GANs) has proven
to be hugely successful. Regular GANs hypothesize the discriminator as a
classifier with the sigmoid cross entropy loss function. However, we found that
this loss function may lead to the vanishing gradients problem during the
learning process. To overcome such a problem, we propose in this paper the
Least Squares Generative Adversarial Networks (LSGANs) which adopt the least
squares loss for both the discriminator and the generator. We show that
minimizing the objective function of LSGAN yields minimizing the Pearson
$\chi^2$ divergence. We also show that the derived objective function that
yields minimizing the Pearson $\chi^2$ divergence performs better than the
classical one of using least squares for classification. There are two benefits
of LSGANs over regular GANs. First, LSGANs are able to generate higher quality
images than regular GANs. Second, LSGANs perform more stably during the
learning process. For evaluating the image quality, we conduct both qualitative
and quantitative experiments, and the experimental results show that LSGANs can
generate higher quality images than regular GANs. Furthermore, we evaluate the
stability of LSGANs in two groups. One is to compare between LSGANs and regular
GANs without gradient penalty. We conduct three experiments, including Gaussian
mixture distribution, difficult architectures, and a newly proposed method ---
datasets with small variability, to illustrate the stability of LSGANs. The
other one is to compare between LSGANs with gradient penalty (LSGANs-GP) and
WGANs with gradient penalty (WGANs-GP). The experimental results show that
LSGANs-GP succeed in training for all the difficult architectures used in
WGANs-GP, including 101-layer ResNet.","['cs.CV', 'cs.LG', 'stat.ML']",267,139
Deep Time Series Forecasting with Shape and Temporal Criteria,"This paper addresses the problem of multi-step time series forecasting for
non-stationary signals that can present sudden changes. Current
state-of-the-art deep learning forecasting methods, often trained with variants
of the MSE, lack the ability to provide sharp predictions in deterministic and
probabilistic contexts. To handle these challenges, we propose to incorporate
shape and temporal criteria in the training objective of deep models. We define
shape and temporal similarities and dissimilarities, based on a smooth
relaxation of Dynamic Time Warping (DTW) and Temporal Distortion Index (TDI),
that enable to build differentiable loss functions and positive semi-definite
(PSD) kernels. With these tools, we introduce DILATE (DIstortion Loss including
shApe and TimE), a new objective for deterministic forecasting, that explicitly
incorporates two terms supporting precise shape and temporal change detection.
For probabilistic forecasting, we introduce STRIPE++ (Shape and Time diverRsIty
in Probabilistic forEcasting), a framework for providing a set of sharp and
diverse forecasts, where the structured shape and time diversity is enforced
with a determinantal point process (DPP) diversity loss. Extensive experiments
and ablations studies on synthetic and real-world datasets confirm the benefits
of leveraging shape and time features in time series forecasting.","['stat.ML', 'cs.AI', 'cs.LG']",198,132
Conditional Meta-Learning of Linear Representations,"Standard meta-learning for representation learning aims to find a common
representation to be shared across multiple tasks. The effectiveness of these
methods is often limited when the nuances of the tasks' distribution cannot be
captured by a single representation. In this work we overcome this issue by
inferring a conditioning function, mapping the tasks' side information (such as
the tasks' training dataset itself) into a representation tailored to the task
at hand. We study environments in which our conditional strategy outperforms
standard meta-learning, such as those in which tasks can be organized in
separate clusters according to the representation they share. We then propose a
meta-algorithm capable of leveraging this advantage in practice. In the
unconditional setting, our method yields a new estimator enjoying faster
learning rates and requiring less hyper-parameters to tune than current
state-of-the-art methods. Our results are supported by preliminary experiments.",['cs.LG'],151,103
Generating Attention from Classifier Activations for Fine-grained Recognition,"Recent advances in fine-grained recognition utilize attention maps to
localize objects of interest. Although there are many ways to generate
attention maps, most of them rely on sophisticated loss functions or complex
training processes. In this work, we propose a simple and straightforward
attention generation model based on the output activations of classifiers. The
advantage of our model is that it can be easily trained with image level labels
and softmax loss functions. More specifically, multiple linear local
classifiers are firstly adopted to perform fine-grained classification at each
location of high level CNN feature maps. The attention map is generated by
aggregating and max-pooling the output activations. Then the attention map
serves as a surrogate target object mask to train those local classifiers,
similar to training models for semantic segmentation. Our model achieves
state-of-the-art results on three heavily benchmarked datasets, i.e. 87.9% on
CUB-200-2011 dataset, 94.1% on Stanford Cars dataset and 92.1% on FGVC-Aircraft
dataset, demonstrating its effectiveness on fine-grained recognition tasks.",['cs.CV'],176,125
Estimating Skin Tone and Effects on Classification Performance in Dermatology Datasets,"Recent advances in computer vision and deep learning have led to
breakthroughs in the development of automated skin image analysis. In
particular, skin cancer classification models have achieved performance higher
than trained expert dermatologists. However, no attempt has been made to
evaluate the consistency in performance of machine learning models across
populations with varying skin tones. In this paper, we present an approach to
estimate skin tone in benchmark skin disease datasets, and investigate whether
model performance is dependent on this measure. Specifically, we use individual
typology angle (ITA) to approximate skin tone in dermatology datasets. We look
at the distribution of ITA values to better understand skin color
representation in two benchmark datasets: 1) the ISIC 2018 Challenge dataset, a
collection of dermoscopic images of skin lesions for the detection of skin
cancer, and 2) the SD-198 dataset, a collection of clinical images capturing a
wide variety of skin diseases. To estimate ITA, we first develop segmentation
models to isolate non-diseased areas of skin. We find that the majority of the
data in the the two datasets have ITA values between 34.5{\deg} and 48{\deg},
which are associated with lighter skin, and is consistent with
under-representation of darker skinned populations in these datasets. We also
find no measurable correlation between performance of machine learning model
and ITA values, though more comprehensive data is needed for further
validation.","['cs.CV', 'cs.CY', 'stat.ML']",233,136
In the Eye of the Beholder: Gaze and Actions in First Person Video,"We address the task of jointly determining what a person is doing and where
they are looking based on the analysis of video captured by a headworn camera.
To facilitate our research, we first introduce the EGTEA Gaze+ dataset. Our
dataset comes with videos, gaze tracking data, hand masks and action
annotations, thereby providing the most comprehensive benchmark for First
Person Vision (FPV). Moving beyond the dataset, we propose a novel deep model
for joint gaze estimation and action recognition in FPV. Our method describes
the participant's gaze as a probabilistic variable and models its distribution
using stochastic units in a deep network. We further sample from these
stochastic units, generating an attention map to guide the aggregation of
visual features for action recognition. Our method is evaluated on our EGTEA
Gaze+ dataset and achieves a performance level that exceeds the
state-of-the-art by a significant margin. More importantly, we demonstrate that
our model can be applied to larger scale FPV dataset---EPIC-Kitchens even
without using gaze, offering new state-of-the-art results on FPV action
recognition.",['cs.CV'],182,118
360-Indoor: Towards Learning Real-World Objects in 360 Indoor Equirectangular Images,"While there are several widely used object detection datasets, current
computer vision algorithms are still limited in conventional images. Such
images narrow our vision in a restricted region. On the other hand, 360{\deg}
images provide a thorough sight. In this paper, our goal is to provide a
standard dataset to facilitate the vision and machine learning communities in
360{\deg} domain. To facilitate the research, we present a real-world 360{\deg}
panoramic object detection dataset, 360-Indoor, which is a new benchmark for
visual object detection and class recognition in 360{\deg} indoor images. It is
achieved by gathering images of complex indoor scenes containing common objects
and the intensive annotated bounding field-of-view. In addition, 360-Indoor has
several distinct properties: (1) the largest category number (37 labels in
total). (2) the most complete annotations on average (27 bounding boxes per
image). The selected 37 objects are all common in indoor scene. With around 3k
images and 90k labels in total, 360-Indoor achieves the largest dataset for
detection in 360{\deg} images. In the end, extensive experiments on the
state-of-the-art methods for both classification and detection are provided. We
will release this dataset in the near future.",['cs.CV'],205,122
Deep Hashing for Secure Multimodal Biometrics,"When compared to unimodal systems, multimodal biometric systems have several
advantages, including lower error rate, higher accuracy, and larger population
coverage. However, multimodal systems have an increased demand for integrity
and privacy because they must store multiple biometric traits associated with
each user. In this paper, we present a deep learning framework for
feature-level fusion that generates a secure multimodal template from each
user's face and iris biometrics. We integrate a deep hashing (binarization)
technique into the fusion architecture to generate a robust binary multimodal
shared latent representation. Further, we employ a hybrid secure architecture
by combining cancelable biometrics with secure sketch techniques and integrate
it with a deep hashing framework, which makes it computationally prohibitive to
forge a combination of multiple biometrics that pass the authentication. The
efficacy of the proposed approach is shown using a multimodal database of face
and iris and it is observed that the matching performance is improved due to
the fusion of multiple biometrics. Furthermore, the proposed approach also
provides cancelability and unlinkability of the templates along with improved
privacy of the biometric data. Additionally, we also test the proposed hashing
function for an image retrieval application using a benchmark dataset. The main
goal of this paper is to develop a method for integrating multimodal fusion,
deep hashing, and biometric security, with an emphasis on structural data from
modalities like face and iris. The proposed approach is in no way a general
biometric security framework that can be applied to all biometric modalities,
as further research is needed to extend the proposed framework to other
unconstrained biometric modalities.","['cs.CV', 'cs.AI', 'cs.IT', 'math.IT']",266,145
Assigning Apples to Individual Trees in Dense Orchards using 3D Color Point Clouds,"We propose a 3D color point cloud processing pipeline to count apples on
individual apple trees in trellis structured orchards. Fruit counting at the
tree level requires separating trees, which is challenging in dense orchards.
We employ point clouds acquired from the leaf-off orchard in winter period,
where the branch structure is visible, to delineate tree crowns. We localize
apples in point clouds acquired in harvest period. Alignment of the two point
clouds enables mapping apple locations to the delineated winter cloud and
assigning each apple to its bearing tree. Our apple assignment method achieves
an accuracy rate higher than 95%. In addition to presenting a first proof of
feasibility, we also provide suggestions for further improvement on our apple
assignment pipeline.",['cs.CV'],123,85
Gradient-EM Bayesian Meta-learning,"Bayesian meta-learning enables robust and fast adaptation to new tasks with
uncertainty assessment. The key idea behind Bayesian meta-learning is empirical
Bayes inference of hierarchical model. In this work, we extend this framework
to include a variety of existing methods, before proposing our variant based on
gradient-EM algorithm. Our method improves computational efficiency by avoiding
back-propagation computation in the meta-update step, which is exhausting for
deep neural networks. Furthermore, it provides flexibility to the inner-update
optimization procedure by decoupling it from meta-update. Experiments on
sinusoidal regression, few-shot image classification, and policy-based
reinforcement learning show that our method not only achieves better accuracy
with less computation cost, but is also more robust to uncertainty.","['cs.LG', 'stat.ML']",122,95
Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning,"The ability to act in multiple environments and transfer previous knowledge
to new situations can be considered a critical aspect of any intelligent agent.
Towards this goal, we define a novel method of multitask and transfer learning
that enables an autonomous agent to learn how to behave in multiple tasks
simultaneously, and then generalize its knowledge to new domains. This method,
termed ""Actor-Mimic"", exploits the use of deep reinforcement learning and model
compression techniques to train a single policy network that learns how to act
in a set of distinct tasks by using the guidance of several expert teachers. We
then show that the representations learnt by the deep policy network are
capable of generalizing to new tasks with no prior expert guidance, speeding up
learning in novel environments. Although our method can in general be applied
to a wide range of problems, we use Atari games as a testing environment to
demonstrate these methods.",['cs.LG'],156,97
On Lyapunov Exponents for RNNs: Understanding Information Propagation Using Dynamical Systems Tools,"Recurrent neural networks (RNNs) have been successfully applied to a variety
of problems involving sequential data, but their optimization is sensitive to
parameter initialization, architecture, and optimizer hyperparameters.
Considering RNNs as dynamical systems, a natural way to capture stability,
i.e., the growth and decay over long iterates, are the Lyapunov Exponents
(LEs), which form the Lyapunov spectrum. The LEs have a bearing on stability of
RNN training dynamics because forward propagation of information is related to
the backward propagation of error gradients. LEs measure the asymptotic rates
of expansion and contraction of nonlinear system trajectories, and generalize
stability analysis to the time-varying attractors structuring the
non-autonomous dynamics of data-driven RNNs. As a tool to understand and
exploit stability of training dynamics, the Lyapunov spectrum fills an existing
gap between prescriptive mathematical approaches of limited scope and
computationally-expensive empirical approaches. To leverage this tool, we
implement an efficient way to compute LEs for RNNs during training, discuss the
aspects specific to standard RNN architectures driven by typical sequential
datasets, and show that the Lyapunov spectrum can serve as a robust readout of
training stability across hyperparameters. With this exposition-oriented
contribution, we hope to draw attention to this understudied, but theoretically
grounded tool for understanding training stability in RNNs.","['cs.LG', 'math.DS', 'nlin.CD', 'stat.ML']",213,134
A Simple General Approach to Balance Task Difficulty in Multi-Task Learning,"In multi-task learning, difficulty levels of different tasks are varying.
There are many works to handle this situation and we classify them into five
categories, including the direct sum approach, the weighted sum approach, the
maximum approach, the curriculum learning approach, and the multi-objective
optimization approach. Those approaches have their own limitations, for
example, using manually designed rules to update task weights, non-smooth
objective function, and failing to incorporate other functions than training
losses. In this paper, to alleviate those limitations, we propose a Balanced
Multi-Task Learning (BMTL) framework. Different from existing studies which
rely on task weighting, the BMTL framework proposes to transform the training
loss of each task to balance difficulty levels among tasks based on an
intuitive idea that tasks with larger training losses will receive more
attention during the optimization procedure. We analyze the transformation
function and derive necessary conditions. The proposed BMTL framework is very
simple and it can be combined with most multi-task learning models. Empirical
studies show the state-of-the-art performance of the proposed BMTL framework.","['cs.LG', 'stat.ML']",180,119
Attribute-Induced Bias Eliminating for Transductive Zero-Shot Learning,"Transductive Zero-shot learning (ZSL) targets to recognize the unseen
categories by aligning the visual and semantic information in a joint embedding
space. There exist four kinds of domain biases in Transductive ZSL, i.e.,
visual bias and semantic bias between two domains and two visual-semantic
biases in respective seen and unseen domains, but existing work only focuses on
the part of them, which leads to severe semantic ambiguity during the knowledge
transfer. To solve the above problem, we propose a novel Attribute-Induced Bias
Eliminating (AIBE) module for Transductive ZSL. Specifically, for the visual
bias between two domains, the Mean-Teacher module is first leveraged to bridge
the visual representation discrepancy between two domains with unsupervised
learning and unlabelled images. Then, an attentional graph attribute embedding
is proposed to reduce the semantic bias between seen and unseen categories,
which utilizes the graph operation to capture the semantic relationship between
categories. Besides, to reduce the semantic-visual bias in the seen domain, we
align the visual center of each category, instead of the individual visual data
point, with the corresponding semantic attributes, which further preserves the
semantic relationship in the embedding space. Finally, for the semantic-visual
bias in the unseen domain, an unseen semantic alignment constraint is designed
to align visual and semantic space in an unsupervised manner. The evaluations
on several benchmarks demonstrate the effectiveness of the proposed method,
e.g., obtaining the 82.8%/75.5%, 97.1%/82.5%, and 73.2%/52.1% for
Conventional/Generalized ZSL settings for CUB, AwA2, and SUN datasets,
respectively.",['cs.CV'],261,134
Reliable Fidelity and Diversity Metrics for Generative Models,"Devising indicative evaluation metrics for the image generation task remains
an open problem. The most widely used metric for measuring the similarity
between real and generated images has been the Fr\'echet Inception Distance
(FID) score. Because it does not differentiate the fidelity and diversity
aspects of the generated images, recent papers have introduced variants of
precision and recall metrics to diagnose those properties separately. In this
paper, we show that even the latest version of the precision and recall metrics
are not reliable yet. For example, they fail to detect the match between two
identical distributions, they are not robust against outliers, and the
evaluation hyperparameters are selected arbitrarily. We propose density and
coverage metrics that solve the above issues. We analytically and
experimentally show that density and coverage provide more interpretable and
reliable signals for practitioners than the existing metrics. Code:
https://github.com/clovaai/generative-evaluation-prdc.","['cs.CV', 'cs.LG', 'stat.ML']",150,104
Self-Configuring and Evolving Fuzzy Image Thresholding,"Every segmentation algorithm has parameters that need to be adjusted in order
to achieve good results. Evolving fuzzy systems for adjustment of segmentation
parameters have been proposed recently (Evolving fuzzy image segmentation --
EFIS [1]. However, similar to any other algorithm, EFIS too suffers from a few
limitations when used in practice. As a major drawback, EFIS depends on
detection of the object of interest for feature calculation, a task that is
highly application-dependent. In this paper, a new version of EFIS is proposed
to overcome these limitations. The new EFIS, called self-configuring EFIS
(SC-EFIS), uses available training data to auto-configure the parameters that
are fixed in EFIS. As well, the proposed SC-EFIS relies on a feature selection
process that does not require the detection of a region of interest (ROI).",['cs.CV'],135,86
Point Cloud Distortion Quantification based on Potential Energy for Human and Machine Perception,"Distortion quantification of point clouds plays a stealth, yet vital role in
a wide range of human and machine perception tasks. For human perception tasks,
a distortion quantification can substitute subjective experiments to guide 3D
visualization; while for machine perception tasks, a distortion quantification
can work as a loss function to guide the training of deep neural networks for
unsupervised learning tasks. To handle a variety of demands in many
applications, a distortion quantification needs to be distortion discriminable,
differentiable, and have a low computational complexity. Currently, however,
there is a lack of a general distortion quantification that can satisfy all
three conditions. To fill this gap, this work proposes multiscale potential
energy discrepancy (MPED), a distortion quantification to measure point cloud
geometry and color difference. By evaluating at various neighborhood sizes, the
proposed MPED achieves global-local tradeoffs, capturing distortion in a
multiscale fashion. Extensive experimental studies validate MPED's superiority
for both human and machine perception tasks.","['cs.CV', 'eess.IV']",159,101
Graph Residual Flow for Molecular Graph Generation,"Statistical generative models for molecular graphs attract attention from
many researchers from the fields of bio- and chemo-informatics. Among these
models, invertible flow-based approaches are not fully explored yet. In this
paper, we propose a powerful invertible flow for molecular graphs, called graph
residual flow (GRF). The GRF is based on residual flows, which are known for
more flexible and complex non-linear mappings than traditional coupling flows.
We theoretically derive non-trivial conditions such that GRF is invertible, and
present a way of keeping the entire flows invertible throughout the training
and sampling. Experimental results show that a generative model based on the
proposed GRF achieves comparable generation performance, with much smaller
number of trainable parameters compared to the existing flow-based model.","['cs.LG', 'stat.ML']",126,87
Unsupervised High-Resolution Depth Learning From Videos With Dual Networks,"Unsupervised depth learning takes the appearance difference between a target
view and a view synthesized from its adjacent frame as supervisory signal.
Since the supervisory signal only comes from images themselves, the resolution
of training data significantly impacts the performance. High-resolution images
contain more fine-grained details and provide more accurate supervisory signal.
However, due to the limitation of memory and computation power, the original
images are typically down-sampled during training, which suffers heavy loss of
details and disparity accuracy. In order to fully explore the information
contained in high-resolution data, we propose a simple yet effective dual
networks architecture, which can directly take high-resolution images as input
and generate high-resolution and high-accuracy depth map efficiently. We also
propose a Self-assembled Attention (SA-Attention) module to handle low-texture
region. The evaluation on the benchmark KITTI and Make3D datasets demonstrates
that our method achieves state-of-the-art results in the monocular depth
estimation task.",['cs.CV'],162,111
Zero-sample surface defect detection and classification based on semantic feedback neural network,"Defect detection and classification technology has changed from traditional
artificial visual inspection to current intelligent automated inspection, but
most of the current defect detection methods are training related detection
models based on a data-driven approach, taking into account the difficulty of
collecting some sample data in the industrial field. We apply zero-shot
learning technology to the industrial field. Aiming at the problem of the
existing ""Latent Feature Guide Attribute Attention"" (LFGAA) zero-shot image
classification network, the output latent attributes and artificially defined
attributes are different in the semantic space, which leads to the problem of
model performance degradation, proposed an LGFAA network based on semantic
feedback, and improved model performance by constructing semantic embedded
modules and feedback mechanisms. At the same time, for the common domain shift
problem in zero-shot learning, based on the idea of co-training algorithm using
the difference information between different views of data to learn from each
other, we propose an Ensemble Co-training algorithm, which adaptively reduces
the prediction error in image tag embedding from multiple angles. Various
experiments conducted on the zero-shot dataset and the cylinder liner dataset
in the industrial field provide competitive results.","['cs.CV', 'cs.AI']",197,121
Weakly Supervised Thoracic Disease Localization via Disease Masks,"To enable a deep learning-based system to be used in the medical domain as a
computer-aided diagnosis system, it is essential to not only classify diseases
but also present the locations of the diseases. However, collecting
instance-level annotations for various thoracic diseases is expensive.
Therefore, weakly supervised localization methods have been proposed that use
only image-level annotation. While the previous methods presented the disease
location as the most discriminative part for classification, this causes a deep
network to localize wrong areas for indistinguishable X-ray images. To solve
this issue, we propose a spatial attention method using disease masks that
describe the areas where diseases mainly occur. We then apply the spatial
attention to find the precise disease area by highlighting the highest
probability of disease occurrence. Meanwhile, the various sizes, rotations and
noise in chest X-ray images make generating the disease masks challenging. To
reduce the variation among images, we employ an alignment module to transform
an input X-ray image into a generalized image. Through extensive experiments on
the NIH-Chest X-ray dataset with eight kinds of diseases, we show that the
proposed method results in superior localization performances compared to
state-of-the-art methods.","['cs.CV', 'cs.LG']",204,131
Quantifying Explainers of Graph Neural Networks in Computational Pathology,"Explainability of deep learning methods is imperative to facilitate their
clinical adoption in digital pathology. However, popular deep learning methods
and explainability techniques (explainers) based on pixel-wise processing
disregard biological entities' notion, thus complicating comprehension by
pathologists. In this work, we address this by adopting biological entity-based
graph processing and graph explainers enabling explanations accessible to
pathologists. In this context, a major challenge becomes to discern meaningful
explainers, particularly in a standardized and quantifiable fashion. To this
end, we propose herein a set of novel quantitative metrics based on statistics
of class separability using pathologically measurable concepts to characterize
graph explainers. We employ the proposed metrics to evaluate three types of
graph explainers, namely the layer-wise relevance propagation, gradient-based
saliency, and graph pruning approaches, to explain Cell-Graph representations
for Breast Cancer Subtyping. The proposed metrics are also applicable in other
domains by using domain-specific intuitive concepts. We validate the
qualitative and quantitative findings on the BRACS dataset, a large cohort of
breast cancer RoIs, by expert pathologists.",['cs.CV'],173,116
Discrete-Valued Neural Communication,"Deep learning has advanced from fully connected architectures to structured
models organized into components, e.g., the transformer composed of positional
elements, modular architectures divided into slots, and graph neural nets made
up of nodes. In structured models, an interesting question is how to conduct
dynamic and possibly sparse communication among the separate components. Here,
we explore the hypothesis that restricting the transmitted information among
components to discrete representations is a beneficial bottleneck. The
motivating intuition is human language in which communication occurs through
discrete symbols. Even though individuals have different understandings of what
a ""cat"" is based on their specific experiences, the shared discrete token makes
it possible for communication among individuals to be unimpeded by individual
differences in internal representation. To discretize the values of concepts
dynamically communicated among specialist components, we extend the
quantization mechanism from the Vector-Quantized Variational Autoencoder to
multi-headed discretization with shared codebooks and use it for
discrete-valued neural communication (DVNC). Our experiments show that DVNC
substantially improves systematic generalization in a variety of architectures
-- transformers, modular architectures, and graph neural networks. We also show
that the DVNC is robust to the choice of hyperparameters, making the method
very useful in practice. Moreover, we establish a theoretical justification of
our discretization process, proving that it has the ability to increase noise
robustness and reduce the underlying dimensionality of the model.","['cs.LG', 'cs.AI']",229,151
Sensitive Data Detection with High-Throughput Neural Network Models for Financial Institutions,"Named Entity Recognition has been extensively investigated in many fields.
However, the application of sensitive entity detection for production systems
in financial institutions has not been well explored due to the lack of
publicly available, labeled datasets. In this paper, we use internal and
synthetic datasets to evaluate various methods of detecting NPI (Nonpublic
Personally Identifiable) information commonly found within financial
institutions, in both unstructured and structured data formats. Character-level
neural network models including CNN, LSTM, BiLSTM-CRF, and CNN-CRF are
investigated on two prediction tasks: (i) entity detection on multiple data
formats, and (ii) column-wise entity prediction on tabular datasets. We compare
these models with other standard approaches on both real and synthetic data,
with respect to F1-score, precision, recall, and throughput. The real datasets
include internal structured data and public email data with manually tagged
labels. Our experimental results show that the CNN model is simple yet
effective with respect to accuracy and throughput and thus, is the most
suitable candidate model to be deployed in the production environment(s).
Finally, we provide several lessons learned on data limitations, data labelling
and the intrinsic overlap of data entities.",['cs.LG'],194,128
Learning to search efficiently for causally near-optimal treatments,"Finding an effective medical treatment often requires a search by trial and
error. Making this search more efficient by minimizing the number of
unnecessary trials could lower both costs and patient suffering. We formalize
this problem as learning a policy for finding a near-optimal treatment in a
minimum number of trials using a causal inference framework. We give a
model-based dynamic programming algorithm which learns from observational data
while being robust to unmeasured confounding. To reduce time complexity, we
suggest a greedy algorithm which bounds the near-optimality constraint. The
methods are evaluated on synthetic and real-world healthcare data and compared
to model-free reinforcement learning. We find that our methods compare
favorably to the model-free baseline while offering a more transparent
trade-off between search time and treatment efficacy.","['cs.LG', 'stat.ML']",134,96
A Little Fog for a Large Turn,"Small, carefully crafted perturbations called adversarial perturbations can
easily fool neural networks. However, these perturbations are largely additive
and not naturally found. We turn our attention to the field of Autonomous
navigation wherein adverse weather conditions such as fog have a drastic effect
on the predictions of these systems. These weather conditions are capable of
acting like natural adversaries that can help in testing models. To this end,
we introduce a general notion of adversarial perturbations, which can be
created using generative models and provide a methodology inspired by
Cycle-Consistent Generative Adversarial Networks to generate adversarial
weather conditions for a given image. Our formulation and results show that
these images provide a suitable testbed for steering models used in Autonomous
navigation models. Our work also presents a more natural and general definition
of Adversarial perturbations based on Perceptual Similarity.","['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML']",140,97
SBSGAN: Suppression of Inter-Domain Background Shift for Person Re-Identification,"Cross-domain person re-identification (re-ID) is challenging due to the bias
between training and testing domains. We observe that if backgrounds in the
training and testing datasets are very different, it dramatically introduces
difficulties to extract robust pedestrian features, and thus compromises the
cross-domain person re-ID performance. In this paper, we formulate such
problems as a background shift problem. A Suppression of Background Shift
Generative Adversarial Network (SBSGAN) is proposed to generate images with
suppressed backgrounds. Unlike simply removing backgrounds using binary masks,
SBSGAN allows the generator to decide whether pixels should be preserved or
suppressed to reduce segmentation errors caused by noisy foreground masks.
Additionally, we take ID-related cues, such as vehicles and companions into
consideration. With high-quality generated images, a Densely Associated
2-Stream (DA-2S) network is introduced with Inter Stream Densely Connection
(ISDC) modules to strengthen the complementarity of the generated data and
ID-related cues. The experiments show that the proposed method achieves
competitive performance on three re-ID datasets, ie., Market-1501,
DukeMTMC-reID, and CUHK03, under the cross-domain person re-ID scenario.",['cs.CV'],186,130
Learning values across many orders of magnitude,"Most learning algorithms are not invariant to the scale of the function that
is being approximated. We propose to adaptively normalize the targets used in
learning. This is useful in value-based reinforcement learning, where the
magnitude of appropriate value approximations can change over time when we
update the policy of behavior. Our main motivation is prior work on learning to
play Atari games, where the rewards were all clipped to a predetermined range.
This clipping facilitates learning across many different games with a single
learning algorithm, but a clipped reward function can result in qualitatively
different behavior. Using the adaptive normalization we can remove this
domain-specific heuristic without diminishing overall performance.","['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",113,80
Learning color space adaptation from synthetic to real images of cirrus clouds,"Cloud segmentation plays a crucial role in image analysis for climate
modeling. Manually labeling the training data for cloud segmentation is
time-consuming and error-prone. We explore to train segmentation networks with
synthetic data due to the natural acquisition of pixel-level labels.
Nevertheless, the domain gap between synthetic and real images significantly
degrades the performance of the trained model. We propose a color space
adaptation method to bridge the gap, by training a color-sensitive generator
and discriminator to adapt synthetic data to real images in color space.
Instead of transforming images by general convolutional kernels, we adopt a set
of closed-form operations to make color-space adjustments while preserving the
labels. We also construct a synthetic-to-real cirrus cloud dataset SynCloud and
demonstrate the adaptation efficacy on the semantic segmentation task of cirrus
clouds. With our adapted synthetic data for training the semantic segmentation,
we achieve an improvement of 6:59% when applied to real images, superior to
alternative methods.",['cs.CV'],165,99
SentiMATE: Learning to play Chess through Natural Language Processing,"We present SentiMATE, a novel end-to-end Deep Learning model for Chess,
employing Natural Language Processing that aims to learn an effective
evaluation function assessing move quality. This function is pre-trained on the
sentiment of commentary associated with the training moves and is used to guide
and optimize the agent's game-playing decision making. The contributions of
this research are three-fold: we build and put forward both a classifier which
extracts commentary describing the quality of Chess moves in vast commentary
datasets, and a Sentiment Analysis model trained on Chess commentary to
accurately predict the quality of said moves, to then use those predictions to
evaluate the optimal next move of a Chess agent. Both classifiers achieve over
90 % classification accuracy. Lastly, we present a Chess engine, SentiMATE,
which evaluates Chess moves based on a pre-trained sentiment evaluation
function. Our results exhibit strong evidence to support our initial hypothesis
- ""Can Natural Language Processing be used to train a novel and sample
efficient evaluation function in Chess Engines?"" - as we integrate our
evaluation function into modern Chess engines and play against agents with
traditional Chess move evaluation functions, beating both random agents and a
DeepChess implementation at a level-one search depth - representing the number
of moves a traditional Chess agent (employing the alpha-beta search algorithm)
looks ahead in order to evaluate a given chess state.","['cs.LG', 'cs.AI', 'cs.CL']",231,136
Reinforcement Learning-based Joint Path and Energy Optimization of Cellular-Connected Unmanned Aerial Vehicles,"Unmanned Aerial Vehicles (UAVs) have attracted considerable research interest
recently. Especially when it comes to the realm of Internet of Things, the UAVs
with Internet connectivity are one of the main demands. Furthermore, the energy
constraint i.e. battery limit is a bottle-neck of the UAVs that can limit their
applications. We try to address and solve the energy problem. Therefore, a path
planning method for a cellular-connected UAV is proposed that will enable the
UAV to plan its path in an area much larger than its battery range by getting
recharged in certain positions equipped with power stations (PSs). In addition
to the energy constraint, there are also no-fly zones; for example, due to Air
to Air (A2A) and Air to Ground (A2G) interference or for lack of necessary
connectivity that impose extra constraints in the trajectory optimization of
the UAV. No-fly zones determine the infeasible areas that should be avoided. We
have used a reinforcement learning (RL) hierarchically to extend typical
short-range path planners to consider battery recharge and solve the problem of
UAVs in long missions. The problem is simulated for the UAV that flies over a
large area, and Q-learning algorithm could enable the UAV to find the optimal
path and recharge policy.","['cs.LG', 'eess.SP']",213,129
Friendly Training: Neural Networks Can Adapt Data To Make Learning Easier,"In the last decade, motivated by the success of Deep Learning, the scientific
community proposed several approaches to make the learning procedure of Neural
Networks more effective. When focussing on the way in which the training data
are provided to the learning machine, we can distinguish between the classic
random selection of stochastic gradient-based optimization and more involved
techniques that devise curricula to organize data, and progressively increase
the complexity of the training set. In this paper, we propose a novel training
procedure named Friendly Training that, differently from the aforementioned
approaches, involves altering the training examples in order to help the model
to better fulfil its learning criterion. The model is allowed to simplify those
examples that are too hard to be classified at a certain stage of the training
procedure. The data transformation is controlled by a developmental plan that
progressively reduces its impact during training, until it completely vanishes.
In a sense, this is the opposite of what is commonly done in order to increase
robustness against adversarial examples, i.e., Adversarial Training.
Experiments on multiple datasets are provided, showing that Friendly Training
yields improvements with respect to informed data sub-selection routines and
random selection, especially in deep convolutional architectures. Results
suggest that adapting the input data is a feasible way to stabilize learning
and improve the generalization skills of the network.",['cs.LG'],227,136
Unsupervised Learning of Object Landmarks through Conditional Image Generation,"We propose a method for learning landmark detectors for visual objects (such
as the eyes and the nose in a face) without any manual supervision. We cast
this as the problem of generating images that combine the appearance of the
object as seen in a first example image with the geometry of the object as seen
in a second example image, where the two examples differ by a viewpoint change
and/or an object deformation. In order to factorize appearance and geometry, we
introduce a tight bottleneck in the geometry-extraction process that selects
and distils geometry-related features. Compared to standard image generation
problems, which often use generative adversarial networks, our generation task
is conditioned on both appearance and geometry and thus is significantly less
ambiguous, to the point that adopting a simple perceptual loss formulation is
sufficient. We demonstrate that our approach can learn object landmarks from
synthetic image deformations or videos, all without manual supervision, while
outperforming state-of-the-art unsupervised landmark detectors. We further show
that our method is applicable to a large variety of datasets - faces, people,
3D objects, and digits - without any modifications.",['cs.CV'],190,116
InfoMax-GAN: Improved Adversarial Image Generation via Information Maximization and Contrastive Learning,"While Generative Adversarial Networks (GANs) are fundamental to many
generative modelling applications, they suffer from numerous issues. In this
work, we propose a principled framework to simultaneously mitigate two
fundamental issues in GANs: catastrophic forgetting of the discriminator and
mode collapse of the generator. We achieve this by employing for GANs a
contrastive learning and mutual information maximization approach, and perform
extensive analyses to understand sources of improvements. Our approach
significantly stabilizes GAN training and improves GAN performance for image
synthesis across five datasets under the same training and evaluation
conditions against state-of-the-art works. In particular, compared to the
state-of-the-art SSGAN, our approach does not suffer from poorer performance on
image domains such as faces, and instead improves performance significantly.
Our approach is simple to implement and practical: it involves only one
auxiliary objective, has a low computational cost, and performs robustly across
a wide range of training settings and datasets without any hyperparameter
tuning. For reproducibility, our code is available in Mimicry:
https://github.com/kwotsin/mimicry.","['cs.LG', 'cs.CV', 'stat.ML']",174,121
Spatial Transformer for 3D Point Clouds,"Deep neural networks are widely used for understanding 3D point clouds. At
each point convolution layer, features are computed from local neighborhoods of
3D points and combined for subsequent processing in order to extract semantic
information. Existing methods adopt the same individual point neighborhoods
throughout the network layers, defined by the same metric on the fixed input
point coordinates. This common practice is easy to implement but not
necessarily optimal. Ideally, local neighborhoods should be different at
different layers, as more latent information is extracted at deeper layers. We
propose a novel end-to-end approach to learn different non-rigid
transformations of the input point cloud so that optimal local neighborhoods
can be adopted at each layer. We propose both linear (affine) and non-linear
(projective and deformable) spatial transformers for 3D point clouds. With
spatial transformers on the ShapeNet part segmentation dataset, the network
achieves higher accuracy for all categories, with 8\% gain on earphones and
rockets in particular. Our method also outperforms the state-of-the-art on
other point cloud tasks such as classification, detection, and semantic
segmentation. Visualizations show that spatial transformers can learn features
more efficiently by dynamically altering local neighborhoods according to the
geometry and semantics of 3D shapes in spite of their within-category
variations. Our code is publicly available at
https://github.com/samaonline/spatial-transformer-for-3d-point-clouds.",['cs.CV'],229,140
Illumination Estimation Challenge: experience of past two years,"Illumination estimation is the essential step of computational color
constancy, one of the core parts of various image processing pipelines of
modern digital cameras. Having an accurate and reliable illumination estimation
is important for reducing the illumination influence on the image colors. To
motivate the generation of new ideas and the development of new algorithms in
this field, the 2nd Illumination estimation challenge~(IEC\#2) was conducted.
The main advantage of testing a method on a challenge over testing in on some
of the known datasets is the fact that the ground-truth illuminations for the
challenge test images are unknown up until the results have been submitted,
which prevents any potential hyperparameter tuning that may be biased.
  The challenge had several tracks: general, indoor, and two-illuminant with
each of them focusing on different parameters of the scenes. Other main
features of it are a new large dataset of images (about 5000) taken with the
same camera sensor model, a manual markup accompanying each image, diverse
content with scenes taken in numerous countries under a huge variety of
illuminations extracted by using the SpyderCube calibration object, and a
contest-like markup for the images from the Cube+ dataset that was used in
IEC\#1.
  This paper focuses on the description of the past two challenges, algorithms
which won in each track, and the conclusions that were drawn based on the
results obtained during the 1st and 2nd challenge that can be useful for
similar future developments.",['cs.CV'],247,150
Disentangling Observed Causal Effects from Latent Confounders using Method of Moments,"Discovering the complete set of causal relations among a group of variables
is a challenging unsupervised learning problem. Often, this challenge is
compounded by the fact that there are latent or hidden confounders. When only
observational data is available, the problem is ill-posed, i.e. the causal
relationships are non-identifiable unless strong modeling assumptions are made.
When interventions are available, we provide guarantees on identifiability and
learnability under mild assumptions. We assume a linear structural equation
model (SEM) with independent latent factors and directed acyclic graph (DAG)
relationships among the observables. Since the latent variable inference is
based on independent component analysis (ICA), we call this model SEM-ICA. We
use the method of moments principle to establish model identifiability. We
develop efficient algorithms based on coupled tensor decomposition with linear
constraints to obtain scalable and guaranteed solutions. Thus, we provide a
principled approach to tackling the joint problem of causal discovery and
latent variable inference.",['cs.LG'],158,102
CI-Net: Contextual Information for Joint Semantic Segmentation and Depth Estimation,"Monocular depth estimation and semantic segmentation are two fundamental
goals of scene understanding. Due to the advantages of task interaction, many
works study the joint task learning algorithm. However, most existing methods
fail to fully leverage the semantic labels, ignoring the provided context
structures and only using them to supervise the prediction of segmentation
split, which limit the performance of both tasks. In this paper, we propose a
network injected with contextual information (CI-Net) to solve the problem.
Specifically, we introduce self-attention block in the encoder to generate
attention map. With supervision from the ideal attention map created by
semantic label, the network is embedded with contextual information so that it
could understand scene better and utilize correlated features to make accurate
prediction. Besides, a feature sharing module is constructed to make the
task-specific features deeply fused and a consistency loss is devised to make
the features mutually guided. We evaluate the proposed CI-Net on the
NYU-Depth-v2 and SUN-RGBD datasets. The experimental results validate that our
proposed CI-Net could effectively improve the accuracy of semantic segmentation
and depth estimation.",['cs.CV'],187,123
Text-to-Image Generation Grounded by Fine-Grained User Attention,"Localized Narratives is a dataset with detailed natural language descriptions
of images paired with mouse traces that provide a sparse, fine-grained visual
grounding for phrases. We propose TReCS, a sequential model that exploits this
grounding to generate images. TReCS uses descriptions to retrieve segmentation
masks and predict object labels aligned with mouse traces. These alignments are
used to select and position masks to generate a fully covered segmentation
canvas; the final image is produced by a segmentation-to-image generator using
this canvas. This multi-step, retrieval-based approach outperforms existing
direct text-to-image generation models on both automatic metrics and human
evaluations: overall, its generated images are more photo-realistic and better
match descriptions.","['cs.CV', 'cs.AI']",117,84
Shape Complexes in Continuous Max-Flow Hierarchical Multi-Labeling Problems,"Although topological considerations amongst multiple labels have been
previously investigated in the context of continuous max-flow image
segmentation, similar investigations have yet to be made about shape
considerations in a general and extendable manner. This paper presents shape
complexes for segmentation, which capture more complex shapes by combining
multiple labels and super-labels constrained by geodesic star convexity. Shape
complexes combine geodesic star convexity constraints with hierarchical label
organization, which together allow for more complex shapes to be represented.
This framework avoids the use of co-ordinate system warping techniques to
convert shape constraints into topological constraints, which may be ambiguous
or ill-defined for certain segmentation problems.",['cs.CV'],109,76
Inspirational Adversarial Image Generation,"The task of image generation started to receive some attention from artists
and designers to inspire them in new creations. However, exploiting the results
of deep generative models such as Generative Adversarial Networks can be long
and tedious given the lack of existing tools. In this work, we propose a simple
strategy to inspire creators with new generations learned from a dataset of
their choice, while providing some control on them. We design a simple
optimization method to find the optimal latent parameters corresponding to the
closest generation to any input inspirational image. Specifically, we allow the
generation given an inspirational image of the user choice by performing
several optimization steps to recover optimal parameters from the model's
latent space. We tested several exploration methods starting with classic
gradient descents to gradient-free optimizers. Many gradient-free optimizers
just need comparisons (better/worse than another image), so that they can even
be used without numerical criterion, without inspirational image, but with only
with human preference. Thus, by iterating on one's preferences we could make
robust Facial Composite or Fashion Generation algorithms. High resolution of
the produced design generations are obtained using progressive growing of GANs.
Our results on four datasets of faces, fashion images, and textures show that
satisfactory images are effectively retrieved in most cases.","['cs.CV', 'cs.LG', 'stat.ML']",218,147
Posterior Sampling for Reinforcement Learning Without Episodes,"This is a brief technical note to clarify some of the issues with applying
the application of the algorithm posterior sampling for reinforcement learning
(PSRL) in environments without fixed episodes. In particular, this paper aims
to:
  - Review some of results which have been proven for finite horizon MDPs
(Osband et al 2013, 2014a, 2014b, 2016) and also for MDPs with finite ergodic
structure (Gopalan et al 2014).
  - Review similar results for optimistic algorithms in infinite horizon
problems (Jaksch et al 2010, Bartlett and Tewari 2009, Abbasi-Yadkori and
Szepesvari 2011), with particular attention to the dynamic episode growth.
  - Highlight the delicate technical issue which has led to a fault in the
proof of the lazy-PSRL algorithm (Abbasi-Yadkori and Szepesvari 2015). We
present an explicit counterexample to this style of argument. Therefore, we
suggest that the Theorem 2 in (Abbasi-Yadkori and Szepesvari 2015) be instead
considered a conjecture, as it has no rigorous proof.
  - Present pragmatic approaches to apply PSRL in infinite horizon problems. We
conjecture that, under some additional assumptions, it will be possible to
obtain bounds $O( \sqrt{T} )$ even without episodic reset.
  We hope that this note serves to clarify existing results in the field of
reinforcement learning and provides interesting motivation for future work.","['stat.ML', 'cs.LG']",210,128
Max-Margin Object Detection,"Most object detection methods operate by applying a binary classifier to
sub-windows of an image, followed by a non-maximum suppression step where
detections on overlapping sub-windows are removed. Since the number of possible
sub-windows in even moderately sized image datasets is extremely large, the
classifier is typically learned from only a subset of the windows. This avoids
the computational difficulty of dealing with the entire set of sub-windows,
however, as we will show in this paper, it leads to sub-optimal detector
performance.
  In particular, the main contribution of this paper is the introduction of a
new method, Max-Margin Object Detection (MMOD), for learning to detect objects
in images. This method does not perform any sub-sampling, but instead optimizes
over all sub-windows. MMOD can be used to improve any object detection method
which is linear in the learned parameters, such as HOG or bag-of-visual-word
models. Using this approach we show substantial performance gains on three
publicly available datasets. Strikingly, we show that a single rigid HOG filter
can outperform a state-of-the-art deformable part model on the Face Detection
Data Set and Benchmark when the HOG filter is learned via MMOD.",['cs.CV'],204,130
Hyper RPCA: Joint Maximum Correntropy Criterion and Laplacian Scale Mixture Modeling On-the-Fly for Moving Object Detection,"Moving object detection is critical for automated video analysis in many
vision-related tasks, such as surveillance tracking, video compression coding,
etc. Robust Principal Component Analysis (RPCA), as one of the most popular
moving object modelling methods, aims to separate the temporally varying (i.e.,
moving) foreground objects from the static background in video, assuming the
background frames to be low-rank while the foreground to be spatially sparse.
Classic RPCA imposes sparsity of the foreground component using l1-norm, and
minimizes the modeling error via 2-norm. We show that such assumptions can be
too restrictive in practice, which limits the effectiveness of the classic
RPCA, especially when processing videos with dynamic background, camera jitter,
camouflaged moving object, etc. In this paper, we propose a novel RPCA-based
model, called Hyper RPCA, to detect moving objects on the fly. Different from
classic RPCA, the proposed Hyper RPCA jointly applies the maximum correntropy
criterion (MCC) for the modeling error, and Laplacian scale mixture (LSM) model
for foreground objects. Extensive experiments have been conducted, and the
results demonstrate that the proposed Hyper RPCA has competitive performance
for foreground detection to the state-of-the-art algorithms on several
well-known benchmark datasets.",['cs.CV'],201,132
Coordinate Attention for Efficient Mobile Network Design,"Recent studies on mobile network design have demonstrated the remarkable
effectiveness of channel attention (e.g., the Squeeze-and-Excitation attention)
for lifting model performance, but they generally neglect the positional
information, which is important for generating spatially selective attention
maps. In this paper, we propose a novel attention mechanism for mobile networks
by embedding positional information into channel attention, which we call
""coordinate attention"". Unlike channel attention that transforms a feature
tensor to a single feature vector via 2D global pooling, the coordinate
attention factorizes channel attention into two 1D feature encoding processes
that aggregate features along the two spatial directions, respectively. In this
way, long-range dependencies can be captured along one spatial direction and
meanwhile precise positional information can be preserved along the other
spatial direction. The resulting feature maps are then encoded separately into
a pair of direction-aware and position-sensitive attention maps that can be
complementarily applied to the input feature map to augment the representations
of the objects of interest. Our coordinate attention is simple and can be
flexibly plugged into classic mobile networks, such as MobileNetV2, MobileNeXt,
and EfficientNet with nearly no computational overhead. Extensive experiments
demonstrate that our coordinate attention is not only beneficial to ImageNet
classification but more interestingly, behaves better in down-stream tasks,
such as object detection and semantic segmentation. Code is available at
https://github.com/Andrew-Qibin/CoordAttention.",['cs.CV'],232,149
Sampling Techniques in Bayesian Target Encoding,"Target encoding is an effective encoding technique of categorical variables
and is often used in machine learning systems for processing tabular data sets
with mixed numeric and categorical variables. Recently en enhanced version of
this encoding technique was proposed by using conjugate Bayesian modeling. This
paper presents a further development of Bayesian encoding method by using
sampling techniques, which helps in extracting information from intra-category
distribution of the target variable, improves generalization and reduces target
leakage.","['cs.LG', 'cs.DM', 'stat.ML']",77,60
Non-Local Graph Neural Networks,"Modern graph neural networks (GNNs) learn node embeddings through multilayer
local aggregation and achieve great success in applications on assortative
graphs. However, tasks on disassortative graphs usually require non-local
aggregation. In this work, we propose a simple yet effective non-local
aggregation framework with an efficient attention-guided sorting for GNNs.
Based on it, we develop various non-local GNNs. We perform thorough experiments
to analyze disassortative graph datasets and evaluate our non-local GNNs.
Experimental results demonstrate that our non-local GNNs significantly
outperform previous state-of-the-art methods on six benchmark datasets of
disassortative graphs, in terms of both model performance and efficiency.","['cs.LG', 'stat.ML']",107,76
Optimism in Reinforcement Learning with Generalized Linear Function Approximation,"We design a new provably efficient algorithm for episodic reinforcement
learning with generalized linear function approximation. We analyze the
algorithm under a new expressivity assumption that we call ""optimistic
closure,"" which is strictly weaker than assumptions from prior analyses for the
linear setting. With optimistic closure, we prove that our algorithm enjoys a
regret bound of $\tilde{O}(\sqrt{d^3 T})$ where $d$ is the dimensionality of
the state-action features and $T$ is the number of episodes. This is the first
statistically and computationally efficient algorithm for reinforcement
learning with generalized linear functions.","['stat.ML', 'cs.LG']",95,62
Online Pricing with Offline Data: Phase Transition and Inverse Square Law,"This paper investigates the impact of pre-existing offline data on online
learning, in the context of dynamic pricing. We study a single-product dynamic
pricing problem over a selling horizon of $T$ periods. The demand in each
period is determined by the price of the product according to a linear demand
model with unknown parameters. We assume that before the start of the selling
horizon, the seller already has some pre-existing offline data. The offline
data set contains $n$ samples, each of which is an input-output pair consisting
of a historical price and an associated demand observation. The seller wants to
utilize both the pre-existing offline data and the sequential online data to
minimize the regret of the online learning process.
  We characterize the joint effect of the size, location and dispersion of the
offline data on the optimal regret of the online learning process.
Specifically, the size, location and dispersion of the offline data are
measured by the number of historical samples $n$, the absolute difference
between the average historical price and the optimal price $\delta$, and the
standard deviation of the historical prices $\sigma$, respectively. We show
that the optimal regret is $\widetilde \Theta\left(\sqrt{T}\wedge
\frac{T}{(n\wedge T)\delta^2+n\sigma^2}\right)$, and design a learning
algorithm based on the ""optimism in the face of uncertainty"" principle, whose
regret is optimal up to a logarithmic factor. Our results reveal surprising
transformations of the optimal regret rate with respect to the size of the
offline data, which we refer to as phase transitions. In addition, our results
demonstrate that the location and dispersion of the offline data also have an
intrinsic effect on the optimal regret, and we quantify this effect via the
inverse-square law.","['cs.LG', 'stat.ML']",298,138
Video Surveillance for Road Traffic Monitoring,"This paper presents the learned techniques during the Video Analysis Module
of the Master in Computer Vision from the Universitat Aut\`onoma de Barcelona,
used to solve the third track of the AI-City Challenge. This challenge aims to
track vehicles across multiple cameras placed in multiple intersections spread
out over a city. The methodology followed focuses first in solving
multi-tracking in a single camera and then extending it to multiple cameras.
The qualitative results of the implemented techniques are presented using
standard metrics for video analysis such as mAP for object detection and IDF1
for tracking. The source code is publicly available at:
https://github.com/mcv-m6-video/mcv-m6-2021-team4.",['cs.CV'],115,86
From Principal Subspaces to Principal Components with Linear Autoencoders,"The autoencoder is an effective unsupervised learning model which is widely
used in deep learning. It is well known that an autoencoder with a single
fully-connected hidden layer, a linear activation function and a squared error
cost function trains weights that span the same subspace as the one spanned by
the principal component loading vectors, but that they are not identical to the
loading vectors. In this paper, we show how to recover the loading vectors from
the autoencoder weights.","['stat.ML', 'cs.LG']",81,59
SGAS: Sequential Greedy Architecture Search,"Architecture design has become a crucial component of successful deep
learning. Recent progress in automatic neural architecture search (NAS) shows a
lot of promise. However, discovered architectures often fail to generalize in
the final evaluation. Architectures with a higher validation accuracy during
the search phase may perform worse in the evaluation. Aiming to alleviate this
common issue, we introduce sequential greedy architecture search (SGAS), an
efficient method for neural architecture search. By dividing the search
procedure into sub-problems, SGAS chooses and prunes candidate operations in a
greedy fashion. We apply SGAS to search architectures for Convolutional Neural
Networks (CNN) and Graph Convolutional Networks (GCN). Extensive experiments
show that SGAS is able to find state-of-the-art architectures for tasks such as
image classification, point cloud classification and node classification in
protein-protein interaction graphs with minimal computational cost. Please
visit https://www.deepgcns.org/auto/sgas for more information about SGAS.","['cs.LG', 'cs.CV', 'stat.ML']",153,110
Interpreting Super-Resolution Networks with Local Attribution Maps,"Image super-resolution (SR) techniques have been developing rapidly,
benefiting from the invention of deep networks and its successive
breakthroughs. However, it is acknowledged that deep learning and deep neural
networks are difficult to interpret. SR networks inherit this mysterious nature
and little works make attempt to understand them. In this paper, we perform
attribution analysis of SR networks, which aims at finding the input pixels
that strongly influence the SR results. We propose a novel attribution approach
called local attribution map (LAM), which inherits the integral gradient method
yet with two unique features. One is to use the blurred image as the baseline
input, and the other is to adopt the progressive blurring function as the path
function. Based on LAM, we show that: (1) SR networks with a wider range of
involved input pixels could achieve better performance. (2) Attention networks
and non-local networks extract features from a wider range of input pixels. (3)
Comparing with the range that actually contributes, the receptive field is
large enough for most deep networks. (4) For SR networks, textures with regular
stripes or grids are more likely to be noticed, while complex semantics are
difficult to utilize. Our work opens new directions for designing SR networks
and interpreting low-level vision deep models.",['cs.CV'],213,137
SALD: Sign Agnostic Learning with Derivatives,"Learning 3D geometry directly from raw data, such as point clouds, triangle
soups, or unoriented meshes is still a challenging task that feeds many
downstream computer vision and graphics applications.
  In this paper, we introduce SALD: a method for learning implicit neural
representations of shapes directly from raw data. We generalize sign agnostic
learning (SAL) to include derivatives: given an unsigned distance function to
the input raw data, we advocate a novel sign agnostic regression loss,
incorporating both pointwise values and gradients of the unsigned distance
function. Optimizing this loss leads to a signed implicit function solution,
the zero level set of which is a high quality and valid manifold approximation
to the input 3D data. The motivation behind SALD is that incorporating
derivatives in a regression loss leads to a lower sample complexity, and
consequently better fitting. In addition, we prove that SAL enjoys a minimal
length property in 2D, favoring minimal length solutions. More importantly, we
are able to show that this property still holds for SALD, i.e., with
derivatives included.
  We demonstrate the efficacy of SALD for shape space learning on two
challenging datasets: ShapeNet that contains inconsistent orientation and
non-manifold meshes, and D-Faust that contains raw 3D scans (triangle soups).
On both these datasets, we present state-of-the-art results.","['cs.CV', 'cs.GR', 'cs.LG']",218,131
Distributed Real-Time Sentiment Analysis for Big Data Social Streams,"Big data trend has enforced the data-centric systems to have continuous fast
data streams. In recent years, real-time analytics on stream data has formed
into a new research field, which aims to answer queries about
what-is-happening-now with a negligible delay. The real challenge with
real-time stream data processing is that it is impossible to store instances of
data, and therefore online analytical algorithms are utilized. To perform
real-time analytics, pre-processing of data should be performed in a way that
only a short summary of stream is stored in main memory. In addition, due to
high speed of arrival, average processing time for each instance of data should
be in such a way that incoming instances are not lost without being captured.
Lastly, the learner needs to provide high analytical accuracy measures.
Sentinel is a distributed system written in Java that aims to solve this
challenge by enforcing both the processing and learning process to be done in
distributed form. Sentinel is built on top of Apache Storm, a distributed
computing platform. Sentinels learner, Vertical Hoeffding Tree, is a parallel
decision tree-learning algorithm based on the VFDT, with ability of enabling
parallel classification in distributed environments. Sentinel also uses
SpaceSaving to keep a summary of the data stream and stores its summary in a
synopsis data structure. Application of Sentinel on Twitter Public Stream API
is shown and the results are discussed.","['stat.ML', 'cs.CL', 'cs.DB', 'cs.DC', 'cs.IR']",240,141
FFNet: Video Fast-Forwarding via Reinforcement Learning,"For many applications with limited computation, communication, storage and
energy resources, there is an imperative need of computer vision methods that
could select an informative subset of the input video for efficient processing
at or near real time. In the literature, there are two relevant groups of
approaches: generating a trailer for a video or fast-forwarding while
watching/processing the video. The first group is supported by video
summarization techniques, which require processing of the entire video to
select an important subset for showing to users. In the second group, current
fast-forwarding methods depend on either manual control or automatic adaptation
of playback speed, which often do not present an accurate representation and
may still require processing of every frame. In this paper, we introduce
FastForwardNet (FFNet), a reinforcement learning agent that gets inspiration
from video summarization and does fast-forwarding differently. It is an online
framework that automatically fast-forwards a video and presents a
representative subset of frames to users on the fly. It does not require
processing the entire video, but just the portion that is selected by the
fast-forward agent, which makes the process very computationally efficient. The
online nature of our proposed method also enables the users to begin
fast-forwarding at any point of the video. Experiments on two real-world
datasets demonstrate that our method can provide better representation of the
input video with much less processing requirement.",['cs.CV'],238,136
Interpretability and Explainability: A Machine Learning Zoo Mini-tour,"In this review, we examine the problem of designing interpretable and
explainable machine learning models. Interpretability and explainability lie at
the core of many machine learning and statistical applications in medicine,
economics, law, and natural sciences. Although interpretability and
explainability have escaped a clear universal definition, many techniques
motivated by these properties have been developed over the recent 30 years with
the focus currently shifting towards deep learning methods. In this review, we
emphasise the divide between interpretability and explainability and illustrate
these two different research directions with concrete examples of the
state-of-the-art. The review is intended for a general machine learning
audience with interest in exploring the problems of interpretation and
explanation beyond logistic regression or random forest variable importance.
This work is not an exhaustive literature survey, but rather a primer focusing
selectively on certain lines of research which the authors found interesting or
informative.",['cs.LG'],150,106
SmartChoices: Hybridizing Programming and Machine Learning,"We present SmartChoices, an approach to making machine learning (ML) a first
class citizen in programming languages which we see as one way to lower the
entrance cost to applying ML to problems in new domains. There is a growing
divide in approaches to building systems: on the one hand, programming
leverages human experts to define a system while on the other hand behavior is
learned from data in machine learning. We propose to hybridize these two by
providing a 3-call API which we expose through an object called SmartChoice. We
describe the SmartChoices-interface, how it can be used in programming with
minimal code changes, and demonstrate that it is an easy to use but still
powerful tool by demonstrating improvements over not using ML at all on three
algorithmic problems: binary search, QuickSort, and caches. In these three
examples, we replace the commonly used heuristics with an ML model entirely
encapsulated within a SmartChoice and thus requiring minimal code changes. As
opposed to previous work applying ML to algorithmic problems, our proposed
approach does not require to drop existing implementations but seamlessly
integrates into the standard software development workflow and gives full
control to the software developer over how ML methods are applied. Our
implementation relies on standard Reinforcement Learning (RL) methods. To learn
faster, we use the heuristic function, which they are replacing, as an initial
function. We show how this initial function can be used to speed up and
stabilize learning while providing a safety net that prevents performance to
become substantially worse -- allowing for a safe deployment in critical
applications in real life.","['cs.LG', 'cs.PL', 'stat.ML']",269,168
Causal Inference in Non-linear Time-series usingDeep Networks and Knockoff Counterfactuals,"Estimating causal relations is vital in understanding the complex
interactions in multivariate time series. Non-linear coupling of variables is
one of the major challenges inaccurate estimation of cause-effect relations. In
this paper, we propose to use deep autoregressive networks (DeepAR) in tandem
with counterfactual analysis to infer nonlinear causal relations in
multivariate time series. We extend the concept of Granger causality using
probabilistic forecasting with DeepAR. Since deep networks can neither handle
missing input nor out-of-distribution intervention, we propose to use the
Knockoffs framework (Barberand Cand`es, 2015) for generating intervention
variables and consequently counterfactual probabilistic forecasting. Knockoff
samples are independent of their output given the observed variables and
exchangeable with their counterpart variables without changing the underlying
distribution of the data. We test our method on synthetic as well as real-world
time series datasets. Overall our method outperforms the widely used vector
autoregressive Granger causality and PCMCI in detecting nonlinear causal
dependency in multivariate time series.","['cs.LG', 'cs.AI']",162,102
Graph Pooling via Coarsened Graph Infomax,"Graph pooling that summaries the information in a large graph into a compact
form is essential in hierarchical graph representation learning. Existing graph
pooling methods either suffer from high computational complexity or cannot
capture the global dependencies between graphs before and after pooling. To
address the problems of existing graph pooling methods, we propose Coarsened
Graph Infomax Pooling (CGIPool) that maximizes the mutual information between
the input and the coarsened graph of each pooling layer to preserve graph-level
dependencies. To achieve mutual information neural maximization, we apply
contrastive learning and propose a self-attention-based algorithm for learning
positive and negative samples. Extensive experimental results on seven datasets
illustrate the superiority of CGIPool comparing to the state-of-the-art
methods.","['cs.LG', 'cs.AI']",122,80
Image-Conditioned Graph Generation for Road Network Extraction,"Deep generative models for graphs have shown great promise in the area of
drug design, but have so far found little application beyond generating
graph-structured molecules. In this work, we demonstrate a proof of concept for
the challenging task of road network extraction from image data. This task can
be framed as image-conditioned graph generation, for which we develop the
Generative Graph Transformer (GGT), a deep autoregressive model that makes use
of attention mechanisms for image conditioning and the recurrent generation of
graphs. We benchmark GGT on the application of road network extraction from
semantic segmentation data. For this, we introduce the Toulouse Road Network
dataset, based on real-world publicly-available data. We further propose the
StreetMover distance: a metric based on the Sinkhorn distance for effectively
evaluating the quality of road network generation. The code and dataset are
publicly available.","['cs.LG', 'stat.ML']",144,94
DisTop: Discovering a Topological representation to learn diverse and rewarding skills,"The optimal way for a deep reinforcement learning (DRL) agent to explore is
to learn a set of skills that achieves a uniform distribution of states.
Following this,we introduce DisTop, a new model that simultaneously learns
diverse skills and focuses on improving rewarding skills. DisTop progressively
builds a discrete topology of the environment using an unsupervised contrastive
loss, a growing network and a goal-conditioned policy. Using this topology, a
state-independent hierarchical policy can select where the agent has to keep
discovering skills in the state space. In turn, the newly visited states allows
an improved learnt representation and the learning loop continues. Our
experiments emphasize that DisTop is agnostic to the ground state
representation and that the agent can discover the topology of its environment
whether the states are high-dimensional binary data, images, or proprioceptive
inputs. We demonstrate that this paradigm is competitiveon MuJoCo benchmarks
with state-of-the-art algorithms on both single-task dense rewards and diverse
skill discovery. By combining these two aspects, we showthat DisTop achieves
state-of-the-art performance in comparison with hierarchical reinforcement
learning (HRL) when rewards are sparse. We believe DisTop opens new
perspectives by showing that bottom-up skill discovery combined with
representation learning can unlock the exploration challenge in DRL.",['cs.LG'],214,130
An information-theoretic derivation of min-cut based clustering,"Min-cut clustering, based on minimizing one of two heuristic cost-functions
proposed by Shi and Malik, has spawned tremendous research, both analytic and
algorithmic, in the graph partitioning and image segmentation communities over
the last decade. It is however unclear if these heuristics can be derived from
a more general principle facilitating generalization to new problem settings.
Motivated by an existing graph partitioning framework, we derive relationships
between optimizing relevance information, as defined in the Information
Bottleneck method, and the regularized cut in a K-partitioned graph. For fast
mixing graphs, we show that the cost functions introduced by Shi and Malik can
be well approximated as the rate of loss of predictive information about the
location of random walkers on the graph. For graphs generated from a stochastic
algorithm designed to model community structure, the optimal information
theoretic partition and the optimal min-cut partition are shown to be the same
with high probability.",['stat.ML'],156,107
Understanding Adversarial Examples Through Deep Neural Network's Response Surface and Uncertainty Regions,"Deep neural network (DNN) is a popular model implemented in many systems to
handle complex tasks such as image classification, object recognition, natural
language processing etc. Consequently DNN structural vulnerabilities become
part of the security vulnerabilities in those systems. In this paper we study
the root cause of DNN adversarial examples. We examine the DNN response surface
to understand its classification boundary. Our study reveals the structural
problem of DNN classification boundary that leads to the adversarial examples.
Existing attack algorithms can generate from a handful to a few hundred
adversarial examples given one clean image. We show there are infinitely many
adversarial images given one clean sample, all within a small neighborhood of
the clean sample. We then define DNN uncertainty regions and show
transferability of adversarial examples is not universal. We also argue that
generalization error, the large sample theoretical guarantee established for
DNN, cannot adequately capture the phenomenon of adversarial examples. We need
new theory to measure DNN robustness.",['cs.LG'],162,104
Low-Rank Bottleneck in Multi-head Attention Models,"Attention based Transformer architecture has enabled significant advances in
the field of natural language processing. In addition to new pre-training
techniques, recent improvements crucially rely on working with a relatively
larger embedding dimension for tokens. Unfortunately, this leads to models that
are prohibitively large to be employed in the downstream tasks. In this paper
we identify one of the important factors contributing to the large embedding
size requirement. In particular, our analysis highlights that the scaling
between the number of heads and the size of each head in the current
architecture gives rise to a low-rank bottleneck in attention heads, causing
this limitation. We further validate this in our experiments. As a solution we
propose to set the head size of an attention unit to input sequence length, and
independent of the number of heads, resulting in multi-head attention layers
with provably more expressive power. We empirically show that this allows us to
train models with a relatively smaller embedding dimension and with better
performance scaling.","['cs.LG', 'stat.ML']",169,106
Understanding Black-box Predictions via Influence Functions,"How can we explain the predictions of a black-box model? In this paper, we
use influence functions -- a classic technique from robust statistics -- to
trace a model's prediction through the learning algorithm and back to its
training data, thereby identifying training points most responsible for a given
prediction. To scale up influence functions to modern machine learning
settings, we develop a simple, efficient implementation that requires only
oracle access to gradients and Hessian-vector products. We show that even on
non-convex and non-differentiable models where the theory breaks down,
approximations to influence functions can still provide valuable information.
On linear models and convolutional neural networks, we demonstrate that
influence functions are useful for multiple purposes: understanding model
behavior, debugging models, detecting dataset errors, and even creating
visually-indistinguishable training-set attacks.","['stat.ML', 'cs.AI', 'cs.LG']",135,98
Embedding Task Knowledge into 3D Neural Networks via Self-supervised Learning,"Deep learning highly relies on the amount of annotated data. However,
annotating medical images is extremely laborious and expensive. To this end,
self-supervised learning (SSL), as a potential solution for deficient annotated
data, attracts increasing attentions from the community. However, SSL
approaches often design a proxy task that is not necessarily related to target
task. In this paper, we propose a novel SSL approach for 3D medical image
classification, namely Task-related Contrastive Prediction Coding (TCPC), which
embeds task knowledge into training 3D neural networks. The proposed TCPC first
locates the initial candidate lesions via supervoxel estimation using simple
linear iterative clustering. Then, we extract features from the sub-volume
cropped around potential lesion areas, and construct a calibrated contrastive
predictive coding scheme for self-supervised learning. Extensive experiments
are conducted on public and private datasets. The experimental results
demonstrate the effectiveness of embedding lesion-related prior-knowledge into
neural networks for 3D medical image classification.",['cs.CV'],157,112
$S^3$: Learnable Sparse Signal Superdensity for Guided Depth Estimation,"Dense depth estimation plays a key role in multiple applications such as
robotics, 3D reconstruction, and augmented reality. While sparse signal, e.g.,
LiDAR and Radar, has been leveraged as guidance for enhancing dense depth
estimation, the improvement is limited due to its low density and imbalanced
distribution. To maximize the utility from the sparse source, we propose $S^3$
technique, which expands the depth value from sparse cues while estimating the
confidence of expanded region. The proposed $S^3$ can be applied to various
guided depth estimation approaches and trained end-to-end at different stages,
including input, cost volume and output. Extensive experiments demonstrate the
effectiveness, robustness, and flexibility of the $S^3$ technique on LiDAR and
Radar signal.",['cs.CV'],121,88
Peak Detection On Data Independent Acquisition Mass Spectrometry Data With Semisupervised Convolutional Transformers,"Liquid Chromatography coupled to Mass Spectrometry (LC-MS) based methods are
commonly used for high-throughput, quantitative measurements of the proteome
(i.e. the set of all proteins in a sample at a given time). Targeted LC-MS
produces data in the form of a two-dimensional time series spectrum, with the
mass to charge ratio of analytes (m/z) on one axis, and the retention time from
the chromatography on the other. The elution of a peptide of interest produces
highly specific patterns across multiple fragment ion traces (extracted ion
chromatograms, or XICs). In this paper, we formulate this peak detection
problem as a multivariate time series segmentation problem, and propose a novel
approach based on the Transformer architecture. Here we augment Transformers,
which are capable of capturing long distance dependencies with a global view,
with Convolutional Neural Networks (CNNs), which can capture local context
important to the task at hand, in the form of Transformers with Convolutional
Self-Attention. We further train this model in a semisupervised manner by
adapting state of the art semisupervised image classification techniques for
multi-channel time series data. Experiments on a representative LC-MS dataset
are benchmarked using manual annotations to showcase the encouraging
performance of our method; it outperforms baseline neural network architectures
and is competitive against the current state of the art in automated peak
detection.","['cs.CV', 'q-bio.QM']",226,149
Towards real-time object recognition and pose estimation in point clouds,"Object recognition and 6DoF pose estimation are quite challenging tasks in
computer vision applications. Despite efficiency in such tasks, standard
methods deliver far from real-time processing rates. This paper presents a
novel pipeline to estimate a fine 6DoF pose of objects, applied to realistic
scenarios in real-time. We split our proposal into three main parts. Firstly, a
Color feature classification leverages the use of pre-trained CNN color
features trained on the ImageNet for object detection. A Feature-based
registration module conducts a coarse pose estimation, and finally, a
Fine-adjustment step performs an ICP-based dense registration. Our proposal
achieves, in the best case, an accuracy performance of almost 83\% on the RGB-D
Scenes dataset. Regarding processing time, the object detection task is done at
a frame processing rate up to 90 FPS, and the pose estimation at almost 14 FPS
in a full execution strategy. We discuss that due to the proposal's modularity,
we could let the full execution occurs only when necessary and perform a
scheduled execution that unlocks real-time processing, even for multitask
situations.",['cs.CV'],183,124
mSHAP: SHAP Values for Two-Part Models,"Two-part models are important to and used throughout insurance and actuarial
science. Since insurance is required for registering a car, obtaining a
mortgage, and participating in certain businesses, it is especially important
that the models which price insurance policies are fair and non-discriminatory.
Black box models can make it very difficult to know which covariates are
influencing the results. SHAP values enable interpretation of various black box
models, but little progress has been made in two-part models. In this paper, we
propose mSHAP (or multiplicative SHAP), a method for computing SHAP values of
two-part models using the SHAP values of the individual models. This method
will allow for the predictions of two-part models to be explained at an
individual observation level. After developing mSHAP, we perform an in-depth
simulation study. Although the kernelSHAP algorithm is also capable of
computing approximate SHAP values for a two-part model, a comparison with our
method demonstrates that mSHAP is exponentially faster. Ultimately, we apply
mSHAP to a two-part ratemaking model for personal auto property damage
insurance coverage. Additionally, an R package (mshap) is available to easily
implement the method in a wide variety of applications.","['stat.ML', 'cs.LG']",199,118
Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent,"The capacity of neural networks like the widely adopted transformer is known
to be very high. Evidence is emerging that they learn successfully due to
inductive bias in the training routine, typically a variant of gradient descent
(GD). To better understand this bias, we study the tendency for transformer
parameters to grow in magnitude ($\ell_2$ norm) during training, and its
implications for the emergent representations within self attention layers.
Empirically, we document norm growth in the training of transformer language
models, including T5 during its pretraining. As the parameters grow in
magnitude, we prove that the network approximates a discretized network with
saturated activation functions. Such ""saturated"" networks are known to have a
reduced capacity compared to the full network family that can be described in
terms of formal languages and automata. Our results suggest saturation is a new
characterization of an inductive bias implicit in GD of particular interest for
NLP. We leverage the emergent discrete structure in a saturated transformer to
analyze the role of different attention heads, finding that some focus locally
on a small number of positions, while other heads compute global averages,
allowing counting. We believe understanding the interplay between these two
capabilities may shed further light on the structure of computation within
large transformers.","['cs.LG', 'cs.CL']",210,135
Edge Based Grid Super-Imposition for Crowd Emotion Recognition,"Numerous automatic continuous emotion detection system studies have examined
mostly use of videos and images containing individual person expressing
emotions. This study examines the detection of spontaneous emotions in a group
and crowd settings. Edge detection was used with a grid of lines
superimposition to extract the features. The feature movement in terms of
movement from the reference point was used to track across sequences of images
from the color channel. Additionally the video data capturing was done on
spontaneous emotions invoked by watching sports events from group of
participants. The method was view and occlusion independent and the results
were not affected by presence of multiple people chaotically expressing various
emotions. The edge thresholds of 0.2 and grid thresholds of 20 showed the best
accuracy results. The overall accuracy of the group emotion classifier was
70.9%.","['cs.CV', 'cs.HC']",139,89
Spatially varying white balancing for mixed and non-uniform illuminants,"In this paper, we propose a novel white balance adjustment, called ""spatially
varying white balancing,"" for single, mixed, and non-uniform illuminants. By
using n diagonal matrices along with a weight, the proposed method can reduce
lighting effects on all spatially varying colors in an image under such
illumination conditions. In contrast, conventional white balance adjustments do
not consider the correcting of all colors except under a single illuminant.
Also, multi-color balance adjustments can map multiple colors into
corresponding ground truth colors, although they may cause the rank deficiency
problem to occur as a non-diagonal matrix is used, unlike white balancing. In
an experiment, the effectiveness of the proposed method is shown under mixed
and non-uniform illuminants, compared with conventional white and multi-color
balancing. Moreover, under a single illuminant, the proposed method has almost
the same performance as the conventional white balancing.",['cs.CV'],146,88
Representation Learning using Graph Autoencoders with Residual Connections,"Graph autoencoders are very efficient at embedding graph-based complex data
sets. However, most of the autoencoders have shallow depths and their
efficiency tends to decrease with the increase of layer depth. In this paper,
we study the effect of adding residual connections to shallow and deep graph
variational and vanilla autoencoders. We show that residual connections improve
the accuracy of the deep graph-based autoencoders. Furthermore, we propose
Res-VGAE, a graph variational autoencoder with different residual connections.
Our experiments show that our model achieves superior results when compared
with other autoencoder-based models for the link prediction task.","['cs.LG', 'cs.SI']",100,68
Learning to Play General Video-Games via an Object Embedding Network,"Deep reinforcement learning (DRL) has proven to be an effective tool for
creating general video-game AI. However most current DRL video-game agents
learn end-to-end from the video-output of the game, which is superfluous for
many applications and creates a number of additional problems. More
importantly, directly working on pixel-based raw video data is substantially
distinct from what a human player does.In this paper, we present a novel method
which enables DRL agents to learn directly from object information. This is
obtained via use of an object embedding network (OEN) that compresses a set of
object feature vectors of different lengths into a single fixed-length unified
feature vector representing the current game-state and fulfills the DRL
simultaneously. We evaluate our OEN-based DRL agent by comparing to several
state-of-the-art approaches on a selection of games from the GVG-AI
Competition. Experimental results suggest that our object-based DRL agent
yields performance comparable to that of those approaches used in our
comparative study.","['cs.LG', 'cs.AI']",173,111
Line Profile Based Segmentation Algorithm for Touching Corn Kernels,"Image segmentation of touching objects plays a key role in providing accurate
classification for computer vision technologies. A new line profile based
imaging segmentation algorithm has been developed to provide a robust and
accurate segmentation of a group of touching corns. The performance of the line
profile based algorithm has been compared to a watershed based imaging
segmentation algorithm. Both algorithms are tested on three different patterns
of images, which are isolated corns, single-lines, and random distributed
formations. The experimental results show that the algorithm can segment a
large number of touching corn kernels efficiently and accurately.",['cs.CV'],98,66
Equivariant Wavelets: Fast Rotation and Translation Invariant Wavelet Scattering Transforms,"Wavelet scattering networks, which are convolutional neural networks (CNNs)
with fixed filters and weights, are promising tools for image analysis.
Imposing symmetry on image statistics can improve human interpretability, aid
in generalization, and provide dimension reduction. In this work, we introduce
a fast-to-compute, translationally invariant and rotationally equivariant
wavelet scattering network (EqWS) and filter bank of wavelets (triglets). We
demonstrate the interpretability and quantify the invariance/equivariance of
the coefficients, briefly commenting on difficulties with implementing scale
equivariance. On MNIST, we show that training on a rotationally invariant
reduction of the coefficients maintains rotational invariance when generalized
to test data and visualize residual symmetry breaking terms. Rotation
equivariance is leveraged to estimate the rotation angle of digits and
reconstruct the full rotation dependence of each coefficient from a single
angle. We benchmark EqWS with linear classifiers on EMNIST and CIFAR-10/100,
introducing a new second-order, cross-color channel coupling for the color
images. We conclude by comparing the performance of an isotropic reduction of
the scattering coefficients and RWST, a previous coefficient reduction, on an
isotropic classification of magnetohydrodynamic simulations with astrophysical
relevance.","['cs.CV', 'astro-ph.IM']",187,123
Unsupervised Parallel Extraction based Texture for Efficient Image Representation,"SOM is a type of unsupervised learning where the goal is to discover some
underlying structure of the data. In this paper, a new extraction method based
on the main idea of Concurrent Self-Organizing Maps (CSOM), representing a
winner-takes-all collection of small SOM networks is proposed. Each SOM of the
system is trained individually to provide best results for one class only. The
experiments confirm that the proposed features based CSOM is capable to
represent image content better than extracted features based on a single big
SOM and these proposed features improve the final decision of the CAD.
Experiments held on Mammographic Image Analysis Society (MIAS) dataset.",['cs.CV'],110,78
Graph Filtration Learning,"We propose an approach to learning with graph-structured data in the problem
domain of graph classification. In particular, we present a novel type of
readout operation to aggregate node features into a graph-level representation.
To this end, we leverage persistent homology computed via a real-valued,
learnable, filter function. We establish the theoretical foundation for
differentiating through the persistent homology computation. Empirically, we
show that this type of readout operation compares favorably to previous
techniques, especially when the graph connectivity structure is informative for
the learning problem.","['cs.LG', 'math.AT', 'stat.ML']",89,64
DeshuffleGAN: A Self-Supervised GAN to Improve Structure Learning,"Generative Adversarial Networks (GANs) triggered an increased interest in
problem of image generation due to their improved output image quality and
versatility for expansion towards new methods. Numerous GAN-based works attempt
to improve generation by architectural and loss-based extensions. We argue that
one of the crucial points to improve the GAN performance in terms of realism
and similarity to the original data distribution is to be able to provide the
model with a capability to learn the spatial structure in data. To that end, we
propose the DeshuffleGAN to enhance the learning of the discriminator and the
generator, via a self-supervision approach. Specifically, we introduce a
deshuffling task that solves a puzzle of randomly shuffled image tiles, which
in turn helps the DeshuffleGAN learn to increase its expressive capacity for
spatial structure and realistic appearance. We provide experimental evidence
for the performance improvement in generated images, compared to the baseline
methods, which is consistently observed over two different datasets.",['cs.CV'],162,105
Shallow Attention Network for Polyp Segmentation,"Accurate polyp segmentation is of great importance for colorectal cancer
diagnosis. However, even with a powerful deep neural network, there still
exists three big challenges that impede the development of polyp segmentation.
(i) Samples collected under different conditions show inconsistent colors,
causing the feature distribution gap and overfitting issue; (ii) Due to
repeated feature downsampling, small polyps are easily degraded; (iii)
Foreground and background pixels are imbalanced, leading to a biased training.
To address the above issues, we propose the Shallow Attention Network (SANet)
for polyp segmentation. Specifically, to eliminate the effects of color, we
design the color exchange operation to decouple the image contents and colors,
and force the model to focus more on the target shape and structure.
Furthermore, to enhance the segmentation quality of small polyps, we propose
the shallow attention module to filter out the background noise of shallow
features. Thanks to the high resolution of shallow features, small polyps can
be preserved correctly. In addition, to ease the severe pixel imbalance for
small polyps, we propose a probability correction strategy (PCS) during the
inference phase. Note that even though PCS is not involved in the training
phase, it can still work well on a biased model and consistently improve the
segmentation performance. Quantitative and qualitative experimental results on
five challenging benchmarks confirm that our proposed SANet outperforms
previous state-of-the-art methods by a large margin and achieves a speed about
72FPS.",['cs.CV'],238,154
Distilling a Powerful Student Model via Online Knowledge Distillation,"Existing online knowledge distillation approaches either adopt the student
with the best performance or construct an ensemble model for better holistic
performance. However, the former strategy ignores other students' information,
while the latter increases the computational complexity. In this paper, we
propose a novel method for online knowledge distillation, termed FFSD, which
comprises two key components: Feature Fusion and Self-Distillation, towards
solving the above problems in a unified framework. Different from previous
works, where all students are treated equally, the proposed FFSD splits them
into a student leader and a common student set. Then, the feature fusion module
converts the concatenation of feature maps from all common students into a
fused feature map. The fused representation is used to assist the learning of
the student leader. To enable the student leader to absorb more diverse
information, we design an enhancement strategy to increase the diversity among
students. Besides, a self-distillation module is adopted to convert the feature
map of deeper layers into a shallower one. Then, the shallower layers are
encouraged to mimic the transformed feature maps of the deeper layers, which
helps the students to generalize better. After training, we simply adopt the
student leader, which achieves superior performance, over the common students,
without increasing the storage or inference cost. Extensive experiments on
CIFAR-100 and ImageNet demonstrate the superiority of our FFSD over existing
works. The code is available at https://github.com/SJLeo/FFSD.",['cs.CV'],239,143
Painting on Placement: Forecasting Routing Congestion using Conditional Generative Adversarial Nets,"Physical design process commonly consumes hours to days for large designs,
and routing is known as the most critical step. Demands for accurate routing
quality prediction raise to a new level to accelerate hardware innovation with
advanced technology nodes. This work presents an approach that forecasts the
density of all routing channels over the entire floorplan, with features
collected up to placement, using conditional GANs. Specifically, forecasting
the routing congestion is constructed as an image translation (colorization)
problem. The proposed approach is applied to a) placement exploration for
minimum congestion, b) constrained placement exploration and c) forecasting
congestion in real-time during incremental placement, using eight designs
targeting a fixed FPGA architecture.","['cs.LG', 'cs.CV']",112,82
Multi-object Tracking with a Hierarchical Single-branch Network,"Recent Multiple Object Tracking (MOT) methods have gradually attempted to
integrate object detection and instance re-identification (Re-ID) into a united
network to form a one-stage solution. Typically, these methods use two
separated branches within a single network to accomplish detection and Re-ID
respectively without studying the inter-relationship between them, which
inevitably impedes the tracking performance. In this paper, we propose an
online multi-object tracking framework based on a hierarchical single-branch
network to solve this problem. Specifically, the proposed single-branch network
utilizes an improved Hierarchical Online In-stance Matching (iHOIM) loss to
explicitly model the inter-relationship between object detection and Re-ID. Our
novel iHOIM loss function unifies the objectives of the two sub-tasks and
encourages better detection performance and feature learning even in extremely
crowded scenes. Moreover, we propose to introduce the object positions,
predicted by a motion model, as region proposals for subsequent object
detection, where the intuition is that detection results and motion predictions
can complement each other in different scenarios. Experimental results on MOT16
and MOT20 datasets show that we can achieve state-of-the-art tracking
performance, and the ablation study verifies the effectiveness of each proposed
component.",['cs.CV'],202,129
Low-Dimensional State and Action Representation Learning with MDP Homomorphism Metrics,"Deep Reinforcement Learning has shown its ability in solving complicated
problems directly from high-dimensional observations. However, in end-to-end
settings, Reinforcement Learning algorithms are not sample-efficient and
requires long training times and quantities of data. In this work, we proposed
a framework for sample-efficient Reinforcement Learning that take advantage of
state and action representations to transform a high-dimensional problem into a
low-dimensional one. Moreover, we seek to find the optimal policy mapping
latent states to latent actions. Because now the policy is learned on abstract
representations, we enforce, using auxiliary loss functions, the lifting of
such policy to the original problem domain. Results show that the novel
framework can efficiently learn low-dimensional and interpretable state and
action representations and the optimal latent policy.","['cs.LG', 'cs.AI']",130,84
Triaging moderate COVID-19 and other viral pneumonias from routine blood tests,"The COVID-19 is sweeping the world with deadly consequences. Its contagious
nature and clinical similarity to other pneumonias make separating subjects
contracted with COVID-19 and non-COVID-19 viral pneumonia a priority and a
challenge. However, COVID-19 testing has been greatly limited by the
availability and cost of existing methods, even in developed countries like the
US. Intrigued by the wide availability of routine blood tests, we propose to
leverage them for COVID-19 testing using the power of machine learning. Two
proven-robust machine learning model families, random forests (RFs) and support
vector machines (SVMs), are employed to tackle the challenge. Trained on blood
data from 208 moderate COVID-19 subjects and 86 subjects with non-COVID-19
moderate viral pneumonia, the best result is obtained in an SVM-based
classifier with an accuracy of 84%, a sensitivity of 88%, a specificity of 80%,
and a precision of 92%. The results are found explainable from both machine
learning and medical perspectives. A privacy-protected web portal is set up to
help medical personnel in their practice and the trained models are released
for developers to further build other applications. We hope our results can
help the world fight this pandemic and welcome clinical verification of our
approach on larger populations.","['cs.LG', 'stat.ML', 'I.5.4']",214,134
Attention-based Multi-Reference Learning for Image Super-Resolution,"This paper proposes a novel Attention-based Multi-Reference Super-resolution
network (AMRSR) that, given a low-resolution image, learns to adaptively
transfer the most similar texture from multiple reference images to the
super-resolution output whilst maintaining spatial coherence. The use of
multiple reference images together with attention-based sampling is
demonstrated to achieve significantly improved performance over
state-of-the-art reference super-resolution approaches on multiple benchmark
datasets. Reference super-resolution approaches have recently been proposed to
overcome the ill-posed problem of image super-resolution by providing
additional information from a high-resolution reference image. Multi-reference
super-resolution extends this approach by providing a more diverse pool of
image features to overcome the inherent information deficit whilst maintaining
memory efficiency. A novel hierarchical attention-based sampling approach is
introduced to learn the similarity between low-resolution image features and
multiple reference images based on a perceptual loss. Ablation demonstrates the
contribution of both multi-reference and hierarchical attention-based sampling
to overall performance. Perceptual and quantitative ground-truth evaluation
demonstrates significant improvement in performance even when the reference
images deviate significantly from the target image. The project website can be
found at https://marcopesavento.github.io/AMRSR/","['cs.CV', 'cs.AI', 'cs.LG']",202,116
Tri-axial Self-Attention for Concurrent Activity Recognition,"We present a system for concurrent activity recognition. To extract features
associated with different activities, we propose a feature-to-activity
attention that maps the extracted global features to sub-features associated
with individual activities. To model the temporal associations of individual
activities, we propose a transformer-network encoder that models independent
temporal associations for each activity. To make the concurrent activity
prediction aware of the potential associations between activities, we propose
self-attention with an association mask. Our system achieved state-of-the-art
or comparable performance on three commonly used concurrent activity detection
datasets. Our visualizations demonstrate that our system is able to locate the
important spatial-temporal features for final decision making. We also showed
that our system can be applied to general multilabel classification problems.","['cs.CV', 'cs.LG']",129,79
"Robust Factorization of Real-world Tensor Streams with Patterns, Missing Values, and Outliers","Consider multiple seasonal time series being collected in real-time, in the
form of a tensor stream. Real-world tensor streams often include missing
entries (e.g., due to network disconnection) and at the same time unexpected
outliers (e.g., due to system errors). Given such a real-world tensor stream,
how can we estimate missing entries and predict future evolution accurately in
real-time? In this work, we answer this question by introducing SOFIA, a robust
factorization method for real-world tensor streams. In a nutshell, SOFIA
smoothly and tightly integrates tensor factorization, outlier removal, and
temporal-pattern detection, which naturally reinforce each other. Moreover,
SOFIA integrates them in linear time, in an online manner, despite the presence
of missing entries. We experimentally show that SOFIA is (a) robust and
accurate: yielding up to 76% lower imputation error and 71% lower forecasting
error; (b) fast: up to 935X faster than the second-most accurate competitor;
and (c) scalable: scaling linearly with the number of new entries per time
step.","['cs.LG', 'cs.DB']",170,110
Amortized Inference Regularization,"The variational autoencoder (VAE) is a popular model for density estimation
and representation learning. Canonically, the variational principle suggests to
prefer an expressive inference model so that the variational approximation is
accurate. However, it is often overlooked that an overly-expressive inference
model can be detrimental to the test set performance of both the amortized
posterior approximator and, more importantly, the generative density estimator.
In this paper, we leverage the fact that VAEs rely on amortized inference and
propose techniques for amortized inference regularization (AIR) that control
the smoothness of the inference model. We demonstrate that, by applying AIR, it
is possible to improve VAE generalization on both inference and generative
performance. Our paper challenges the belief that amortized inference is simply
a mechanism for approximating maximum likelihood training and illustrates that
regularization of the amortization family provides a new direction for
understanding and improving generalization in VAEs.","['stat.ML', 'cs.AI', 'cs.LG']",148,87
Seq-NMS for Video Object Detection,"Video object detection is challenging because objects that are easily
detected in one frame may be difficult to detect in another frame within the
same clip. Recently, there have been major advances for doing object detection
in a single image. These methods typically contain three phases: (i) object
proposal generation (ii) object classification and (iii) post-processing. We
propose a modification of the post-processing phase that uses high-scoring
object detections from nearby frames to boost scores of weaker detections
within the same clip. We show that our method obtains superior results to
state-of-the-art single image object detection techniques. Our method placed
3rd in the video object detection (VID) task of the ImageNet Large Scale Visual
Recognition Challenge 2015 (ILSVRC2015).",['cs.CV'],124,88
Kernel Transform Learning,"This work proposes kernel transform learning. The idea of dictionary learning
is well known; it is a synthesis formulation where a basis is learnt along with
the coefficients so as to generate or synthesize the data. Transform learning
is its analysis equivalent; the transforms operates or analyses on the data to
generate the coefficients. The concept of kernel dictionary learning has been
introduced in the recent past, where the dictionary is represented as a linear
combination of non-linear version of the data. Its success has been showcased
in feature extraction. In this work we propose to kernelize transform learning
on line similar to kernel dictionary learning. An efficient solution for kernel
transform learning has been proposed especially for problems where the number
of samples is much larger than the dimensionality of the input samples making
the kernel matrix very high dimensional. Kernel transform learning has been
compared with other representation learning tools like autoencoder, restricted
Boltzmann machine as well as with dictionary learning (and its kernelized
version). Our proposed kernel transform learning yields better results than all
the aforesaid techniques; experiments have been carried out on benchmark
databases.","['cs.CV', 'cs.LG']",189,107
Learning 3D-aware Egocentric Spatial-Temporal Interaction via Graph Convolutional Networks,"To enable intelligent automated driving systems, a promising strategy is to
understand how human drives and interacts with road users in complicated
driving situations. In this paper, we propose a 3D-aware egocentric
spatial-temporal interaction framework for automated driving applications.
Graph convolution networks (GCN) is devised for interaction modeling. We
introduce three novel concepts into GCN. First, we decompose egocentric
interactions into ego-thing and ego-stuff interaction, modeled by two GCNs. In
both GCNs, ego nodes are introduced to encode the interaction between thing
objects (e.g., car and pedestrian), and interaction between stuff objects
(e.g., lane marking and traffic light). Second, objects' 3D locations are
explicitly incorporated into GCN to better model egocentric interactions.
Third, to implement ego-stuff interaction in GCN, we propose a MaskAlign
operation to extract features for irregular objects.
  We validate the proposed framework on tactical driver behavior recognition.
Extensive experiments are conducted using Honda Research Institute Driving
Dataset, the largest dataset with diverse tactical driver behavior annotations.
Our framework demonstrates substantial performance boost over baselines on the
two experimental settings by 3.9% and 6.0%, respectively. Furthermore, we
visualize the learned affinity matrices, which encode ego-thing and ego-stuff
interactions, to showcase the proposed framework can capture interactions
effectively.","['cs.CV', 'cs.RO']",210,132
Spatial-Temporal Graph ODE Networks for Traffic Flow Forecasting,"Spatial-temporal forecasting has attracted tremendous attention in a wide
range of applications, and traffic flow prediction is a canonical and typical
example. The complex and long-range spatial-temporal correlations of traffic
flow bring it to a most intractable challenge. Existing works typically utilize
shallow graph convolution networks (GNNs) and temporal extracting modules to
model spatial and temporal dependencies respectively. However, the
representation ability of such models is limited due to: (1) shallow GNNs are
incapable to capture long-range spatial correlations, (2) only spatial
connections are considered and a mass of semantic connections are ignored,
which are of great importance for a comprehensive understanding of traffic
networks. To this end, we propose Spatial-Temporal Graph Ordinary Differential
Equation Networks (STGODE). Specifically, we capture spatial-temporal dynamics
through a tensor-based ordinary differential equation (ODE), as a result,
deeper networks can be constructed and spatial-temporal features are utilized
synchronously. To understand the network more comprehensively, semantical
adjacency matrix is considered in our model, and a well-design temporal
dialated convolution structure is used to capture long term temporal
dependencies. We evaluate our model on multiple real-world traffic datasets and
superior performance is achieved over state-of-the-art baselines.",['cs.LG'],202,129
Efficient Visual Pretraining with Contrastive Detection,"Self-supervised pretraining has been shown to yield powerful representations
for transfer learning. These performance gains come at a large computational
cost however, with state-of-the-art methods requiring an order of magnitude
more computation than supervised pretraining. We tackle this computational
bottleneck by introducing a new self-supervised objective, contrastive
detection, which tasks representations with identifying object-level features
across augmentations. This objective extracts a rich learning signal per image,
leading to state-of-the-art transfer accuracy on a variety of downstream tasks,
while requiring up to 10x less pretraining. In particular, our strongest
ImageNet-pretrained model performs on par with SEER, one of the largest
self-supervised systems to date, which uses 1000x more pretraining data.
Finally, our objective seamlessly handles pretraining on more complex images
such as those in COCO, closing the gap with supervised transfer learning from
COCO to PASCAL.",['cs.CV'],146,101
Predictive modeling of brain tumor: A Deep learning approach,"Image processing concepts can visualize the different anatomy structure of
the human body. Recent advancements in the field of deep learning have made it
possible to detect the growth of cancerous tissue just by a patient's brain
Magnetic Resonance Imaging (MRI) scans. These methods require very high
accuracy and meager false negative rates to be of any practical use. This paper
presents a Convolutional Neural Network (CNN) based transfer learning approach
to classify the brain MRI scans into two classes using three pre-trained
models. The performances of these models are compared with each other.
Experimental results show that the Resnet-50 model achieves the highest
accuracy and least false negative rates as 95% and zero respectively. It is
followed by VGG-16 and Inception-V3 model with an accuracy of 90% and 55%
respectively.","['cs.CV', 'cs.LG', 'eess.IV']",136,104
Scalable Power Control/Beamforming in Heterogeneous Wireless Networks with Graph Neural Networks,"Machine learning (ML) has been widely used for efficient resource allocation
(RA) in wireless networks. Although superb performance is achieved on small and
simple networks, most existing ML-based approaches are confronted with
difficulties when heterogeneity occurs and network size expands. In this paper,
specifically focusing on power control/beamforming (PC/BF) in heterogeneous
device-to-device (D2D) networks, we propose a novel unsupervised learning-based
framework named heterogeneous interference graph neural network (HIGNN) to
handle these challenges. First, we characterize diversified link features and
interference relations with heterogeneous graphs. Then, HIGNN is proposed to
empower each link to obtain its individual transmission scheme after limited
information exchange with neighboring links. It is noteworthy that HIGNN is
scalable to wireless networks of growing sizes with robust performance after
trained on small-sized networks. Numerical results show that compared with
state-of-the-art benchmarks, HIGNN achieves much higher execution efficiency
while providing strong performance.","['cs.LG', 'eess.SP']",154,114
DANCin SEQ2SEQ: Fooling Text Classifiers with Adversarial Text Example Generation,"Machine learning models are powerful but fallible. Generating adversarial
examples - inputs deliberately crafted to cause model misclassification or
other errors - can yield important insight into model assumptions and
vulnerabilities. Despite significant recent work on adversarial example
generation targeting image classifiers, relatively little work exists exploring
adversarial example generation for text classifiers; additionally, many
existing adversarial example generation algorithms require full access to
target model parameters, rendering them impractical for many real-world
attacks. In this work, we introduce DANCin SEQ2SEQ, a GAN-inspired algorithm
for adversarial text example generation targeting largely black-box text
classifiers. We recast adversarial text example generation as a reinforcement
learning problem, and demonstrate that our algorithm offers preliminary but
promising steps towards generating semantically meaningful adversarial text
examples in a real-world attack scenario.","['cs.LG', 'cs.CR']",129,91
Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks,"In applied image segmentation tasks, the ability to provide numerous and
precise labels for training is paramount to the accuracy of the model at
inference time. However, this overhead is often neglected, and recently
proposed segmentation architectures rely heavily on the availability and
fidelity of ground truth labels to achieve state-of-the-art accuracies. Failure
to acknowledge the difficulty in creating adequate ground truths can lead to an
over-reliance on pre-trained models or a lack of adoption in real-world
applications. We introduce Points2Polygons (P2P), a model which makes use of
contextual metric learning techniques that directly addresses this problem.
Points2Polygons performs well against existing fully-supervised segmentation
baselines with limited training data, despite using lightweight segmentation
models (U-Net with a ResNet18 backbone) and having access to only weak labels
in the form of object centroids and no pre-training. We demonstrate this on
several different small but non-trivial datasets. We show that metric learning
using contextual data provides key insights for self-supervised tasks in
general, and allow segmentation models to easily generalize across
traditionally label-intensive domains in computer vision.",['cs.CV'],187,128
LatticeNet: Fast Spatio-Temporal Point Cloud Segmentation Using Permutohedral Lattices,"Deep convolutional neural networks (CNNs) have shown outstanding performance
in the task of semantically segmenting images. Applying the same methods on 3D
data still poses challenges due to the heavy memory requirements and the lack
of structured data. Here, we propose LatticeNet, a novel approach for 3D
semantic segmentation, which takes raw point clouds as input. A PointNet
describes the local geometry which we embed into a sparse permutohedral
lattice. The lattice allows for fast convolutions while keeping a low memory
footprint. Further, we introduce DeformSlice, a novel learned data-dependent
interpolation for projecting lattice features back onto the point cloud. We
present results of 3D segmentation on multiple datasets where our method
achieves state-of-the-art performance. We also extend and evaluate our network
for instance and dynamic object segmentation.","['cs.CV', 'cs.LG']",132,97
A Survey on Object Detection in Optical Remote Sensing Images,"Object detection in optical remote sensing images, being a fundamental but
challenging problem in the field of aerial and satellite image analysis, plays
an important role for a wide range of applications and is receiving significant
attention in recent years. While enormous methods exist, a deep review of the
literature concerning generic object detection is still lacking. This paper
aims to provide a review of the recent progress in this field. Different from
several previously published surveys that focus on a specific object class such
as building and road, we concentrate on more generic object categories
including, but are not limited to, road, building, tree, vehicle, ship,
airport, urban-area. Covering about 270 publications we survey 1) template
matching-based object detection methods, 2) knowledge-based object detection
methods, 3) object-based image analysis (OBIA)-based object detection methods,
4) machine learning-based object detection methods, and 5) five publicly
available datasets and three standard evaluation metrics. We also discuss the
challenges of current studies and propose two promising research directions,
namely deep learning-based feature representation and weakly supervised
learning-based geospatial object detection. It is our hope that this survey
will be beneficial for the researchers to have better understanding of this
research field.",['cs.CV'],206,135
Detecting floodwater on roadways from image data with handcrafted features and deep transfer learning,"Detecting roadway segments inundated due to floodwater has important
applications for vehicle routing and traffic management decisions. This paper
proposes a set of algorithms to automatically detect floodwater that may be
present in an image captured by mobile phones or other types of optical
cameras. For this purpose, image classification and flood area segmentation
methods are developed. For the classification task, we used Local Binary
Patterns (LBP), Histogram of Oriented Gradients (HOG) and pre-trained deep
neural network (VGG-16) as feature extractors and trained logistic regression,
k-nearest neighbors, and decision tree classifiers on the extracted features.
Pre-trained VGG-16 network with logistic regression classifier outperformed all
other methods. For the flood area segmentation task, we investigated superpixel
based methods and Fully Convolutional Neural Network (FCN). Similar to the
classification task, we trained logistic regression and k-nearest neighbors
classifiers on the superpixel areas and compared that with an end-to-end
trained FCN. Conditional Random Fields (CRF) method was applied after both
segmentation methods to post-process coarse segmentation results. FCN offered
the highest scores in all metrics; it was followed by superpixel-based logistic
regression and then superpixel-based KNN.",['cs.CV'],194,123
ModaNet: A Large-Scale Street Fashion Dataset with Polygon Annotations,"Understanding clothes from a single image has strong commercial and cultural
impacts on modern societies. However, this task remains a challenging computer
vision problem due to wide variations in the appearance, style, brand and
layering of clothing items. We present a new database called ModaNet, a
large-scale collection of images based on Paperdoll dataset. Our dataset
provides 55,176 street images, fully annotated with polygons on top of the 1
million weakly annotated street images in Paperdoll. ModaNet aims to provide a
technical benchmark to fairly evaluate the progress of applying the latest
computer vision techniques that rely on large data for fashion understanding.
The rich annotation of the dataset allows to measure the performance of
state-of-the-art algorithms for object detection, semantic segmentation and
polygon prediction on street fashion images in detail. The polygon-based
annotation dataset has been released https://github.com/eBay/modanet, we also
host the leaderboard at EvalAI:
https://evalai.cloudcv.org/featured-challenges/136/overview.",['cs.CV'],164,114
The Method of Multimodal MRI Brain Image Segmentation Based on Differential Geometric Features,"Accurate segmentation of brain tissue in magnetic resonance images (MRI) is a
diffcult task due to different types of brain abnormalities. Using information
and features from multimodal MRI including T1, T1-weighted inversion recovery
(T1-IR) and T2-FLAIR and differential geometric features including the Jacobian
determinant(JD) and the curl vector(CV) derived from T1 modality can result in
a more accurate analysis of brain images. In this paper, we use the
differential geometric information including JD and CV as image characteristics
to measure the differences between different MRI images, which represent local
size changes and local rotations of the brain image, and we can use them as one
CNN channel with other three modalities (T1-weighted, T1-IR and T2-FLAIR) to
get more accurate results of brain segmentation. We test this method on two
datasets including IBSR dataset and MRBrainS datasets based on the deep
voxelwise residual network, namely VoxResNet, and obtain excellent improvement
over single modality or three modalities and increases average
DSC(Cerebrospinal Fluid (CSF), Gray Matter (GM) and White Matter (WM)) by about
1.5% on the well-known MRBrainS18 dataset and about 2.5% on the IBSR dataset.
Moreover, we discuss that one modality combined with its JD or CV information
can replace the segmentation effect of three modalities, which can provide
medical conveniences for doctor to diagnose because only to extract T1-modality
MRI image of patients. Finally, we also compare the segmentation performance of
our method in two networks, VoxResNet and U-Net network. The results show
VoxResNet has a better performance than U-Net network with our method in brain
MRI segmentation. We believe the proposed method can advance the performance in
brain segmentation and clinical diagnosis.",['cs.CV'],287,152
Deep Closest Point: Learning Representations for Point Cloud Registration,"Point cloud registration is a key problem for computer vision applied to
robotics, medical imaging, and other applications. This problem involves
finding a rigid transformation from one point cloud into another so that they
align. Iterative Closest Point (ICP) and its variants provide simple and
easily-implemented iterative methods for this task, but these algorithms can
converge to spurious local optima. To address local optima and other
difficulties in the ICP pipeline, we propose a learning-based method, titled
Deep Closest Point (DCP), inspired by recent techniques in computer vision and
natural language processing. Our model consists of three parts: a point cloud
embedding network, an attention-based module combined with a pointer generation
layer, to approximate combinatorial matching, and a differentiable singular
value decomposition (SVD) layer to extract the final rigid transformation. We
train our model end-to-end on the ModelNet40 dataset and show in several
settings that it performs better than ICP, its variants (e.g., Go-ICP, FGR),
and the recently-proposed learning-based method PointNetLK. Beyond providing a
state-of-the-art registration technique, we evaluate the suitability of our
learned features transferred to unseen objects. We also provide preliminary
analysis of our learned model to help understand whether domain-specific and/or
global features facilitate rigid registration.",['cs.CV'],213,144
Robust Gaussian Process Regression with a Bias Model,"This paper presents a new approach to a robust Gaussian process (GP)
regression. Most existing approaches replace an outlier-prone Gaussian
likelihood with a non-Gaussian likelihood induced from a heavy tail
distribution, such as the Laplace distribution and Student-t distribution.
However, the use of a non-Gaussian likelihood would incur the need for a
computationally expensive Bayesian approximate computation in the posterior
inferences. The proposed approach models an outlier as a noisy and biased
observation of an unknown regression function, and accordingly, the likelihood
contains bias terms to explain the degree of deviations from the regression
function. We entail how the biases can be estimated accurately with other
hyperparameters by a regularized maximum likelihood estimation. Conditioned on
the bias estimates, the robust GP regression can be reduced to a standard GP
regression problem with analytical forms of the predictive mean and variance
estimates. Therefore, the proposed approach is simple and very computationally
attractive. It also gives a very robust and accurate GP estimate for many
tested scenarios. For the numerical evaluation, we perform a comprehensive
simulation study to evaluate the proposed approach with the comparison to the
existing robust GP approaches under various simulated scenarios of different
outlier proportions and different noise levels. The approach is applied to data
from two measurement systems, where the predictors are based on robust
environmental parameter measurements and the response variables utilize more
complex chemical sensing methods that contain a certain percentage of outliers.
The utility of the measurement systems and value of the environmental data are
improved through the computationally efficient GP regression and bias model.","['cs.LG', 'stat.ME', 'stat.ML', '62G08']",265,150
Retinal Vasculature Segmentation Using Local Saliency Maps and Generative Adversarial Networks For Image Super Resolution,"We propose an image super resolution(ISR) method using generative adversarial
networks (GANs) that takes a low resolution input fundus image and generates a
high resolution super resolved (SR) image upto scaling factor of $16$. This
facilitates more accurate automated image analysis, especially for small or
blurred landmarks and pathologies. Local saliency maps, which define each
pixel's importance, are used to define a novel saliency loss in the GAN cost
function. Experimental results show the resulting SR images have perceptual
quality very close to the original images and perform better than competing
methods that do not weigh pixels according to their importance. When used for
retinal vasculature segmentation, our SR images result in accuracy levels close
to those obtained when using the original images.",['cs.CV'],125,94
What are the Statistical Limits of Offline RL with Linear Function Approximation?,"Offline reinforcement learning seeks to utilize offline (observational) data
to guide the learning of (causal) sequential decision making strategies. The
hope is that offline reinforcement learning coupled with function approximation
methods (to deal with the curse of dimensionality) can provide a means to help
alleviate the excessive sample complexity burden in modern sequential decision
making problems. However, the extent to which this broader approach can be
effective is not well understood, where the literature largely consists of
sufficient conditions.
  This work focuses on the basic question of what are necessary
representational and distributional conditions that permit provable
sample-efficient offline reinforcement learning. Perhaps surprisingly, our main
result shows that even if: i) we have realizability in that the true value
function of \emph{every} policy is linear in a given set of features and 2) our
off-policy data has good coverage over all features (under a strong spectral
condition), then any algorithm still (information-theoretically) requires a
number of offline samples that is exponential in the problem horizon in order
to non-trivially estimate the value of \emph{any} given policy. Our results
highlight that sample-efficient offline policy evaluation is simply not
possible unless significantly stronger conditions hold; such conditions include
either having low distribution shift (where the offline data distribution is
close to the distribution of the policy to be evaluated) or significantly
stronger representational conditions (beyond realizability).","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",231,143
Regret Bounds and Reinforcement Learning Exploration of EXP-based Algorithms,"EXP-based algorithms are often used for exploration in multi-armed bandit. We
revisit the EXP3.P algorithm and establish both the lower and upper bounds of
regret in the Gaussian multi-armed bandit setting, as well as a more general
distribution option. The analyses do not require bounded rewards compared to
classical regret assumptions. We also extend EXP4 from multi-armed bandit to
reinforcement learning to incentivize exploration by multiple agents. The
resulting algorithm has been tested on hard-to-explore games and it shows an
improvement on exploration compared to state-of-the-art.","['cs.LG', 'cs.AI', 'stat.ML']",96,70
Learning from History for Byzantine Robust Optimization,"Byzantine robustness has received significant attention recently given its
importance for distributed and federated learning. In spite of this, we
identify severe flaws in existing algorithms even when the data across the
participants is identically distributed. First, we show realistic examples
where current state of the art robust aggregation rules fail to converge even
in the absence of any Byzantine attackers. Secondly, we prove that even if the
aggregation rules may succeed in limiting the influence of the attackers in a
single round, the attackers can couple their attacks across time eventually
leading to divergence. To address these issues, we present two surprisingly
simple strategies: a new robust iterative clipping procedure, and incorporating
worker momentum to overcome time-coupled attacks. This is the first provably
robust method for the standard stochastic optimization setting. Our code is
open sourced at https://github.com/epfml/byzantine-robust-optimizer.","['cs.LG', 'cs.DC', 'math.OC', 'stat.ML', 'I.2.6; I.5.1']",146,107
Meta-Learning of Neural Architectures for Few-Shot Learning,"The recent progress in neural architecture search (NAS) has allowed scaling
the automated design of neural architectures to real-world domains, such as
object detection and semantic segmentation. However, one prerequisite for the
application of NAS are large amounts of labeled data and compute resources.
This renders its application challenging in few-shot learning scenarios, where
many related tasks need to be learned, each with limited amounts of data and
compute time. Thus, few-shot learning is typically done with a fixed neural
architecture. To improve upon this, we propose MetaNAS, the first method which
fully integrates NAS with gradient-based meta-learning. MetaNAS optimizes a
meta-architecture along with the meta-weights during meta-training. During
meta-testing, architectures can be adapted to a novel task with a few steps of
the task optimizer, that is: task adaptation becomes computationally cheap and
requires only little data per task. Moreover, MetaNAS is agnostic in that it
can be used with arbitrary model-agnostic meta-learning algorithms and
arbitrary gradient-based NAS methods. %We present encouraging results for
MetaNAS with a combination of DARTS and REPTILE on few-shot classification
benchmarks. Empirical results on standard few-shot classification benchmarks
show that MetaNAS with a combination of DARTS and REPTILE yields
state-of-the-art results.","['cs.LG', 'stat.ML']",214,125
Conformer: Local Features Coupling Global Representations for Visual Recognition,"Within Convolutional Neural Network (CNN), the convolution operations are
good at extracting local features but experience difficulty to capture global
representations. Within visual transformer, the cascaded self-attention modules
can capture long-distance feature dependencies but unfortunately deteriorate
local feature details. In this paper, we propose a hybrid network structure,
termed Conformer, to take advantage of convolutional operations and
self-attention mechanisms for enhanced representation learning. Conformer roots
in the Feature Coupling Unit (FCU), which fuses local features and global
representations under different resolutions in an interactive fashion.
Conformer adopts a concurrent structure so that local features and global
representations are retained to the maximum extent. Experiments show that
Conformer, under the comparable parameter complexity, outperforms the visual
transformer (DeiT-B) by 2.3% on ImageNet. On MSCOCO, it outperforms ResNet-101
by 3.7% and 3.6% mAPs for object detection and instance segmentation,
respectively, demonstrating the great potential to be a general backbone
network. Code is available at https://github.com/pengzhiliang/Conformer.",['cs.CV'],165,116
Unconstrained Scene Generation with Locally Conditioned Radiance Fields,"We tackle the challenge of learning a distribution over complex, realistic,
indoor scenes. In this paper, we introduce Generative Scene Networks (GSN),
which learns to decompose scenes into a collection of many local radiance
fields that can be rendered from a free moving camera. Our model can be used as
a prior to generate new scenes, or to complete a scene given only sparse 2D
observations. Recent work has shown that generative models of radiance fields
can capture properties such as multi-view consistency and view-dependent
lighting. However, these models are specialized for constrained viewing of
single objects, such as cars or faces. Due to the size and complexity of
realistic indoor environments, existing models lack the representational
capacity to adequately capture them. Our decomposition scheme scales to larger
and more complex scenes while preserving details and diversity, and the learned
prior enables high-quality rendering from viewpoints that are significantly
different from observed viewpoints. When compared to existing models, GSN
produces quantitatively higher-quality scene renderings across several
different scene datasets.","['cs.CV', 'cs.LG']",173,118
Looking Ahead: Anticipating Pedestrians Crossing with Future Frames Prediction,"In this paper, we present an end-to-end future-prediction model that focuses
on pedestrian safety. Specifically, our model uses previous video frames,
recorded from the perspective of the vehicle, to predict if a pedestrian will
cross in front of the vehicle. The long term goal of this work is to design a
fully autonomous system that acts and reacts as a defensive human driver would
--- predicting future events and reacting to mitigate risk. We focus on
pedestrian-vehicle interactions because of the high risk of harm to the
pedestrian if their actions are miss-predicted. Our end-to-end model consists
of two stages: the first stage is an encoder/decoder network that learns to
predict future video frames. The second stage is a deep spatio-temporal network
that utilizes the predicted frames of the first stage to predict the
pedestrian's future action. Our system achieves state-of-the-art accuracy on
pedestrian behavior prediction and future frames prediction on the Joint
Attention for Autonomous Driving (JAAD) dataset.","['cs.CV', 'cs.LG']",171,98
HyperSAGE: Generalizing Inductive Representation Learning on Hypergraphs,"Graphs are the most ubiquitous form of structured data representation used in
machine learning. They model, however, only pairwise relations between nodes
and are not designed for encoding the higher-order relations found in many
real-world datasets. To model such complex relations, hypergraphs have proven
to be a natural representation. Learning the node representations in a
hypergraph is more complex than in a graph as it involves information
propagation at two levels: within every hyperedge and across the hyperedges.
Most current approaches first transform a hypergraph structure to a graph for
use in existing geometric deep learning algorithms. This transformation leads
to information loss, and sub-optimal exploitation of the hypergraph's
expressive power. We present HyperSAGE, a novel hypergraph learning framework
that uses a two-level neural message passing strategy to accurately and
efficiently propagate information through hypergraphs. The flexible design of
HyperSAGE facilitates different ways of aggregating neighborhood information.
Unlike the majority of related work which is transductive, our approach,
inspired by the popular GraphSAGE method, is inductive. Thus, it can also be
used on previously unseen nodes, facilitating deployment in problems such as
evolving or partially observed hypergraphs. Through extensive experimentation,
we show that HyperSAGE outperforms state-of-the-art hypergraph learning methods
on representative benchmark datasets. We also demonstrate that the higher
expressive power of HyperSAGE makes it more stable in learning node
representations as compared to the alternatives.","['cs.LG', 'stat.ML']",234,153
Dual Transfer Learning for Event-based End-task Prediction via Pluggable Event to Image Translation,"Event cameras are novel sensors that perceive the per-pixel intensity changes
and output asynchronous event streams with high dynamic range and less motion
blur. It has been shown that events alone can be used for end-task learning,
\eg, semantic segmentation, based on encoder-decoder-like networks. However, as
events are sparse and mostly reflect edge information, it is difficult to
recover original details merely relying on the decoder. Moreover, most methods
resort to pixel-wise loss alone for supervision, which might be insufficient to
fully exploit the visual details from sparse events, thus leading to less
optimal performance. In this paper, we propose a simple yet flexible two-stream
framework named Dual Transfer Learning (DTL) to effectively enhance the
performance on the end-tasks without adding extra inference cost. The proposed
approach consists of three parts: event to end-task learning (EEL) branch,
event to image translation (EIT) branch, and transfer learning (TL) module that
simultaneously explores the feature-level affinity information and pixel-level
knowledge from the EIT branch to improve the EEL branch. This simple yet novel
method leads to strong representation learning from events and is evidenced by
the significant performance boost on the end-tasks such as semantic
segmentation and depth estimation.",['cs.CV'],208,139
Near-optimal Representation Learning for Linear Bandits and Linear RL,"This paper studies representation learning for multi-task linear bandits and
multi-task episodic RL with linear value function approximation. We first
consider the setting where we play $M$ linear bandits with dimension $d$
concurrently, and these bandits share a common $k$-dimensional linear
representation so that $k\ll d$ and $k \ll M$. We propose a sample-efficient
algorithm, MTLR-OFUL, which leverages the shared representation to achieve
$\tilde{O}(M\sqrt{dkT} + d\sqrt{kMT} )$ regret, with $T$ being the number of
total steps. Our regret significantly improves upon the baseline
$\tilde{O}(Md\sqrt{T})$ achieved by solving each task independently. We further
develop a lower bound that shows our regret is near-optimal when $d > M$.
Furthermore, we extend the algorithm and analysis to multi-task episodic RL
with linear value function approximation under low inherent Bellman error
\citep{zanette2020learning}. To the best of our knowledge, this is the first
theoretical result that characterizes the benefits of multi-task representation
learning for exploration in RL with function approximation.",['cs.LG'],172,102
Unsupervised Features Learning for Sampled Vector Fields,"In this paper we introduce a new approach to computing hidden features of
sampled vector fields. The basic idea is to convert the vector field data to a
graph structure and use tools designed for automatic, unsupervised analysis of
graphs. Using a few data sets we show that the collected features of the vector
fields are correlated with the dynamics known for analytic models which
generates the data. In particular the method may be useful in analysis of data
sets where the analytic model is poorly understood or not known.","['cs.LG', 'math.AT', 'math.DS']",90,62
Hierarchical Image Classification using Entailment Cone Embeddings,"Image classification has been studied extensively, but there has been limited
work in using unconventional, external guidance other than traditional
image-label pairs for training. We present a set of methods for leveraging
information about the semantic hierarchy embedded in class labels. We first
inject label-hierarchy knowledge into an arbitrary CNN-based classifier and
empirically show that availability of such external semantic information in
conjunction with the visual semantics from images boosts overall performance.
Taking a step further in this direction, we model more explicitly the
label-label and label-image interactions using order-preserving embeddings
governed by both Euclidean and hyperbolic geometries, prevalent in natural
language, and tailor them to hierarchical image classification and
representation learning. We empirically validate all the models on the
hierarchical ETHEC dataset.","['cs.CV', 'cs.LG', 'stat.ML']",129,96
Relational Deep Reinforcement Learning,"We introduce an approach for deep reinforcement learning (RL) that improves
upon the efficiency, generalization capacity, and interpretability of
conventional approaches through structured perception and relational reasoning.
It uses self-attention to iteratively reason about the relations between
entities in a scene and to guide a model-free policy. Our results show that in
a novel navigation and planning task called Box-World, our agent finds
interpretable solutions that improve upon baselines in terms of sample
complexity, ability to generalize to more complex scenes than experienced
during training, and overall performance. In the StarCraft II Learning
Environment, our agent achieves state-of-the-art performance on six mini-games
-- surpassing human grandmaster performance on four. By considering
architectural inductive biases, our work opens new directions for overcoming
important, but stubborn, challenges in deep RL.","['cs.LG', 'stat.ML']",134,105
Domain Adaptation by Topology Regularization,"Deep learning has become the leading approach to assisted target recognition.
While these methods typically require large amounts of labeled training data,
domain adaptation (DA) or transfer learning (TL) enables these algorithms to
transfer knowledge from a labelled (source) data set to an unlabelled but
related (target) data set of interest. DA enables networks to overcome the
distribution mismatch between the source and target that leads to poor
generalization in the target domain. DA techniques align these distributions by
minimizing a divergence measurement between source and target, making the
transfer of knowledge from source to target possible. While these algorithms
have advanced significantly in recent years, most do not explicitly leverage
global data manifold structure in aligning the source and target. We propose to
leverage global data structure by applying a topological data analysis (TDA)
technique called persistent homology to TL.
  In this paper, we examine the use of persistent homology in a domain
adversarial (DAd) convolutional neural network (CNN) architecture. The
experiments show that aligning persistence alone is insufficient for transfer,
but must be considered along with the lifetimes of the topological
singularities. In addition, we found that longer lifetimes indicate robust
discriminative features and more favorable structure in data. We found that
existing divergence minimization based approaches to DA improve the topological
structure, as indicated over a baseline without these regularization
techniques. We hope these experiments highlight how topological structure can
be leveraged to boost performance in TL tasks.",['cs.CV'],241,141
Auto-painter: Cartoon Image Generation from Sketch by Using Conditional Generative Adversarial Networks,"Recently, realistic image generation using deep neural networks has become a
hot topic in machine learning and computer vision. Images can be generated at
the pixel level by learning from a large collection of images. Learning to
generate colorful cartoon images from black-and-white sketches is not only an
interesting research problem, but also a potential application in digital
entertainment. In this paper, we investigate the sketch-to-image synthesis
problem by using conditional generative adversarial networks (cGAN). We propose
the auto-painter model which can automatically generate compatible colors for a
sketch. The new model is not only capable of painting hand-draw sketch with
proper colors, but also allowing users to indicate preferred colors.
Experimental results on two sketch datasets show that the auto-painter performs
better that existing image-to-image methods.","['cs.CV', 'I.4.9; I.4.8; I.3.3']",136,98
Student-Initiated Action Advising via Advice Novelty,"Action advising is a budget-constrained knowledge exchange mechanism between
teacher-student peers that can help tackle exploration and sample inefficiency
problems in deep reinforcement learning (RL). Most recently, student-initiated
techniques that utilise state novelty and uncertainty estimations have obtained
promising results. However, the approaches built on these estimations have some
potential weaknesses. First, they assume that the convergence of the student's
RL model implies less need for advice. This can be misleading in scenarios with
teacher absence early on where the student is likely to learn suboptimally by
itself; yet also ignore the teacher's assistance later. Secondly, the delays
between encountering states and having them to take effect in the RL model
updates in presence of the experience replay dynamics cause a feedback lag in
what the student actually needs advice for. We propose a student-initiated
algorithm that alleviates these by employing Random Network Distillation (RND)
to measure the novelty of a piece of advice. Furthermore, we perform RND
updates only for the advised states to ensure that the student's own learning
does not impair its ability to leverage the teacher. Experiments in GridWorld
and MinAtar show that our approach performs on par with the state-of-the-art
and demonstrates significant advantages in the scenarios where the existing
methods are prone to fail.","['cs.LG', 'stat.ML']",220,140
Understanding Visual Ads by Aligning Symbols and Objects using Co-Attention,"We tackle the problem of understanding visual ads where given an ad image,
our goal is to rank appropriate human generated statements describing the
purpose of the ad. This problem is generally addressed by jointly embedding
images and candidate statements to establish correspondence. Decoding a visual
ad requires inference of both semantic and symbolic nuances referenced in an
image and prior methods may fail to capture such associations especially with
weakly annotated symbols. In order to create better embeddings, we leverage an
attention mechanism to associate image proposals with symbols and thus
effectively aggregate information from aligned multimodal representations. We
propose a multihop co-attention mechanism that iteratively refines the
attention map to ensure accurate attention estimation. Our attention based
embedding model is learned end-to-end guided by a max-margin loss function. We
show that our model outperforms other baselines on the benchmark Ad dataset and
also show qualitative results to highlight the advantages of using multihop
co-attention.",['cs.CV'],161,110
Shift Equivariance in Object Detection,"Robustness to small image translations is a highly desirable property for
object detectors. However, recent works have shown that CNN-based classifiers
are not shift invariant. It is unclear to what extent this could impact object
detection, mainly because of the architectural differences between the two and
the dimensionality of the prediction space of modern detectors. To assess shift
equivariance of object detection models end-to-end, in this paper we propose an
evaluation metric, built upon a greedy search of the lower and upper bounds of
the mean average precision on a shifted image set. Our new metric shows that
modern object detection architectures, no matter if one-stage or two-stage,
anchor-based or anchor-free, are sensitive to even one pixel shift to the input
images. Furthermore, we investigate several possible solutions to this problem,
both taken from the literature and newly proposed, quantifying the
effectiveness of each one with the suggested metric. Our results indicate that
none of these methods can provide full shift equivariance. Measuring and
analyzing the extent of shift variance of different models and the
contributions of possible factors, is a first step towards being able to devise
methods that mitigate or even leverage such variabilities.",['cs.CV'],203,130
Adaptive transfer learning,"In transfer learning, we wish to make inference about a target population
when we have access to data both from the distribution itself, and from a
different but related source distribution. We introduce a flexible framework
for transfer learning in the context of binary classification, allowing for
covariate-dependent relationships between the source and target distributions
that are not required to preserve the Bayes decision boundary. Our main
contributions are to derive the minimax optimal rates of convergence (up to
poly-logarithmic factors) in this problem, and show that the optimal rate can
be achieved by an algorithm that adapts to key aspects of the unknown transfer
relationship, as well as the smoothness and tail parameters of our
distributional classes. This optimal rate turns out to have several regimes,
depending on the interplay between the relative sample sizes and the strength
of the transfer relationship, and our algorithm achieves optimality by careful,
decision tree-based calibration of local nearest-neighbour procedures.","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH', '62G05']",161,107
Multi-Aspect Temporal Network Embedding: A Mixture of Hawkes Process View,"Recent years have witnessed the tremendous research interests in network
embedding. Extant works have taken the neighborhood formation as the critical
information to reveal the inherent dynamics of network structures, and
suggested encoding temporal edge formation sequences to capture the historical
influences of neighbors. In this paper, however, we argue that the edge
formation can be attributed to a variety of driving factors including the
temporal influence, which is better referred to as multiple aspects. As a
matter of fact, different node aspects can drive the formation of distinctive
neighbors, giving birth to the multi-aspect embedding that relates to but goes
beyond a temporal scope. Along this vein, we propose a Mixture of Hawkes-based
Temporal Network Embeddings (MHNE) model to capture the aspect-driven
neighborhood formation of networks. In MHNE, we encode the multi-aspect
embeddings into the mixture of Hawkes processes to gain the advantages in
modeling the excitation effects and the latent aspects. Specifically, a graph
attention mechanism is used to assign different weights to account for the
excitation effects of history events, while a Gumbel-Softmax is plugged in to
derive the distribution over the aspects. Extensive experiments on 8 different
temporal networks have demonstrated the great performance of the multi-aspect
embeddings obtained by MHNE in comparison with the state-of-the-art methods.",['cs.LG'],220,129
Multi-Stage Temporal Difference Learning for 2048-like Games,"Szubert and Jaskowski successfully used temporal difference (TD) learning
together with n-tuple networks for playing the game 2048. However, we observed
a phenomenon that the programs based on TD learning still hardly reach large
tiles. In this paper, we propose multi-stage TD (MS-TD) learning, a kind of
hierarchical reinforcement learning method, to effectively improve the
performance for the rates of reaching large tiles, which are good metrics to
analyze the strength of 2048 programs. Our experiments showed significant
improvements over the one without using MS-TD learning. Namely, using 3-ply
expectimax search, the program with MS-TD learning reached 32768-tiles with a
rate of 18.31%, while the one with TD learning did not reach any. After further
tuned, our 2048 program reached 32768-tiles with a rate of 31.75% in 10,000
games, and one among these games even reached a 65536-tile, which is the first
ever reaching a 65536-tile to our knowledge. In addition, MS-TD learning method
can be easily applied to other 2048-like games, such as Threes. Based on MS-TD
learning, our experiments for Threes also demonstrated similar performance
improvement, where the program with MS-TD learning reached 6144-tiles with a
rate of 7.83%, while the one with TD learning only reached 0.45%.",['cs.LG'],220,121
Partial Policy-based Reinforcement Learning for Anatomical Landmark Localization in 3D Medical Images,"Deploying the idea of long-term cumulative return, reinforcement learning has
shown remarkable performance in various fields. We propose a formulation of the
landmark localization in 3D medical images as a reinforcement learning problem.
Whereas value-based methods have been widely used to solve similar problems, we
adopt an actor-critic based direct policy search method framed in a temporal
difference learning approach. Successful behavior learning is challenging in
large state and/or action spaces, requiring many trials. We introduce a partial
policy-based reinforcement learning to enable solving the large problem of
localization by learning the optimal policy on smaller partial domains.
Independent actors efficiently learn the corresponding partial policies, each
utilizing their own independent critic. The proposed policy reconstruction from
the partial policies ensures a robust and efficient localization utilizing the
sub-agents solving simple binary decision problems in their corresponding
partial action spaces. The proposed reinforcement learning requires a small
number of trials to learn the optimal behavior compared with the original
behavior learning scheme.","['cs.CV', 'cs.LG']",168,105
Interpretable Reinforcement Learning with Ensemble Methods,"We propose to use boosted regression trees as a way to compute
human-interpretable solutions to reinforcement learning problems. Boosting
combines several regression trees to improve their accuracy without
significantly reducing their inherent interpretability. Prior work has focused
independently on reinforcement learning and on interpretable machine learning,
but there has been little progress in interpretable reinforcement learning. Our
experimental results show that boosted regression trees compute solutions that
are both interpretable and match the quality of leading reinforcement learning
methods.","['cs.LG', 'stat.ML']",80,55
Explainable Deep Convolutional Candlestick Learner,"Candlesticks are graphical representations of price movements for a given
period. The traders can discovery the trend of the asset by looking at the
candlestick patterns. Although deep convolutional neural networks have achieved
great success for recognizing the candlestick patterns, their reasoning hides
inside a black box. The traders cannot make sure what the model has learned. In
this contribution, we provide a framework which is to explain the reasoning of
the learned model determining the specific candlestick patterns of time series.
Based on the local search adversarial attacks, we show that the learned model
perceives the pattern of the candlesticks in a way similar to the human trader.","['cs.LG', 'cs.CV', 'stat.ML']",109,76
Topology Distillation for Recommender System,"Recommender Systems (RS) have employed knowledge distillation which is a
model compression technique training a compact student model with the knowledge
transferred from a pre-trained large teacher model. Recent work has shown that
transferring knowledge from the teacher's intermediate layer significantly
improves the recommendation quality of the student. However, they transfer the
knowledge of individual representation point-wise and thus have a limitation in
that primary information of RS lies in the relations in the representation
space. This paper proposes a new topology distillation approach that guides the
student by transferring the topological structure built upon the relations in
the teacher space. We first observe that simply making the student learn the
whole topological structure is not always effective and even degrades the
student's performance. We demonstrate that because the capacity of the student
is highly limited compared to that of the teacher, learning the whole
topological structure is daunting for the student. To address this issue, we
propose a novel method named Hierarchical Topology Distillation (HTD) which
distills the topology hierarchically to cope with the large capacity gap. Our
extensive experiments on real-world datasets show that the proposed method
significantly outperforms the state-of-the-art competitors. We also provide
in-depth analyses to ascertain the benefit of distilling the topology for RS.","['cs.LG', 'cs.IR']",218,126
Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video,"In this paper, we tackle the problem of egocentric action anticipation, i.e.,
predicting what actions the camera wearer will perform in the near future and
which objects they will interact with. Specifically, we contribute
Rolling-Unrolling LSTM, a learning architecture to anticipate actions from
egocentric videos. The method is based on three components: 1) an architecture
comprised of two LSTMs to model the sub-tasks of summarizing the past and
inferring the future, 2) a Sequence Completion Pre-Training technique which
encourages the LSTMs to focus on the different sub-tasks, and 3) a Modality
ATTention (MATT) mechanism to efficiently fuse multi-modal predictions
performed by processing RGB frames, optical flow fields and object-based
features. The proposed approach is validated on EPIC-Kitchens, EGTEA Gaze+ and
ActivityNet. The experiments show that the proposed architecture is
state-of-the-art in the domain of egocentric videos, achieving top performances
in the 2019 EPIC-Kitchens egocentric action anticipation challenge. The
approach also achieves competitive performance on ActivityNet with respect to
methods not based on unsupervised pre-training and generalizes to the tasks of
early action recognition and action recognition. To encourage research on this
challenging topic, we made our code, trained models, and pre-extracted features
available at our web page: http://iplab.dmi.unict.it/rulstm.",['cs.CV'],217,141
Contextual Decision Processes with Low Bellman Rank are PAC-Learnable,"This paper studies systematic exploration for reinforcement learning with
rich observations and function approximation. We introduce a new model called
contextual decision processes, that unifies and generalizes most prior
settings. Our first contribution is a complexity measure, the Bellman rank,
that we show enables tractable learning of near-optimal behavior in these
processes and is naturally small for many well-studied reinforcement learning
settings. Our second contribution is a new reinforcement learning algorithm
that engages in systematic exploration to learn contextual decision processes
with low Bellman rank. Our algorithm provably learns near-optimal behavior with
a number of samples that is polynomial in all relevant parameters but
independent of the number of unique observations. The approach uses Bellman
error minimization with optimistic exploration and provides new insights into
efficient exploration for reinforcement learning with function approximation.","['cs.LG', 'stat.ML']",136,80
LioNets: A Neural-Specific Local Interpretation Technique Exploiting Penultimate Layer Information,"Artificial Intelligence (AI) has a tremendous impact on the unexpected growth
of technology in almost every aspect. AI-powered systems are monitoring and
deciding about sensitive economic and societal issues. The future is towards
automation, and it must not be prevented. However, this is a conflicting
viewpoint for a lot of people, due to the fear of uncontrollable AI systems.
This concern could be reasonable if it was originating from considerations
associated with social issues, like gender-biased, or obscure decision-making
systems. Explainable AI (XAI) is recently treated as a huge step towards
reliable systems, enhancing the trust of people to AI. Interpretable machine
learning (IML), a subfield of XAI, is also an urgent topic of research. This
paper presents a small but significant contribution to the IML community,
focusing on a local-based, neural-specific interpretation process applied to
textual and time-series data. The proposed methodology introduces new
approaches to the presentation of feature importance based interpretations, as
well as the production of counterfactual words on textual datasets. Eventually,
an improved evaluation metric is introduced for the assessment of
interpretation techniques, which supports an extensive set of qualitative and
quantitative experiments.","['cs.LG', 'cs.AI', 'I.2.0; I.2.6; I.2.7']",194,135
Discovering Fair Representations in the Data Domain,"Interpretability and fairness are critical in computer vision and machine
learning applications, in particular when dealing with human outcomes, e.g.
inviting or not inviting for a job interview based on application materials
that may include photographs. One promising direction to achieve fairness is by
learning data representations that remove the semantics of protected
characteristics, and are therefore able to mitigate unfair outcomes. All
available models however learn latent embeddings which comes at the cost of
being uninterpretable. We propose to cast this problem as data-to-data
translation, i.e. learning a mapping from an input domain to a fair target
domain, where a fairness definition is being enforced. Here the data domain can
be images, or any tabular data representation. This task would be
straightforward if we had fair target data available, but this is not the case.
To overcome this, we learn a highly unconstrained mapping by exploiting
statistics of residuals - the difference between input data and its translated
version - and the protected characteristics. When applied to the CelebA dataset
of face images with gender attribute as the protected characteristic, our model
enforces equality of opportunity by adjusting the eyes and lips regions.
Intriguingly, on the same dataset we arrive at similar conclusions when using
semantic attribute representations of images for translation. On face images of
the recent DiF dataset, with the same gender attribute, our method adjusts nose
regions. In the Adult income dataset, also with protected gender attribute, our
model achieves equality of opportunity by, among others, obfuscating the wife
and husband relationship. Analyzing those systematic changes will allow us to
scrutinize the interplay of fairness criterion, chosen protected
characteristics, and prediction performance.","['cs.LG', 'stat.ML']",278,166
Bi-Level Image Thresholding obtained by means of Kaniadakis Entropy,"In this paper we are proposing the use of Kaniadakis entropy in the bi-level
thresholding of images, in the framework of a maximum entropy principle. We
discuss the role of its entropic index in determining the threshold and in
driving an ""image transition"", that is, an abrupt transition in the appearance
of the corresponding bi-level image. Some examples are proposed to illustrate
the method and for comparing it to the approach which is using the Tsallis
entropy.",['cs.CV'],79,51
Forecasting adverse surgical events using self-supervised transfer learning for physiological signals,"Hundreds of millions of surgical procedures take place annually across the
world, which generate a prevalent type of electronic health record (EHR) data
comprising time series physiological signals. Here, we present a transferable
embedding method (i.e., a method to transform time series signals into input
features for predictive machine learning models) named PHASE (PHysiologicAl
Signal Embeddings) that enables us to more accurately forecast adverse surgical
outcomes based on physiological signals. We evaluate PHASE on minute-by-minute
EHR data of more than 50,000 surgeries from two operating room (OR) datasets
and patient stays in an intensive care unit (ICU) dataset. PHASE outperforms
other state-of-the-art approaches, such as long-short term memory networks
trained on raw data and gradient boosted trees trained on handcrafted features,
in predicting five distinct outcomes: hypoxemia, hypocapnia, hypotension,
hypertension, and phenylephrine administration. In a transfer learning setting
where we train embedding models in one dataset then embed signals and predict
adverse events in unseen data, PHASE achieves significantly higher prediction
accuracy at lower computational cost compared to conventional approaches.
Finally, given the importance of understanding models in clinical applications
we demonstrate that PHASE is explainable and validate our predictive models
using local feature attribution methods.","['cs.LG', 'eess.SP', 'stat.ML']",204,148
A Structure-aware Online Learning Algorithm for Markov Decision Processes,"To overcome the curse of dimensionality and curse of modeling in Dynamic
Programming (DP) methods for solving classical Markov Decision Process (MDP)
problems, Reinforcement Learning (RL) algorithms are popular. In this paper, we
consider an infinite-horizon average reward MDP problem and prove the
optimality of the threshold policy under certain conditions. Traditional RL
techniques do not exploit the threshold nature of optimal policy while
learning. In this paper, we propose a new RL algorithm which utilizes the known
threshold structure of the optimal policy while learning by reducing the
feasible policy space. We establish that the proposed algorithm converges to
the optimal policy. It provides a significant improvement in convergence speed
and computational and storage complexity over traditional RL algorithms. The
proposed technique can be applied to a wide variety of optimization problems
that include energy efficient data transmission and management of queues. We
exhibit the improvement in convergence speed of the proposed algorithm over
other RL algorithms through simulations.","['cs.LG', 'stat.ML']",161,102
AC-VRNN: Attentive Conditional-VRNN for Multi-Future Trajectory Prediction,"Anticipating human motion in crowded scenarios is essential for developing
intelligent transportation systems, social-aware robots and advanced video
surveillance applications. A key component of this task is represented by the
inherently multi-modal nature of human paths which makes socially acceptable
multiple futures when human interactions are involved. To this end, we propose
a generative architecture for multi-future trajectory predictions based on
Conditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning
mainly relies on prior belief maps, representing most likely moving directions
and forcing the model to consider past observed dynamics in generating future
positions. Human interactions are modeled with a graph-based attention
mechanism enabling an online attentive hidden state refinement of the recurrent
estimation. To corroborate our model, we perform extensive experiments on
publicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS
SportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its
effectiveness in crowded scenes compared to several state-of-the-art methods.","['cs.CV', 'cs.LG']",159,127
Matlab Implementation of Machine Vision Algorithm on Ballast Degradation Evaluation,"America has a massive railway system. As of 2006, U.S. freight railroads have
140,490 route- miles of standard gauge, but maintaining such a huge system and
eliminating any dangers, like reduced track stability and poor drainage, caused
by railway ballast degradation require huge amount of labor. The traditional
way to quantify the degradation of ballast is to use an index called Fouling
Index (FI) through ballast sampling and sieve analysis. However, determining
the FI values in lab is very time-consuming and laborious, but with the help of
recent development in the field of computer vision, a novel method for a
potential machine-vison based ballast inspection system can be employed that
can hopefully replace the traditional mechanical method. The new machine-vision
approach analyses the images of the in-service ballasts, and then utilizes
image segmentation algorithm to get ballast segments. By comparing the segment
results and their corresponding FI values, this novel method produces a
machine-vision-based index that has the best-fit relation with FI. The
implementation details of how this algorithm works are discussed in this
report.",['cs.CV'],184,121
Wavelet-Based Segmentation on the Sphere,"Segmentation, a useful/powerful technique in pattern recognition, is the
process of identifying object outlines within images. There are a number of
efficient algorithms for segmentation in Euclidean space that depend on the
variational approach and partial differential equation modelling. Wavelets have
been used successfully in various problems in image processing, including
segmentation, inpainting, noise removal, super-resolution image restoration,
and many others. Wavelets on the sphere have been developed to solve such
problems for data defined on the sphere, which arise in numerous fields such as
cosmology and geophysics. In this work, we propose a wavelet-based method to
segment images on the sphere, accounting for the underlying geometry of
spherical data. Our method is a direct extension of the tight-frame based
segmentation method used to automatically identify tube-like structures such as
blood vessels in medical imaging. It is compatible with any arbitrary type of
wavelet frame defined on the sphere, such as axisymmetric wavelets, directional
wavelets, curvelets, and hybrid wavelet constructions. Such an approach allows
the desirable properties of wavelets to be naturally inherited in the
segmentation process. In particular, directional wavelets and curvelets, which
were designed to efficiently capture directional signal content, provide
additional advantages in segmenting images containing prominent directional and
curvilinear features. We present several numerical experiments, applying our
wavelet-based segmentation method, as well as the common K-means method, on
real-world spherical images. These experiments demonstrate the superiority of
our method and show that it is capable of segmenting different kinds of
spherical images, including those with prominent directional features.
Moreover, our algorithm is efficient with convergence usually within a few
iterations.","['cs.CV', 'cs.IT', 'math.IT']",272,158
Visual Translation Embedding Network for Visual Relation Detection,"Visual relations, such as ""person ride bike"" and ""bike next to car"", offer a
comprehensive scene understanding of an image, and have already shown their
great utility in connecting computer vision and natural language. However, due
to the challenging combinatorial complexity of modeling
subject-predicate-object relation triplets, very little work has been done to
localize and predict visual relations. Inspired by the recent advances in
relational representation learning of knowledge bases and convolutional object
detection networks, we propose a Visual Translation Embedding network (VTransE)
for visual relation detection. VTransE places objects in a low-dimensional
relation space where a relation can be modeled as a simple vector translation,
i.e., subject + predicate $\approx$ object. We propose a novel feature
extraction layer that enables object-relation knowledge transfer in a
fully-convolutional fashion that supports training and inference in a single
forward/backward pass. To the best of our knowledge, VTransE is the first
end-to-end relation detection network. We demonstrate the effectiveness of
VTransE over other state-of-the-art methods on two large-scale datasets: Visual
Relationship and Visual Genome. Note that even though VTransE is a purely
visual model, it is still competitive to the Lu's multi-modal model with
language priors.","['cs.CV', 'I.4']",207,137
Transformer-Based Attention Networks for Continuous Pixel-Wise Prediction,"While convolutional neural networks have shown a tremendous impact on various
computer vision tasks, they generally demonstrate limitations in explicitly
modeling long-range dependencies due to the intrinsic locality of the
convolution operation. Initially designed for natural language processing
tasks, Transformers have emerged as alternative architectures with innate
global self-attention mechanisms to capture long-range dependencies. In this
paper, we propose TransDepth, an architecture that benefits from both
convolutional neural networks and transformers. To avoid the network losing its
ability to capture local-level details due to the adoption of transformers, we
propose a novel decoder that employs attention mechanisms based on gates.
Notably, this is the first paper that applies transformers to pixel-wise
prediction problems involving continuous labels (i.e., monocular depth
prediction and surface normal estimation). Extensive experiments demonstrate
that the proposed TransDepth achieves state-of-the-art performance on three
challenging datasets. Our code is available at:
https://github.com/ygjwd12345/TransDepth.",['cs.CV'],157,115
An Adaptive Approach for Anomaly Detector Selection and Fine-Tuning in Time Series,"The anomaly detection of time series is a hotspot of time series data mining.
The own characteristics of different anomaly detectors determine the abnormal
data that they are good at. There is no detector can be optimizing in all types
of anomalies. Moreover, it still has difficulties in industrial production due
to problems such as a single detector can't be optimized at different time
windows of the same time series. This paper proposes an adaptive model based on
time series characteristics and selecting appropriate detector and run-time
parameters for anomaly detection, which is called ATSDLN(Adaptive Time Series
Detector Learning Network). We take the time series as the input of the model,
and learn the time series representation through FCN. In order to realize the
adaptive selection of detectors and run-time parameters according to the input
time series, the outputs of FCN are the inputs of two sub-networks: the
detector selection network and the run-time parameters selection network. In
addition, the way that the variable layer width design of the parameter
selection sub-network and the introduction of transfer learning make the model
be with more expandability. Through experiments, it is found that ATSDLN can
select appropriate anomaly detector and run-time parameters, and have strong
expandability, which can quickly transfer. We investigate the performance of
ATSDLN in public data sets, our methods outperform other methods in most cases
with higher effect and better adaptation. We also show experimental results on
public data sets to demonstrate how model structure and transfer learning
affect the effectiveness.","['stat.ML', 'cs.LG']",260,135
Go Wider Instead of Deeper,"More transformer blocks with residual connections have recently achieved
impressive results on various tasks. To achieve better performance with fewer
trainable parameters, recent methods are proposed to go shallower by parameter
sharing or model compressing along with the depth. However, weak modeling
capacity limits their performance. Contrastively, going wider by inducing more
trainable matrixes and parameters would produce a huge model requiring advanced
parallelism to train and inference.
  In this paper, we propose a parameter-efficient framework, going wider
instead of deeper. Specially, following existing works, we adapt parameter
sharing to compress along depth. But, such deployment would limit the
performance. To maximize modeling capacity, we scale along model width by
replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across
transformer blocks, instead of sharing normalization layers, we propose to use
individual layernorms to transform various semantic representations in a more
parameter-efficient way. To evaluate our plug-and-run framework, we design
WideNet and conduct comprehensive experiments on popular computer vision and
natural language processing benchmarks. On ImageNet-1K, our best model
outperforms Vision Transformer (ViT) by $1.5\%$ with $0.72 \times$ trainable
parameters. Using $0.46 \times$ and $0.13 \times$ parameters, our WideNet can
still surpass ViT and ViT-MoE by $0.8\%$ and $2.1\%$, respectively. On four
natural language processing datasets, WideNet outperforms ALBERT by $1.8\%$ on
average and surpass BERT using factorized embedding parameterization by $0.8\%$
with fewer parameters.","['cs.LG', 'cs.AI', 'cs.CV']",240,148
Double Similarity Distillation for Semantic Image Segmentation,"The balance between high accuracy and high speed has always been a
challenging task in semantic image segmentation. Compact segmentation networks
are more widely used in the case of limited resources, while their performances
are constrained. In this paper, motivated by the residual learning and global
aggregation, we propose a simple yet general and effective knowledge
distillation framework called double similarity distillation (DSD) to improve
the classification accuracy of all existing compact networks by capturing the
similarity knowledge in pixel and category dimensions, respectively.
Specifically, we propose a pixel-wise similarity distillation (PSD) module that
utilizes residual attention maps to capture more detailed spatial dependencies
across multiple layers. Compared with exiting methods, the PSD module greatly
reduces the amount of calculation and is easy to expand. Furthermore,
considering the differences in characteristics between semantic segmentation
task and other computer vision tasks, we propose a category-wise similarity
distillation (CSD) module, which can help the compact segmentation network
strengthen the global category correlation by constructing the correlation
matrix. Combining these two modules, DSD framework has no extra parameters and
only a minimal increase in FLOPs. Extensive experiments on four challenging
datasets, including Cityscapes, CamVid, ADE20K, and Pascal VOC 2012, show that
DSD outperforms current state-of-the-art methods, proving its effectiveness and
generality. The code and models will be publicly available.",['cs.CV'],221,146
MathNet: Haar-Like Wavelet Multiresolution-Analysis for Graph Representation and Learning,"Graph Neural Networks (GNNs) have recently caught great attention and
achieved significant progress in graph-level applications. In this paper, we
propose a framework for graph neural networks with multiresolution Haar-like
wavelets, or MathNet, with interrelated convolution and pooling strategies. The
underlying method takes graphs in different structures as input and assembles
consistent graph representations for readout layers, which then accomplishes
label prediction. To achieve this, the multiresolution graph representations
are first constructed and fed into graph convolutional layers for processing.
The hierarchical graph pooling layers are then involved to downsample graph
resolution while simultaneously remove redundancy within graph signals. The
whole workflow could be formed with a multi-level graph analysis, which not
only helps embed the intrinsic topological information of each graph into the
GNN, but also supports fast computation of forward and adjoint graph
transforms. We show by extensive experiments that the proposed framework
obtains notable accuracy gains on graph classification and regression tasks
with performance stability. The proposed MathNet outperforms various existing
GNN models, especially on big data sets.","['cs.LG', 'stat.ML', '68T07, 05C85, 42C40', 'I.2.4; I.2.6']",174,128
Stereo Matching by Self-supervision of Multiscopic Vision,"Self-supervised learning for depth estimation possesses several advantages
over supervised learning. The benefits of no need for ground-truth depth,
online fine-tuning, and better generalization with unlimited data attract
researchers to seek self-supervised solutions. In this work, we propose a new
self-supervised framework for stereo matching utilizing multiple images
captured at aligned camera positions. A cross photometric loss, an
uncertainty-aware mutual-supervision loss, and a new smoothness loss are
introduced to optimize the network in learning disparity maps end-to-end
without ground-truth depth information. To train this framework, we build a new
multiscopic dataset consisting of synthetic images rendered by 3D engines and
real images captured by real cameras. After being trained with only the
synthetic images, our network can perform well in unseen outdoor scenes. Our
experiment shows that our model obtains better disparity maps than previous
unsupervised methods on the KITTI dataset and is comparable to supervised
methods when generalized to unseen data. Our source code and dataset are
available at https://sites.google.com/view/multiscopic.",['cs.CV'],176,119
Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning,"The learning of domain-invariant representations in the context of domain
adaptation with neural networks is considered. We propose a new regularization
method that minimizes the discrepancy between domain-specific latent feature
representations directly in the hidden activation space. Although some standard
distribution matching approaches exist that can be interpreted as the matching
of weighted sums of moments, e.g. Maximum Mean Discrepancy (MMD), an explicit
order-wise matching of higher order moments has not been considered before. We
propose to match the higher order central moments of probability distributions
by means of order-wise moment differences. Our model does not require
computationally expensive distance and kernel matrix computations. We utilize
the equivalent representation of probability distributions by moment sequences
to define a new distance function, called Central Moment Discrepancy (CMD). We
prove that CMD is a metric on the set of probability distributions on a compact
interval. We further prove that convergence of probability distributions on
compact intervals w.r.t. the new metric implies convergence in distribution of
the respective random variables. We test our approach on two different
benchmark data sets for object recognition (Office) and sentiment analysis of
product reviews (Amazon reviews). CMD achieves a new state-of-the-art
performance on most domain adaptation tasks of Office and outperforms networks
trained with MMD, Variational Fair Autoencoders and Domain Adversarial Neural
Networks on Amazon reviews. In addition, a post-hoc parameter sensitivity
analysis shows that the new approach is stable w.r.t. parameter changes in a
certain interval. The source code of the experiments is publicly available.","['stat.ML', 'cs.LG', '68T05']",262,154
An Overview on Data Representation Learning: From Traditional Feature Learning to Recent Deep Learning,"Since about 100 years ago, to learn the intrinsic structure of data, many
representation learning approaches have been proposed, including both linear
ones and nonlinear ones, supervised ones and unsupervised ones. Particularly,
deep architectures are widely applied for representation learning in recent
years, and have delivered top results in many tasks, such as image
classification, object detection and speech recognition. In this paper, we
review the development of data representation learning methods. Specifically,
we investigate both traditional feature learning algorithms and
state-of-the-art deep learning models. The history of data representation
learning is introduced, while available resources (e.g. online course, tutorial
and book information) and toolboxes are provided. Finally, we conclude this
paper with remarks and some interesting research directions on data
representation learning.","['cs.LG', 'stat.ML', '68T05']",127,88
Backprop-Free Reinforcement Learning with Active Neural Generative Coding,"In humans, perceptual awareness facilitates the fast recognition and
extraction of information from sensory input. This awareness largely depends on
how the human agent interacts with the environment. In this work, we propose
active neural generative coding, a computational framework for learning
action-driven generative models without backpropagation of errors (backprop) in
dynamic environments. Specifically, we develop an intelligent agent that
operates even with sparse rewards, drawing inspiration from the cognitive
theory of planning as inference. We demonstrate on several control problems, in
the online learning setting, that our proposed modeling framework performs
competitively with deep Q-learning models. The robust performance of our agent
offers promising evidence that a backprop-free approach for neural inference
and learning can drive goal-directed behavior.",['cs.LG'],123,91
Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding,"Despite deep convolutional neural networks achieved impressive progress in
medical image computing and analysis, its paradigm of supervised learning
demands a large number of annotations for training to avoid overfitting and
achieving promising results. In clinical practices, massive semantic
annotations are difficult to acquire in some conditions where specialized
biomedical expert knowledge is required, and it is also a common condition
where only few annotated classes are available. In this work, we proposed a
novel method for few-shot medical image segmentation, which enables a
segmentation model to fast generalize to an unseen class with few training
images. We construct our few-shot image segmentor using a deep convolutional
network trained episodically. Motivated by the spatial consistency and
regularity in medical images, we developed an efficient global correlation
module to capture the correlation between a support and query image and
incorporate it into the deep network called global correlation network.
Moreover, we enhance discriminability of deep embedding to encourage clustering
of the feature domains of the same class while keep the feature domains of
different organs far apart. Ablation Study proved the effectiveness of the
proposed global correlation module and discriminative embedding loss. Extensive
experiments on anatomical abdomen images on both CT and MRI modalities are
performed to demonstrate the state-of-the-art performance of our proposed
model.",['cs.CV'],219,137
Efficient Certification of Spatial Robustness,"Recent work has exposed the vulnerability of computer vision models to vector
field attacks. Due to the widespread usage of such models in safety-critical
applications, it is crucial to quantify their robustness against such spatial
transformations. However, existing work only provides empirical robustness
quantification against vector field deformations via adversarial attacks, which
lack provable guarantees. In this work, we propose novel convex relaxations,
enabling us, for the first time, to provide a certificate of robustness against
vector field transformations. Our relaxations are model-agnostic and can be
leveraged by a wide range of neural network verifiers. Experiments on various
network architectures and different datasets demonstrate the effectiveness and
scalability of our method.","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",113,84
A Deep Generative Model for Fragment-Based Molecule Generation,"Molecule generation is a challenging open problem in cheminformatics.
Currently, deep generative approaches addressing the challenge belong to two
broad categories, differing in how molecules are represented. One approach
encodes molecular graphs as strings of text, and learns their corresponding
character-based language model. Another, more expressive, approach operates
directly on the molecular graph. In this work, we address two limitations of
the former: generation of invalid and duplicate molecules. To improve validity
rates, we develop a language model for small molecular substructures called
fragments, loosely inspired by the well-known paradigm of Fragment-Based Drug
Design. In other words, we generate molecules fragment by fragment, instead of
atom by atom. To improve uniqueness rates, we present a frequency-based masking
strategy that helps generate molecules with infrequent fragments. We show
experimentally that our model largely outperforms other language model-based
competitors, reaching state-of-the-art performances typical of graph-based
approaches. Moreover, generated molecules display molecular properties similar
to those in the training sample, even in absence of explicit task-specific
supervision.","['stat.ML', 'cs.LG']",174,120
Color Orchestra: Ordering Color Palettes for Interpolation and Prediction,"Color theme or color palette can deeply influence the quality and the feeling
of a photograph or a graphical design. Although color palettes may come from
different sources such as online crowd-sourcing, photographs and graphical
designs, in this paper, we consider color palettes extracted from fine art
collections, which we believe to be an abundant source of stylistic and unique
color themes. We aim to capture color styles embedded in these collections by
means of statistical models and to build practical applications upon these
models. As artists often use their personal color themes in their paintings,
making these palettes appear frequently in the dataset, we employed density
estimation to capture the characteristics of palette data. Via density
estimation, we carried out various predictions and interpolations on palettes,
which led to promising applications such as photo-style exploration, real-time
color suggestion, and enriched photo recolorization. It was, however,
challenging to apply density estimation to palette data as palettes often come
as unordered sets of colors, which make it difficult to use conventional
metrics on them. To this end, we developed a divide-and-conquer sorting
algorithm to rearrange the colors in the palettes in a coherent order, which
allows meaningful interpolation between color palettes. To confirm the
performance of our model, we also conducted quantitative experiments on
datasets of digitized paintings collected from the Internet and received
favorable results.","['cs.CV', 'cs.GR']",230,140
Deep Reinforcement Learning for Autonomous Driving: A Survey,"With the development of deep representation learning, the domain of
reinforcement learning (RL) has become a powerful learning framework now
capable of learning complex policies in high dimensional environments. This
review summarises deep reinforcement learning (DRL) algorithms and provides a
taxonomy of automated driving tasks where (D)RL methods have been employed,
while addressing key computational challenges in real world deployment of
autonomous driving agents. It also delineates adjacent domains such as behavior
cloning, imitation learning, inverse reinforcement learning that are related
but are not classical RL algorithms. The role of simulators in training agents,
methods to validate, test and robustify existing solutions in RL are discussed.","['cs.LG', 'cs.AI', 'cs.RO']",107,78
Patch Slimming for Efficient Vision Transformers,"This paper studies the efficiency problem for visual transformers by
excavating redundant calculation in given networks. The recent transformer
architecture has demonstrated its effectiveness for achieving excellent
performance on a series of computer vision tasks. However, similar to that of
convolutional neural networks, the huge computational cost of vision
transformers is still a severe issue. Considering that the attention mechanism
aggregates different patches layer-by-layer, we present a novel patch slimming
approach that discards useless patches in a top-down paradigm. We first
identify the effective patches in the last layer and then use them to guide the
patch selection process of previous layers. For each layer, the impact of a
patch on the final output feature is approximated and patches with less impact
will be removed. Experimental results on benchmark datasets demonstrate that
the proposed method can significantly reduce the computational costs of vision
transformers without affecting their performances. For example, over 45% FLOPs
of the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the
ImageNet dataset.","['cs.CV', 'cs.LG']",176,122
Shared Representation Learning for Heterogeneous Face Recognition,"After intensive research, heterogenous face recognition is still a
challenging problem. The main difficulties are owing to the complex
relationship between heterogenous face image spaces. The heterogeneity is
always tightly coupled with other variations, which makes the relationship of
heterogenous face images highly nonlinear. Many excellent methods have been
proposed to model the nonlinear relationship, but they apt to overfit to the
training set, due to limited samples. Inspired by the unsupervised algorithms
in deep learning, this paper proposes an novel framework for heterogeneous face
recognition. We first extract Gabor features at some localized facial points,
and then use Restricted Boltzmann Machines (RBMs) to learn a shared
representation locally to remove the heterogeneity around each facial point.
Finally, the shared representations of local RBMs are connected together and
processed by PCA. Two problems (Sketch-Photo and NIR-VIS) and three databases
are selected to evaluate the proposed method. For Sketch-Photo problem, we
obtain perfect results on the CUFS database. For NIR-VIS problem, we produce
new state-of-the-art performance on the CASIA HFB and NIR-VIS 2.0 databases.",['cs.CV'],182,127
Airborne LiDAR Point Cloud Classification with Graph Attention Convolution Neural Network,"Airborne light detection and ranging (LiDAR) plays an increasingly
significant role in urban planning, topographic mapping, environmental
monitoring, power line detection and other fields thanks to its capability to
quickly acquire large-scale and high-precision ground information. To achieve
point cloud classification, previous studies proposed point cloud deep learning
models that can directly process raw point clouds based on PointNet-like
architectures. And some recent works proposed graph convolution neural network
based on the inherent topology of point clouds. However, the above point cloud
deep learning models only pay attention to exploring local geometric
structures, yet ignore global contextual relationships among all points. In
this paper, we present a graph attention convolution neural network (GACNN)
that can be directly applied to the classification of unstructured 3D point
clouds obtained by airborne LiDAR. Specifically, we first introduce a graph
attention convolution module that incorporates global contextual information
and local structural features. Based on the proposed graph attention
convolution module, we further design an end-to-end encoder-decoder network,
named GACNN, to capture multiscale features of the point clouds and therefore
enable more accurate airborne point cloud classification. Experiments on the
ISPRS 3D labeling dataset show that the proposed model achieves a new
state-of-the-art performance in terms of average F1 score (71.5\%) and a
satisfying overall accuracy (83.2\%). Additionally, experiments further
conducted on the 2019 Data Fusion Contest Dataset by comparing with other
prevalent point cloud deep learning models demonstrate the favorable
generalization capability of the proposed model.",['cs.CV'],253,155
White blood cell classification,"This paper proposes a novel automatic classification framework for the
recognition of five types of white blood cells. Segmenting complete white blood
cells from blood smears images and extracting advantageous features from them
remain challenging tasks in the classification of white blood cells. Therefore,
we present an adaptive threshold segmentation method to deal with blood smears
images with non-uniform color and uneven illumination, which is designed based
on color space information and threshold segmentation. Subsequently, after
successfully separating the white blood cell from the blood smear image, a
large number of nonlinear features including geometrical, color and texture
features are extracted. Nevertheless, redundant features can affect the
classification speed and efficiency, and in view of that, a feature selection
algorithm based on classification and regression trees (CART) is designed.
Through in-depth analysis of the nonlinear relationship between features, the
irrelevant and redundant features are successfully removed from the initial
nonlinear features. Afterwards, the selected prominent features are fed into
particle swarm optimization support vector machine (PSO-SVM) classifier to
recognize the types of the white blood cells. Finally, to evaluate the
performance of the proposed white blood cell classification methodology, we
build a white blood cell data set containing 500 blood smear images for
experiments. By comparing with the ground truth obtained manually, the proposed
segmentation method achieves an average of 95.98% and 97.57% dice similarity
for segmented nucleus and cell regions respectively. Furthermore, the proposed
methodology achieves 99.76% classification accuracy, which well demonstrates
its effectiveness.",['cs.CV'],251,143
FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling,"We investigate the use of Neural Radiance Fields (NeRF) to learn high quality
3D object category models from collections of input images. In contrast to
previous work, we are able to do this whilst simultaneously separating
foreground objects from their varying backgrounds. We achieve this via a
2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a
geometrically constant background and a deformable foreground that represents
the object category. We show that this method can learn accurate 3D object
category models using only photometric supervision and casually captured images
of the objects. Additionally, our 2-part decomposition allows the model to
perform accurate and crisp amodal segmentation. We quantitatively evaluate our
method with view synthesis and image fidelity metrics, using synthetic,
lab-captured, and in-the-wild data. Our results demonstrate convincing 3D
object category modelling that exceed the performance of existing methods.",['cs.CV'],147,97
GraLSP: Graph Neural Networks with Local Structural Patterns,"It is not until recently that graph neural networks (GNNs) are adopted to
perform graph representation learning, among which, those based on the
aggregation of features within the neighborhood of a node achieved great
success. However, despite such achievements, GNNs illustrate defects in
identifying some common structural patterns which, unfortunately, play
significant roles in various network phenomena. In this paper, we propose
GraLSP, a GNN framework which explicitly incorporates local structural patterns
into the neighborhood aggregation through random anonymous walks. Specifically,
we capture local graph structures via random anonymous walks, powerful and
flexible tools that represent structural patterns. The walks are then fed into
the feature aggregation, where we design various mechanisms to address the
impact of structural features, including adaptive receptive radius, attention
and amplification. In addition, we design objectives that capture similarities
between structures and are optimized jointly with node proximity objectives.
With the adequate leverage of structural patterns, our model is able to
outperform competitive counterparts in various prediction tasks in multiple
datasets.","['cs.LG', 'stat.ML']",166,112
Video action detection by learning graph-based spatio-temporal interactions,"Action Detection is a complex task that aims to detect and classify human
actions in video clips. Typically, it has been addressed by processing
fine-grained features extracted from a video classification backbone. Recently,
thanks to the robustness of object and people detectors, a deeper focus has
been added on relationship modelling. Following this line, we propose a
graph-based framework to learn high-level interactions between people and
objects, in both space and time. In our formulation, spatio-temporal
relationships are learned through self-attention on a multi-layer graph
structure which can connect entities from consecutive clips, thus considering
long-range spatial and temporal dependencies. The proposed module is backbone
independent by design and does not require end-to-end training. Extensive
experiments are conducted on the AVA dataset, where our model demonstrates
state-of-the-art results and consistent improvements over baselines built with
different backbones. Code is publicly available at
https://github.com/aimagelab/STAGE_action_detection.",['cs.CV'],159,125
Data-Mining Textual Responses to Uncover Misconception Patterns,"An important, yet largely unstudied, problem in student data analysis is to
detect misconceptions from students' responses to open-response questions.
Misconception detection enables instructors to deliver more targeted feedback
on the misconceptions exhibited by many students in their class, thus improving
the quality of instruction. In this paper, we propose a new natural language
processing-based framework to detect the common misconceptions among students'
textual responses to short-answer questions. We propose a probabilistic model
for students' textual responses involving misconceptions and experimentally
validate it on a real-world student-response dataset. Experimental results show
that our proposed framework excels at classifying whether a response exhibits
one or more misconceptions. More importantly, it can also automatically detect
the common misconceptions exhibited across responses from multiple students to
multiple questions; this property is especially important at large scale, since
instructors will no longer need to manually specify all possible misconceptions
that students might exhibit.","['stat.ML', 'cs.CL']",154,104
Scalable Attentive Sentence-Pair Modeling via Distilled Sentence Embedding,"Recent state-of-the-art natural language understanding models, such as BERT
and XLNet, score a pair of sentences (A and B) using multiple cross-attention
operations - a process in which each word in sentence A attends to all words in
sentence B and vice versa. As a result, computing the similarity between a
query sentence and a set of candidate sentences, requires the propagation of
all query-candidate sentence-pairs throughout a stack of cross-attention
layers. This exhaustive process becomes computationally prohibitive when the
number of candidate sentences is large. In contrast, sentence embedding
techniques learn a sentence-to-vector mapping and compute the similarity
between the sentence vectors via simple elementary operations. In this paper,
we introduce Distilled Sentence Embedding (DSE) - a model that is based on
knowledge distillation from cross-attentive models, focusing on sentence-pair
tasks. The outline of DSE is as follows: Given a cross-attentive teacher model
(e.g. a fine-tuned BERT), we train a sentence embedding based student model to
reconstruct the sentence-pair scores obtained by the teacher model. We
empirically demonstrate the effectiveness of DSE on five GLUE sentence-pair
tasks. DSE significantly outperforms several ELMO variants and other sentence
embedding methods, while accelerating computation of the query-candidate
sentence-pairs similarities by several orders of magnitude, with an average
relative degradation of 4.6% compared to BERT. Furthermore, we show that DSE
produces sentence embeddings that reach state-of-the-art performance on
universal sentence representation benchmarks. Our code is made publicly
available at https://github.com/microsoft/Distilled-Sentence-Embedding.","['cs.LG', 'cs.CL', 'stat.ML']",263,151
A Supervised Machine Learning Model For Imputing Missing Boarding Stops In Smart Card Data,"Public transport has become an essential part of urban existence with
increased population densities and environmental awareness. Large quantities of
data are currently generated, allowing for more robust methods to understand
travel behavior by harvesting smart card usage. However, public transport
datasets suffer from data integrity problems; boarding stop information may be
missing due to imperfect acquirement processes or inadequate reporting. We
developed a supervised machine learning method to impute missing boarding stops
based on ordinal classification using GTFS timetable, smart card, and
geospatial datasets. A new metric, Pareto Accuracy, is suggested to evaluate
algorithms where classes have an ordinal nature. Results are based on a case
study in the city of Beer Sheva, Israel, consisting of one month of smart card
data. We show that our proposed method is robust to irregular travelers and
significantly outperforms well-known imputation methods without the need to
mine any additional datasets. Validation of data from another Israeli city
using transfer learning shows the presented model is general and context-free.
The implications for transportation planning and travel behavior research are
further discussed.","['cs.LG', 'stat.ML']",180,132
DeepMI: A Mutual Information Based Framework For Unsupervised Deep Learning of Tasks,"In this work, we propose an information theory based framework DeepMI to
train deep neural networks (DNN) using Mutual Information (MI). The DeepMI
framework is especially targeted but not limited to the learning of real world
tasks in an unsupervised manner. The primary motivation behind this work is the
insufficiency of traditional loss functions for unsupervised task learning.
Moreover, directly using MI for the training purpose is quite challenging to
deal because of its unbounded above nature. Hence, we develop an alternative
linearized representation of MI as a part of the framework. Contributions of
this paper are three fold: i) investigation of MI to train deep neural
networks, ii) novel loss function LLMI, and iii) a fuzzy logic based end-to-end
differentiable pipeline to integrate DeepMI into deep learning framework. We
choose a few unsupervised learning tasks for our experimental study. We
demonstrate that L LM I alone provides better gradients to achieve a neural
network better performance over the cases when multiple loss functions are used
for a given task.","['cs.CV', 'cs.RO']",172,110
Transferable Pedestrian Motion Prediction Models at Intersections,"One desirable capability of autonomous cars is to accurately predict the
pedestrian motion near intersections for safe and efficient trajectory
planning. We are interested in developing transfer learning algorithms that can
be trained on the pedestrian trajectories collected at one intersection and yet
still provide accurate predictions of the trajectories at another, previously
unseen intersection. We first discussed the feature selection for transferable
pedestrian motion models in general. Following this discussion, we developed
one transferable pedestrian motion prediction algorithm based on Inverse
Reinforcement Learning (IRL) that infers pedestrian intentions and predicts
future trajectories based on observed trajectory. We evaluated our algorithm on
a dataset collected at two intersections, trained at one intersection and
tested at the other intersection. We used the accuracy of augmented
semi-nonnegative sparse coding (ASNSC), trained and tested at the same
intersection as a baseline. The result shows that the proposed algorithm
improves the baseline accuracy by 40% in the non-transfer task, and 16% in the
transfer task.","['cs.CV', 'cs.LG']",163,99
Minibatch optimal transport distances; analysis and applications,"Optimal transport distances have become a classic tool to compare probability
distributions and have found many applications in machine learning. Yet,
despite recent algorithmic developments, their complexity prevents their direct
use on large scale datasets. To overcome this challenge, a common workaround is
to compute these distances on minibatches i.e. to average the outcome of
several smaller optimal transport problems. We propose in this paper an
extended analysis of this practice, which effects were previously studied in
restricted cases. We first consider a large variety of Optimal Transport
kernels. We notably argue that the minibatch strategy comes with appealing
properties such as unbiased estimators, gradients and a concentration bound
around the expectation, but also with limits: the minibatch OT is not a
distance. To recover some of the lost distance axioms, we introduce a debiased
minibatch OT function and study its statistical and optimisation properties.
Along with this theoretical analysis, we also conduct empirical experiments on
gradient flows, generative adversarial networks (GANs) or color transfer that
highlight the practical interest of this strategy.","['stat.ML', 'cs.LG']",174,125
Fast and Robust Archetypal Analysis for Representation Learning,"We revisit a pioneer unsupervised learning technique called archetypal
analysis, which is related to successful data analysis methods such as sparse
coding and non-negative matrix factorization. Since it was proposed, archetypal
analysis did not gain a lot of popularity even though it produces more
interpretable models than other alternatives. Because no efficient
implementation has ever been made publicly available, its application to
important scientific problems may have been severely limited. Our goal is to
bring back into favour archetypal analysis. We propose a fast optimization
scheme using an active-set strategy, and provide an efficient open-source
implementation interfaced with Matlab, R, and Python. Then, we demonstrate the
usefulness of archetypal analysis for computer vision tasks, such as codebook
learning, signal classification, and large image collection visualization.","['cs.CV', 'cs.LG', 'stat.ML']",128,103
Learning Target-oriented Dual Attention for Robust RGB-T Tracking,"RGB-Thermal object tracking attempt to locate target object using
complementary visual and thermal infrared data. Existing RGB-T trackers fuse
different modalities by robust feature representation learning or adaptive
modal weighting. However, how to integrate dual attention mechanism for visual
tracking is still a subject that has not been studied yet. In this paper, we
propose two visual attention mechanisms for robust RGB-T object tracking.
Specifically, the local attention is implemented by exploiting the common
visual attention of RGB and thermal data to train deep classifiers. We also
introduce the global attention, which is a multi-modal target-driven attention
estimation network. It can provide global proposals for the classifier together
with local proposals extracted from previous tracking result. Extensive
experiments on two RGB-T benchmark datasets validated the effectiveness of our
proposed algorithm.",['cs.CV'],136,94
Similarities between policy gradient methods (PGM) in Reinforcement learning (RL) and supervised learning (SL),"Reinforcement learning (RL) is about sequential decision making and is
traditionally opposed to supervised learning (SL) and unsupervised learning
(USL). In RL, given the current state, the agent makes a decision that may
influence the next state as opposed to SL (and USL) where, the next state
remains the same, regardless of the decisions taken, either in batch or online
learning. Although this difference is fundamental between SL and RL, there are
connections that have been overlooked. In particular, we prove in this paper
that gradient policy method can be cast as a supervised learning problem where
true label are replaced with discounted rewards. We provide a new proof of
policy gradient methods (PGM) that emphasizes the tight link with the cross
entropy and supervised learning. We provide a simple experiment where we
interchange label and pseudo rewards. We conclude that other relationships with
SL could be made if we modify the reward functions wisely.","['cs.LG', 'cs.AI', 'stat.ML']",155,94
Adversarial Attacks on Multivariate Time Series,"Classification models for the multivariate time series have gained
significant importance in the research community, but not much research has
been done on generating adversarial samples for these models. Such samples of
adversaries could become a security concern. In this paper, we propose
transforming the existing adversarial transformation network (ATN) on a
distilled model to attack various multivariate time series classification
models. The proposed attack on the classification model utilizes a distilled
model as a surrogate that mimics the behavior of the attacked classical
multivariate time series classification models. The proposed methodology is
tested onto 1-Nearest Neighbor Dynamic Time Warping (1-NN DTW) and a Fully
Convolutional Network (FCN), all of which are trained on 18 University of East
Anglia (UEA) and University of California Riverside (UCR) datasets. We show
both models were susceptible to attacks on all 18 datasets. To the best of our
knowledge, adversarial attacks have only been conducted in the domain of
univariate time series and have not been conducted on multivariate time series.
such an attack on time series classification models has never been done before.
Additionally, we recommend future researchers that develop time series
classification models to incorporating adversarial data samples into their
training data sets to improve resilience on adversarial samples and to consider
model robustness as an evaluative metric.","['cs.LG', 'cs.CR', 'stat.ML']",218,124
GNNGuard: Defending Graph Neural Networks against Adversarial Attacks,"Deep learning methods for graphs achieve remarkable performance across a
variety of domains. However, recent findings indicate that small, unnoticeable
perturbations of graph structure can catastrophically reduce performance of
even the strongest and most popular Graph Neural Networks (GNNs). Here, we
develop GNNGuard, a general algorithm to defend against a variety of
training-time attacks that perturb the discrete graph structure. GNNGuard can
be straight-forwardly incorporated into any GNN. Its core principle is to
detect and quantify the relationship between the graph structure and node
features, if one exists, and then exploit that relationship to mitigate
negative effects of the attack.GNNGuard learns how to best assign higher
weights to edges connecting similar nodes while pruning edges between unrelated
nodes. The revised edges allow for robust propagation of neural messages in the
underlying GNN. GNNGuard introduces two novel components, the neighbor
importance estimation, and the layer-wise graph memory, and we show empirically
that both components are necessary for a successful defense. Across five GNNs,
three defense methods, and five datasets,including a challenging human disease
graph, experiments show that GNNGuard outperforms existing defense approaches
by 15.3% on average. Remarkably, GNNGuard can effectively restore
state-of-the-art performance of GNNs in the face of various adversarial
attacks, including targeted and non-targeted attacks, and can defend against
attacks on heterophily graphs.","['cs.LG', 'stat.ML']",224,143
The Cube++ Illumination Estimation Dataset,"Computational color constancy has the important task of reducing the
influence of the scene illumination on the object colors. As such, it is an
essential part of the image processing pipelines of most digital cameras. One
of the important parts of the computational color constancy is illumination
estimation, i.e. estimating the illumination color. When an illumination
estimation method is proposed, its accuracy is usually reported by providing
the values of error metrics obtained on the images of publicly available
datasets. However, over time it has been shown that many of these datasets have
problems such as too few images, inappropriate image quality, lack of scene
diversity, absence of version tracking, violation of various assumptions, GDPR
regulation violation, lack of additional shooting procedure info, etc. In this
paper, a new illumination estimation dataset is proposed that aims to alleviate
many of the mentioned problems and to help the illumination estimation
research. It consists of 4890 images with known illumination colors as well as
with additional semantic data that can further make the learning process more
accurate. Due to the usage of the SpyderCube color target, for every image
there are two ground-truth illumination records covering different directions.
Because of that, the dataset can be used for training and testing of methods
that perform single or two-illuminant estimation. This makes it superior to
many similar existing datasets. The datasets, it's smaller version
SimpleCube++, and the accompanying code are available at
https://github.com/Visillect/CubePlusPlus/.",['cs.CV'],247,152
Parametric generation of conditional geological realizations using generative neural networks,"Deep learning techniques are increasingly being considered for geological
applications where -- much like in computer vision -- the challenges are
characterized by high-dimensional spatial data dominated by multipoint
statistics. In particular, a novel technique called generative adversarial
networks has been recently studied for geological parametrization and
synthesis, obtaining very impressive results that are at least qualitatively
competitive with previous methods. The method obtains a neural network
parametrization of the geology -- so-called a generator -- that is capable of
reproducing very complex geological patterns with dimensionality reduction of
several orders of magnitude. Subsequent works have addressed the conditioning
task, i.e. using the generator to generate realizations honoring spatial
observations (hard data). The current approaches, however, do not provide a
parametrization of the conditional generation process. In this work, we propose
a method to obtain a parametrization for direct generation of conditional
realizations. The main idea is to simply extend the existing generator network
by stacking a second inference network that learns to perform the conditioning.
This inference network is a neural network trained to sample a posterior
distribution derived using a Bayesian formulation of the conditioning task. The
resulting extended neural network thus provides the conditional
parametrization. Our method is assessed on a benchmark image of binary
channelized subsurface, obtaining very promising results for a wide variety of
conditioning configurations.","['stat.ML', 'cs.LG', 'physics.comp-ph']",220,138
The 2021 Image Similarity Dataset and Challenge,"This paper introduces a new benchmark for large-scale image similarity
detection. This benchmark is used for the Image Similarity Challenge at
NeurIPS'21 (ISC2021). The goal is to determine whether a query image is a
modified copy of any image in a reference corpus of size 1~million. The
benchmark features a variety of image transformations such as automated
transformations, hand-crafted image edits and machine-learning based
manipulations. This mimics real-life cases appearing in social media, for
example for integrity-related problems dealing with misinformation and
objectionable content. The strength of the image manipulations, and therefore
the difficulty of the benchmark, is calibrated according to the performance of
a set of baseline approaches. Both the query and reference set contain a
majority of ""distractor"" images that do not match, which corresponds to a
real-life needle-in-haystack setting, and the evaluation metric reflects that.
We expect the DISC21 benchmark to promote image copy detection as an important
and challenging computer vision task and refresh the state of the art.",['cs.CV'],173,107
Generative Flows with Invertible Attentions,"Flow-based generative models have shown excellent ability to explicitly learn
the probability density function of data via a sequence of invertible
transformations. Yet, modeling long-range dependencies over normalizing flows
remains understudied. To fill the gap, in this paper, we introduce two types of
invertible attention mechanisms for generative flow models. To be precise, we
propose map-based and scaled dot-product attention for unconditional and
conditional generative flow models. The key idea is to exploit split-based
attention mechanisms to learn the attention weights and input representations
on every two splits of flow feature maps. Our method provides invertible
attention modules with tractable Jacobian determinants, enabling seamless
integration of it at any positions of the flow-based models. The proposed
attention mechanism can model the global data dependencies, leading to more
comprehensive flow models. Evaluation on multiple generation tasks demonstrates
that the introduced attention flow idea results in efficient flow models and
compares favorably against the state-of-the-art unconditional and conditional
generative flow methods.","['cs.LG', 'cs.CV']",168,108
Biologically inspired architectures for sample-efficient deep reinforcement learning,"Deep reinforcement learning requires a heavy price in terms of sample
efficiency and overparameterization in the neural networks used for function
approximation. In this work, we use tensor factorization in order to learn more
compact representation for reinforcement learning policies. We show empirically
that in the low-data regime, it is possible to learn online policies with 2 to
10 times less total coefficients, with little to no loss of performance. We
also leverage progress in second order optimization, and use the theory of
wavelet scattering to further reduce the number of learned coefficients, by
foregoing learning the topmost convolutional layer filters altogether. We
evaluate our results on the Atari suite against recent baseline algorithms that
represent the state-of-the-art in data efficiency, and get comparable results
with an order of magnitude gain in weight parsimony.","['cs.LG', 'cs.NE', 'stat.ML']",138,96
Active Reinforcement Learning over MDPs,"The past decade has seen the rapid development of Reinforcement Learning,
which acquires impressive performance with numerous training resources.
However, one of the greatest challenges in RL is generalization efficiency
(i.e., generalization performance in a unit time). This paper proposes a
framework of Active Reinforcement Learning (ARL) over MDPs to improve
generalization efficiency in a limited resource by instance selection. Given a
number of instances, the algorithm chooses out valuable instances as training
sets while training the policy, thereby costing fewer resources. Unlike
existing approaches, we attempt to actively select and use training data rather
than train on all the given data, thereby costing fewer resources. Furthermore,
we introduce a general instance evaluation metrics and selection mechanism into
the framework. Experiments results reveal that the proposed framework with
Proximal Policy Optimization as policy optimizer can effectively improve
generalization efficiency than unselect-ed and unbiased selected methods.",['cs.LG'],147,101
Learning to Label Affordances from Simulated and Real Data,"An autonomous robot should be able to evaluate the affordances that are
offered by a given situation. Here we address this problem by designing a
system that can densely predict affordances given only a single 2D RGB image.
This is achieved with a convolutional neural network (ResNet), which we combine
with refinement modules recently proposed for addressing semantic image
segmentation. We define a novel cost function, which is able to handle
(potentially multiple) affordances of objects and their parts in a pixel-wise
manner even in the case of incomplete data. We perform qualitative as well as
quantitative evaluations with simulated and real data assessing 15 different
affordances. In general, we find that affordances, which are well-enough
represented in the training data, are correctly recognized with a substantial
fraction of correctly assigned pixels. Furthermore, we show that our model
outperforms several baselines. Hence, this method can give clear action
guidelines for a robot.","['cs.CV', 'cs.RO']",154,107
Automatic Ship Classification Utilizing Bag of Deep Features,"Detection and classification of ships based on their silhouette profiles in
natural imagery is an important undertaking in computer science. This problem
can be viewed from a variety of perspectives, including security, traffic
control, and even militarism. Therefore, in each of the aforementioned
applications, specific processing is required. In this paper, by applying the
""bag of words"" (BoW), a new method is presented that its words are the features
that are obtained using pre-trained models of deep convolutional networks. ,
Three VGG models are utilized which provide superior accuracy in identifying
objects. The regions of the image that are selected as the initial proposals
are derived from a greedy algorithm on the key points generated by the Scale
Invariant Feature Transform (SIFT) method. Using the deep features in the BOW
method provides a good improvement in the recognition and classification of
ships. Eventually, we obtained an accuracy of 91.8% in the classification of
the ships which shows the improvement of about 5% compared to previous methods.",['cs.CV'],167,109
Adversarial Graph Augmentation to Improve Graph Contrastive Learning,"Self-supervised learning of graph neural networks (GNN) is in great need
because of the widespread label scarcity issue in real-world graph/network
data. Graph contrastive learning (GCL), by training GNNs to maximize the
correspondence between the representations of the same graph in its different
augmented forms, may yield robust and transferable GNNs even without using
labels. However, GNNs trained by traditional GCL often risk capturing redundant
graph features and thus may be brittle and provide sub-par performance in
downstream tasks. Here, we propose a novel principle, termed adversarial-GCL
(AD-GCL), which enables GNNs to avoid capturing redundant information during
the training by optimizing adversarial graph augmentation strategies used in
GCL. We pair AD-GCL with theoretical explanations and design a practical
instantiation based on trainable edge-dropping graph augmentation. We
experimentally validate AD-GCL by comparing with the state-of-the-art GCL
methods and achieve performance gains of up-to $14\%$ in unsupervised, $6\%$ in
transfer, and $3\%$ in semi-supervised learning settings overall with 18
different benchmark datasets for the tasks of molecule property regression and
classification, and social network classification.","['cs.LG', 'cs.AI']",187,121
Cycle-IR: Deep Cyclic Image Retargeting,"Supervised deep learning techniques have achieved great success in various
fields due to getting rid of the limitation of handcrafted representations.
However, most previous image retargeting algorithms still employ fixed design
principles such as using gradient map or handcrafted features to compute
saliency map, which inevitably restricts its generality. Deep learning
techniques may help to address this issue, but the challenging problem is that
we need to build a large-scale image retargeting dataset for the training of
deep retargeting models. However, building such a dataset requires huge human
efforts.
  In this paper, we propose a novel deep cyclic image retargeting approach,
called Cycle-IR, to firstly implement image retargeting with a single deep
model, without relying on any explicit user annotations. Our idea is built on
the reverse mapping from the retargeted images to the given input images. If
the retargeted image has serious distortion or excessive loss of important
visual information, the reverse mapping is unlikely to restore the input image
well. We constrain this forward-reverse consistency by introducing a cyclic
perception coherence loss. In addition, we propose a simple yet effective image
retargeting network (IRNet) to implement the image retargeting process. Our
IRNet contains a spatial and channel attention layer, which is able to
discriminate visually important regions of input images effectively, especially
in cluttered images. Given arbitrary sizes of input images and desired aspect
ratios, our Cycle-IR can produce visually pleasing target images directly.
Extensive experiments on the standard RetargetMe dataset show the superiority
of our Cycle-IR. In addition, our Cycle-IR outperforms the Multiop method and
obtains the best result in the user study. Code is available at
https://github.com/mintanwei/Cycle-IR.",['cs.CV'],282,174
Deep Appearance Models: A Deep Boltzmann Machine Approach for Face Modeling,"The ""interpretation through synthesis"" approach to analyze face images,
particularly Active Appearance Models (AAMs) method, has become one of the most
successful face modeling approaches over the last two decades. AAM models have
ability to represent face images through synthesis using a controllable
parameterized Principal Component Analysis (PCA) model. However, the accuracy
and robustness of the synthesized faces of AAM are highly depended on the
training sets and inherently on the generalizability of PCA subspaces. This
paper presents a novel Deep Appearance Models (DAMs) approach, an efficient
replacement for AAMs, to accurately capture both shape and texture of face
images under large variations. In this approach, three crucial components
represented in hierarchical layers are modeled using the Deep Boltzmann
Machines (DBM) to robustly capture the variations of facial shapes and
appearances. DAMs are therefore superior to AAMs in inferencing a
representation for new face images under various challenging conditions. The
proposed approach is evaluated in various applications to demonstrate its
robustness and capabilities, i.e. facial super-resolution reconstruction,
facial off-angle reconstruction or face frontalization, facial occlusion
removal and age estimation using challenging face databases, i.e. Labeled Face
Parts in the Wild (LFPW), Helen and FG-NET. Comparing to AAMs and other deep
learning based approaches, the proposed DAMs achieve competitive results in
those applications, thus this showed their advantages in handling occlusions,
facial representation, and reconstruction.",['cs.CV'],229,144
Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows,"Time series forecasting is often fundamental to scientific and engineering
problems and enables decision making. With ever increasing data set sizes, a
trivial solution to scale up predictions is to assume independence between
interacting time series. However, modeling statistical dependencies can improve
accuracy and enable analysis of interaction effects. Deep learning methods are
well suited for this problem, but multivariate models often assume a simple
parametric distribution and do not scale to high dimensions. In this work we
model the multivariate temporal dynamics of time series via an autoregressive
deep learning model, where the data distribution is represented by a
conditioned normalizing flow. This combination retains the power of
autoregressive models, such as good performance in extrapolation into the
future, with the flexibility of flows as a general purpose high-dimensional
distribution model, while remaining computationally tractable. We show that it
improves over the state-of-the-art for standard metrics on many real-world data
sets with several thousand interacting time-series.","['cs.LG', 'stat.ML']",163,118
H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes,"Liver cancer is one of the leading causes of cancer death. To assist doctors
in hepatocellular carcinoma diagnosis and treatment planning, an accurate and
automatic liver and tumor segmentation method is highly demanded in the
clinical practice. Recently, fully convolutional neural networks (FCNs),
including 2D and 3D FCNs, serve as the back-bone in many volumetric image
segmentation. However, 2D convolutions can not fully leverage the spatial
information along the third dimension while 3D convolutions suffer from high
computational cost and GPU memory consumption. To address these issues, we
propose a novel hybrid densely connected UNet (H-DenseUNet), which consists of
a 2D DenseUNet for efficiently extracting intra-slice features and a 3D
counterpart for hierarchically aggregating volumetric contexts under the spirit
of the auto-context algorithm for liver and tumor segmentation. We formulate
the learning process of H-DenseUNet in an end-to-end manner, where the
intra-slice representations and inter-slice features can be jointly optimized
through a hybrid feature fusion (HFF) layer. We extensively evaluated our
method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge
and 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on the
segmentation results of tumors and achieved very competitive performance for
liver segmentation even with a single model.",['cs.CV'],212,141
Escaping the Gradient Vanishing: Periodic Alternatives of Softmax in Attention Mechanism,"Softmax is widely used in neural networks for multiclass classification, gate
structure and attention mechanisms. The statistical assumption that the input
is normal distributed supports the gradient stability of Softmax. However, when
used in attention mechanisms such as transformers, since the correlation scores
between embeddings are often not normally distributed, the gradient vanishing
problem appears, and we prove this point through experimental confirmation. In
this work, we suggest that replacing the exponential function by periodic
functions, and we delve into some potential periodic alternatives of Softmax
from the view of value and gradient. Through experiments on a simply designed
demo referenced to LeViT, our method is proved to be able to alleviate the
gradient problem and yield substantial improvements compared to Softmax and its
variants. Further, we analyze the impact of pre-normalization for Softmax and
our methods through mathematics and experiments. Lastly, we increase the depth
of the demo and prove the applicability of our method in deep structures.","['cs.CV', 'cs.LG']",160,103
InfoFocus: 3D Object Detection for Autonomous Driving with Dynamic Information Modeling,"Real-time 3D object detection is crucial for autonomous cars. Achieving
promising performance with high efficiency, voxel-based approaches have
received considerable attention. However, previous methods model the input
space with features extracted from equally divided sub-regions without
considering that point cloud is generally non-uniformly distributed over the
space. To address this issue, we propose a novel 3D object detection framework
with dynamic information modeling. The proposed framework is designed in a
coarse-to-fine manner. Coarse predictions are generated in the first stage via
a voxel-based region proposal network. We introduce InfoFocus, which improves
the coarse detections by adaptively refining features guided by the information
of point cloud density. Experiments are conducted on the large-scale nuScenes
3D detection benchmark. Results show that our framework achieves the
state-of-the-art performance with 31 FPS and improves our baseline
significantly by 9.0% mAP on the nuScenes test set.",['cs.CV'],153,112
Heredity-aware Child Face Image Generation with Latent Space Disentanglement,"Generative adversarial networks have been widely used in image synthesis in
recent years and the quality of the generated image has been greatly improved.
However, the flexibility to control and decouple facial attributes (e.g., eyes,
nose, mouth) is still limited. In this paper, we propose a novel approach,
called ChildGAN, to generate a child's image according to the images of parents
with heredity prior. The main idea is to disentangle the latent space of a
pre-trained generation model and precisely control the face attributes of child
images with clear semantics. We use distances between face landmarks as pseudo
labels to figure out the most influential semantic vectors of the corresponding
face attributes by calculating the gradient of latent vectors to pseudo labels.
Furthermore, we disentangle the semantic vectors by weighting irrelevant
features and orthogonalizing them with Schmidt Orthogonalization. Finally, we
fuse the latent vector of the parents by leveraging the disentangled semantic
vectors under the guidance of biological genetic laws. Extensive experiments
demonstrate that our approach outperforms the existing methods with encouraging
results.",['cs.CV'],176,114
Video Influencers: Unboxing the Mystique,"Influencer marketing is being used increasingly as a tool to reach customers
because of the growing popularity of social media stars who primarily reach
their audience(s) via custom videos. Despite the rapid growth in influencer
marketing, there has been little research on the design and effectiveness of
influencer videos. Using publicly available data on YouTube influencer videos,
we implement novel interpretable deep learning architectures, supported by
transfer learning, to identify significant relationships between advertising
content in videos (across text, audio, and images) and video views, interaction
rates and sentiment. By avoiding ex-ante feature engineering and instead using
ex-post interpretation, our approach avoids making a trade-off between
interpretability and predictive ability. We filter out relationships that are
affected by confounding factors unassociated with an increase in attention to
video elements, thus facilitating the generation of plausible causal
relationships between video elements and marketing outcomes which can be tested
in the field. A key finding is that brand mentions in the first 30 seconds of a
video are on average associated with a significant increase in attention to the
brand but a significant decrease in sentiment expressed towards the video. We
illustrate the learnings from our approach for both influencers and brands.","['cs.LG', 'cs.CL', 'cs.CV', 'cs.SD', 'eess.AS']",204,137
Closing the Generalization Gap in One-Shot Object Detection,"Despite substantial progress in object detection and few-shot learning,
detecting objects based on a single example - one-shot object detection -
remains a challenge: trained models exhibit a substantial generalization gap,
where object categories used during training are detected much more reliably
than novel ones. Here we show that this generalization gap can be nearly closed
by increasing the number of object categories used during training. Our results
show that the models switch from memorizing individual categories to learning
object similarity over the category distribution, enabling strong
generalization at test time. Importantly, in this regime standard methods to
improve object detection models like stronger backbones or longer training
schedules also benefit novel categories, which was not the case for smaller
datasets like COCO. Our results suggest that the key to strong few-shot
detection models may not lie in sophisticated metric learning approaches, but
instead in scaling the number of categories. Future data annotation efforts
should therefore focus on wider datasets and annotate a larger number of
categories rather than gathering more images or instances per category.","['cs.CV', 'cs.LG']",177,114
Ranking Neural Checkpoints,"This paper is concerned with ranking many pre-trained deep neural networks
(DNNs), called checkpoints, for the transfer learning to a downstream task.
Thanks to the broad use of DNNs, we may easily collect hundreds of checkpoints
from various sources. Which of them transfers the best to our downstream task
of interest? Striving to answer this question thoroughly, we establish a neural
checkpoint ranking benchmark (NeuCRaB) and study some intuitive ranking
measures. These measures are generic, applying to the checkpoints of different
output types without knowing how the checkpoints are pre-trained on which
dataset. They also incur low computation cost, making them practically
meaningful. Our results suggest that the linear separability of the features
extracted by the checkpoints is a strong indicator of transferability. We also
arrive at a new ranking measure, NLEEP, which gives rise to the best
performance in the experiments.","['cs.LG', 'cs.CV']",144,100
SharinGAN: Combining Synthetic and Real Data for Unsupervised Geometry Estimation,"We propose a novel method for combining synthetic and real images when
training networks to determine geometric information from a single image. We
suggest a method for mapping both image types into a single, shared domain.
This is connected to a primary network for end-to-end training. Ideally, this
results in images from two domains that present shared information to the
primary network. Our experiments demonstrate significant improvements over the
state-of-the-art in two important domains, surface normal estimation of human
faces and monocular depth estimation for outdoor scenes, both in an
unsupervised setting.",['cs.CV'],97,65
Learning Topology from Synthetic Data for Unsupervised Depth Completion,"We present a method for inferring dense depth maps from images and sparse
depth measurements by leveraging synthetic data to learn the association of
sparse point clouds with dense natural shapes, and using the image as evidence
to validate the predicted depth map. Our learned prior for natural shapes uses
only sparse depth as input, not images, so the method is not affected by the
covariate shift when attempting to transfer learned models from synthetic data
to real ones. This allows us to use abundant synthetic data with ground truth
to learn the most difficult component of the reconstruction process, which is
topology estimation, and use the image to refine the prediction based on
photometric evidence. Our approach uses fewer parameters than previous methods,
yet, achieves the state of the art on both indoor and outdoor benchmark
datasets. Code available at:
https://github.com/alexklwong/learning-topology-synthetic-data.","['cs.CV', 'cs.LG', 'cs.RO']",149,96
Entangled Kernels -- Beyond Separability,"We consider the problem of operator-valued kernel learning and investigate
the possibility of going beyond the well-known separable kernels. Borrowing
tools and concepts from the field of quantum computing, such as partial trace
and entanglement, we propose a new view on operator-valued kernels and define a
general family of kernels that encompasses previously known operator-valued
kernels, including separable and transformable kernels. Within this framework,
we introduce another novel class of operator-valued kernels called entangled
kernels that are not separable. We propose an efficient two-step algorithm for
this framework, where the entangled kernel is learned based on a novel
extension of kernel alignment to operator-valued kernels. We illustrate our
algorithm with an application to supervised dimensionality reduction, and
demonstrate its effectiveness with both artificial and real data for
multi-output regression.","['cs.LG', 'quant-ph', 'stat.ML']",137,85
A Bandit Framework for Optimal Selection of Reinforcement Learning Agents,"Deep Reinforcement Learning has been shown to be very successful in complex
games, e.g. Atari or Go. These games have clearly defined rules, and hence
allow simulation. In many practical applications, however, interactions with
the environment are costly and a good simulator of the environment is not
available. Further, as environments differ by application, the optimal
inductive bias (architecture, hyperparameters, etc.) of a reinforcement agent
depends on the application. In this work, we propose a multi-arm bandit
framework that selects from a set of different reinforcement learning agents to
choose the one with the best inductive bias. To alleviate the problem of sparse
rewards, the reinforcement learning agents are augmented with surrogate
rewards. This helps the bandit framework to select the best agents early, since
these rewards are smoother and less sparse than the environment reward. The
bandit has the double objective of maximizing the reward while the agents are
learning and selecting the best agent after a finite number of learning steps.
Our experimental results on standard environments show that the proposed
framework is able to consistently select the optimal agent after a finite
number of steps, while collecting more cumulative reward compared to selecting
a sub-optimal architecture or uniformly alternating between different agents.","['cs.LG', 'stat.ML']",208,122
SigGPDE: Scaling Sparse Gaussian Processes on Sequential Data,"Making predictions and quantifying their uncertainty when the input data is
sequential is a fundamental learning challenge, recently attracting increasing
attention. We develop SigGPDE, a new scalable sparse variational inference
framework for Gaussian Processes (GPs) on sequential data. Our contribution is
twofold. First, we construct inducing variables underpinning the sparse
approximation so that the resulting evidence lower bound (ELBO) does not
require any matrix inversion. Second, we show that the gradients of the GP
signature kernel are solutions of a hyperbolic partial differential equation
(PDE). This theoretical insight allows us to build an efficient
back-propagation algorithm to optimize the ELBO. We showcase the significant
computational gains of SigGPDE compared to existing methods, while achieving
state-of-the-art performance for classification tasks on large datasets of up
to 1 million multivariate time series.","['stat.ML', 'cs.LG', '60L10 (Primary) 60L20 (Secondary)']",134,106
Efficient 2D and 3D Facade Segmentation using Auto-Context,"This paper introduces a fast and efficient segmentation technique for 2D
images and 3D point clouds of building facades. Facades of buildings are highly
structured and consequently most methods that have been proposed for this
problem aim to make use of this strong prior information. Contrary to most
prior work, we are describing a system that is almost domain independent and
consists of standard segmentation methods. We train a sequence of boosted
decision trees using auto-context features. This is learned using stacked
generalization. We find that this technique performs better, or comparable with
all previous published methods and present empirical results on all available
2D and 3D facade benchmark datasets. The proposed method is simple to
implement, easy to extend, and very efficient at test-time inference.",['cs.CV'],128,91
Adversarial Learned Molecular Graph Inference and Generation,"Recent methods for generating novel molecules use graph representations of
molecules and employ various forms of graph convolutional neural networks for
inference. However, training requires solving an expensive graph isomorphism
problem, which previous approaches do not address or solve only approximately.
In this work, we propose ALMGIG, a likelihood-free adversarial learning
framework for inference and de novo molecule generation that avoids explicitly
computing a reconstruction loss. Our approach extends generative adversarial
networks by including an adversarial cycle-consistency loss to implicitly
enforce the reconstruction property. To capture properties unique to molecules,
such as valence, we extend the Graph Isomorphism Network to multi-graphs. To
quantify the performance of models, we propose to compute the distance between
distributions of physicochemical properties with the 1-Wasserstein distance. We
demonstrate that ALMGIG more accurately learns the distribution over the space
of molecules than all baselines. Moreover, it can be utilized for drug
discovery by efficiently searching the space of molecules using molecules'
continuous latent representation. Our code is available at
https://github.com/ai-med/almgig","['cs.LG', 'stat.ML']",174,129
Applying recent advances in Visual Question Answering to Record Linkage,"Multi-modal Record Linkage is the process of matching multi-modal records
from multiple sources that represent the same entity. This field has not been
explored in research and we propose two solutions based on Deep Learning
architectures that are inspired by recent work in Visual Question Answering.
The neural networks we propose use two different fusion modules, the Recurrent
Neural Network + Convolutional Neural Network fusion module and the Stacked
Attention Network fusion module, that jointly combine the visual and the
textual data of the records. The output of these fusion models is the input of
a Siamese Neural Network that computes the similarity of the records. Using
data from the Avito Duplicate Advertisements Detection dataset, we train these
solutions and from the experiments, we concluded that the Recurrent Neural
Network + Convolutional Neural Network fusion module outperforms a simple model
that uses hand-crafted features. We also find that the Recurrent Neural Network
+ Convolutional Neural Network fusion module classifies dissimilar
advertisements as similar more frequently if their average description is
bigger than 40 words. We conclude that the reason for this is that the longer
advertisements have a different distribution then the shorter advertisements
who are more prevalent in the dataset. In the end, we also conclude that
further research needs to be done with the Stacked Attention Network, to
further explore the effects of the visual data on the performance of the fusion
modules.","['cs.LG', 'cs.AI', 'cs.DB', 'stat.ML']",235,127
Knowledge-Based Construction of Confusion Matrices for Multi-Label Classification Algorithms using Semantic Similarity Measures,"So far, multi-label classification algorithms have been evaluated using
statistical methods that do not consider the semantics of the considered
classes and that fully depend on abstract computations such as Bayesian
Reasoning. Currently, there are several attempts to develop ontology-based
methods for a better assessment of supervised classification algorithms. In
this research paper, we define a novel approach that aligns expected labels
with predicted labels in multi-label classification using ontology-driven
feature-based semantic similarity measures and we use it to develop a method
for creating precise confusion matrices for a more effective evaluation of
multi-label classification algorithms.","['cs.LG', 'cs.CL']",102,74
Strengths and Weaknesses of Deep Learning Models for Face Recognition Against Image Degradations,"Deep convolutional neural networks (CNNs) based approaches are the
state-of-the-art in various computer vision tasks, including face recognition.
Considerable research effort is currently being directed towards further
improving deep CNNs by focusing on more powerful model architectures and better
learning techniques. However, studies systematically exploring the strengths
and weaknesses of existing deep models for face recognition are still
relatively scarce in the literature. In this paper, we try to fill this gap and
study the effects of different covariates on the verification performance of
four recent deep CNN models using the Labeled Faces in the Wild (LFW) dataset.
Specifically, we investigate the influence of covariates related to: image
quality -- blur, JPEG compression, occlusion, noise, image brightness,
contrast, missing pixels; and model characteristics -- CNN architecture, color
information, descriptor computation; and analyze their impact on the face
verification performance of AlexNet, VGG-Face, GoogLeNet, and SqueezeNet. Based
on comprehensive and rigorous experimentation, we identify the strengths and
weaknesses of the deep learning models, and present key areas for potential
future research. Our results indicate that high levels of noise, blur, missing
pixels, and brightness have a detrimental effect on the verification
performance of all models, whereas the impact of contrast changes and
compression artifacts is limited. It has been found that the descriptor
computation strategy and color information does not have a significant
influence on performance.",['stat.ML'],227,140
Which transformer architecture fits my data? A vocabulary bottleneck in self-attention,"After their successful debut in natural language processing, Transformer
architectures are now becoming the de-facto standard in many domains. An
obstacle for their deployment over new modalities is the architectural
configuration: the optimal depth-to-width ratio has been shown to dramatically
vary across data types (e.g., $10$x larger over images than over language). We
theoretically predict the existence of an embedding rank bottleneck that limits
the contribution of self-attention width to the Transformer expressivity. We
thus directly tie the input vocabulary size and rank to the optimal
depth-to-width ratio, since a small vocabulary size or rank dictates an added
advantage of depth over width. We empirically demonstrate the existence of this
bottleneck and its implications on the depth-to-width interplay of Transformer
architectures, linking the architecture variability across domains to the often
glossed-over usage of different vocabulary sizes or embedding ranks in
different domains. As an additional benefit, our rank bottlenecking framework
allows us to identify size redundancies of $25\%-50\%$ in leading NLP models
such as ALBERT and T5.","['cs.LG', 'cs.CL']",179,113
S$^2$-MLP: Spatial-Shift MLP Architecture for Vision,"Recently, visual Transformer (ViT) and its following works abandon the
convolution and exploit the self-attention operation, attaining a comparable or
even higher accuracy than CNNs. More recently, MLP-Mixer abandons both the
convolution and the self-attention operation, proposing an architecture
containing only MLP layers. To achieve cross-patch communications, it devises
an additional token-mixing MLP besides the channel-mixing MLP. It achieves
promising results when training on an extremely large-scale dataset. But it
cannot achieve as outstanding performance as its CNN and ViT counterparts when
training on medium-scale datasets such as ImageNet1K and ImageNet21K. The
performance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We
discover that the token-mixing MLP is a variant of the depthwise convolution
with a global reception field and spatial-specific configuration. But the
global reception field and the spatial-specific property make token-mixing MLP
prone to over-fitting. In this paper, we propose a novel pure MLP architecture,
spatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our S$^2$-MLP only
contains channel-mixing MLP. We utilize a spatial-shift operation for
communications between patches. It has a local reception field and is
spatial-agnostic. It is parameter-free and efficient for computation. The
proposed S$^2$-MLP attains higher recognition accuracy than MLP-Mixer when
training on ImageNet-1K dataset. Meanwhile, S$^2$-MLP accomplishes as excellent
performance as ViT on ImageNet-1K dataset with considerably simpler
architecture and fewer FLOPs and parameters.","['cs.CV', 'cs.AI', 'cs.LG']",252,134
Kickstarting Deep Reinforcement Learning,"We present a method for using previously-trained 'teacher' agents to
kickstart the training of a new 'student' agent. To this end, we leverage ideas
from policy distillation and population based training. Our method places no
constraints on the architecture of the teacher or student agents, and it
regulates itself to allow the students to surpass their teachers in
performance. We show that, on a challenging and computationally-intensive
multi-task benchmark (DMLab-30), kickstarted training improves the data
efficiency of new agents, making it significantly easier to iterate on their
design. We also show that the same kickstarting pipeline can allow a single
student agent to leverage multiple 'expert' teachers which specialize on
individual tasks. In this setting kickstarting yields surprisingly large gains,
with the kickstarted agent matching the performance of an agent trained from
scratch in almost 10x fewer steps, and surpassing its final performance by 42
percent. Kickstarting is conceptually simple and can easily be incorporated
into reinforcement learning experiments.",['cs.LG'],163,109
PI-REC: Progressive Image Reconstruction Network With Edge and Color Domain,"We propose a universal image reconstruction method to represent detailed
images purely from binary sparse edge and flat color domain. Inspired by the
procedures of painting, our framework, based on generative adversarial network,
consists of three phases: Imitation Phase aims at initializing networks,
followed by Generating Phase to reconstruct preliminary images. Moreover,
Refinement Phase is utilized to fine-tune preliminary images into final outputs
with details. This framework allows our model generating abundant high
frequency details from sparse input information. We also explore the defects of
disentangling style latent space implicitly from images, and demonstrate that
explicit color domain in our model performs better on controllability and
interpretability. In our experiments, we achieve outstanding results on
reconstructing realistic images and translating hand drawn drafts into
satisfactory paintings. Besides, within the domain of edge-to-image
translation, our model PI-REC outperforms existing state-of-the-art methods on
evaluations of realism and accuracy, both quantitatively and qualitatively.",['cs.CV'],157,112
An Automatic Digital Terrain Generation Technique for Terrestrial Sensing and Virtual Reality Applications,"The identification and modeling of the terrain from point cloud data is an
important component of Terrestrial Remote Sensing (TRS) applications. The main
focus in terrain modeling is capturing details of complex geological features
of landforms. Traditional terrain modeling approaches rely on the user to exert
control over terrain features. However, relying on the user input to manually
develop the digital terrain becomes intractable when considering the amount of
data generated by new remote sensing systems capable of producing massive
aerial and ground-based point clouds from scanned environments. This article
provides a novel terrain modeling technique capable of automatically generating
accurate and physically realistic Digital Terrain Models (DTM) from a variety
of point cloud data. The proposed method runs efficiently on large-scale point
cloud data with real-time performance over large segments of terrestrial
landforms. Moreover, generated digital models are designed to effectively
render within a Virtual Reality (VR) environment in real time. The paper
concludes with an in-depth discussion of possible research directions and
outstanding technical and scientific challenges to improve the proposed
approach.","['cs.CV', 'cs.MM']",178,116
High Dimensional Time Series Generators,"Multidimensional time series are sequences of real valued vectors. They occur
in different areas, for example handwritten characters, GPS tracking, and
gestures of modern virtual reality motion controllers. Within these areas, a
common task is to search for similar time series. Dynamic Time Warping (DTW) is
a common distance function to compare two time series. The Edit Distance with
Real Penalty (ERP) and the Dog Keeper Distance (DK) are two more distance
functions on time series. Their behaviour has been analyzed on 1-dimensional
time series. However, it is not easy to evaluate their behaviour in relation to
growing dimensionality. For this reason we propose two new data synthesizers
generating multidimensional time series. The first synthesizer extends the well
known cylinder-bell-funnel (CBF) dataset to multidimensional time series. Here,
each time series has an arbitrary type (cylinder, bell, or funnel) in each
dimension, thus for $d$-dimensional time series there are $3^{d}$ different
classes. The second synthesizer (RAM) creates time series with ideas adapted
from Brownian motions which is a common model of movement in physics. Finally,
we evaluate the applicability of a 1-nearest neighbor classifier using DTW on
datasets generated by our synthesizers.","['cs.LG', 'stat.ML']",197,128
Unsupervised Segmentation of 3D Medical Images Based on Clustering and Deep Representation Learning,"This paper presents a novel unsupervised segmentation method for 3D medical
images. Convolutional neural networks (CNNs) have brought significant advances
in image segmentation. However, most of the recent methods rely on supervised
learning, which requires large amounts of manually annotated data. Thus, it is
challenging for these methods to cope with the growing amount of medical
images. This paper proposes a unified approach to unsupervised deep
representation learning and clustering for segmentation. Our proposed method
consists of two phases. In the first phase, we learn deep feature
representations of training patches from a target image using joint
unsupervised learning (JULE) that alternately clusters representations
generated by a CNN and updates the CNN parameters using cluster labels as
supervisory signals. We extend JULE to 3D medical images by utilizing 3D
convolutions throughout the CNN architecture. In the second phase, we apply
k-means to the deep representations from the trained CNN and then project
cluster labels to the target image in order to obtain the fully segmented
image. We evaluated our methods on three images of lung cancer specimens
scanned with micro-computed tomography (micro-CT). The automatic segmentation
of pathological regions in micro-CT could further contribute to the
pathological examination process. Hence, we aim to automatically divide each
image into the regions of invasive carcinoma, noninvasive carcinoma, and normal
tissue. Our experiments show the potential abilities of unsupervised deep
representation learning for medical image segmentation.",['cs.CV'],236,140
Time-Series Representation Learning via Temporal and Contextual Contrasting,"Learning decent representations from unlabeled time-series data with temporal
dynamics is a very challenging task. In this paper, we propose an unsupervised
Time-Series representation learning framework via Temporal and Contextual
Contrasting (TS-TCC), to learn time-series representation from unlabeled data.
First, the raw time-series data are transformed into two different yet
correlated views by using weak and strong augmentations. Second, we propose a
novel temporal contrasting module to learn robust temporal representations by
designing a tough cross-view prediction task. Last, to further learn
discriminative representations, we propose a contextual contrasting module
built upon the contexts from the temporal contrasting module. It attempts to
maximize the similarity among different contexts of the same sample while
minimizing similarity among contexts of different samples. Experiments have
been carried out on three real-world time-series datasets. The results manifest
that training a linear classifier on top of the features learned by our
proposed TS-TCC performs comparably with the supervised training. Additionally,
our proposed TS-TCC shows high efficiency in few-labeled data and transfer
learning scenarios. The code is publicly available at
https://github.com/emadeldeen24/TS-TCC.","['cs.LG', 'cs.AI']",191,123
Robust Point Light Source Estimation Using Differentiable Rendering,"Illumination estimation is often used in mixed reality to re-render a scene
from another point of view, to change the color/texture of an object, or to
insert a virtual object consistently lit into a real video or photograph.
Specifically, the estimation of a point light source is required for the
shadows cast by the inserted object to be consistent with the real scene. We
tackle the problem of illumination retrieval given an RGBD image of the scene
as an inverse problem: we aim to find the illumination that minimizes the
photometric error between the rendered image and the observation. In particular
we propose a novel differentiable renderer based on the Blinn-Phong model with
cast shadows. We compare our differentiable renderer to state-of-the-art
methods and demonstrate its robustness to an incorrect reflectance estimation.",['cs.CV'],138,87
A Framework of Transfer Learning in Object Detection for Embedded Systems,"Transfer learning is one of the subjects undergoing intense study in the area
of machine learning. In object recognition and object detection there are known
experiments for the transferability of parameters, but not for neural networks
which are suitable for object detection in real time embedded applications,
such as the SqueezeDet neural network. We use transfer learning to accelerate
the training of SqueezeDet to a new group of classes. Also, experiments are
conducted to study the transferability and co-adaptation phenomena introduced
by the transfer learning process. To accelerate training, we propose a new
implementation of the SqueezeDet training which provides a faster pipeline for
data processing and achieves 1.8 times speedup compared to the initial
implementation. Finally, we created a mechanism for automatic hyperparameter
optimization using an empirical method.",['cs.CV'],131,83
Adaptive Transfer Learning of Multi-View Time Series Classification,"Time Series Classification (TSC) has been an important and challenging task
in data mining, especially on multivariate time series and multi-view time
series data sets. Meanwhile, transfer learning has been widely applied in
computer vision and natural language processing applications to improve deep
neural network's generalization capabilities. However, very few previous works
applied transfer learning framework to time series mining problems.
Particularly, the technique of measuring similarities between source domain and
target domain based on dynamic representation such as density estimation with
importance sampling has never been combined with transfer learning framework.
In this paper, we first proposed a general adaptive transfer learning framework
for multi-view time series data, which shows strong ability in storing
inter-view importance value in the process of knowledge transfer. Next, we
represented inter-view importance through some time series similarity
measurements and approximated the posterior distribution in latent space for
the importance sampling via density estimation techniques. We then computed the
matrix norm of sampled importance value, which controls the degree of knowledge
transfer in pre-training process. We further evaluated our work, applied it to
many other time series classification tasks, and observed that our architecture
maintained desirable generalization ability. Finally, we concluded that our
framework could be adapted with deep learning techniques to receive significant
model performance improvements.","['cs.LG', 'stat.ML']",219,136
Superpixel based Class-Semantic Texton Occurrences for Natural Roadside Vegetation Segmentation,"Vegetation segmentation from roadside data is a field that has received
relatively little attention in present studies, but can be of great potentials
in a wide range of real-world applications, such as road safety assessment and
vegetation condition monitoring. In this paper, we present a novel approach
that generates class-semantic color-texture textons and aggregates superpixel
based texton occurrences for vegetation segmentation in natural roadside
images. Pixel-level class-semantic textons are first learnt by generating two
individual sets of bag-of-word visual dictionaries from color and filter-bank
texture features separately for each object class using manually cropped
training data. For a testing image, it is first oversegmented into a set of
homogeneous superpixels. The color and texture features of all pixels in each
superpixel are extracted and further mapped to one of the learnt textons using
the nearest distance metric, resulting in a color and a texture texton
occurrence matrix. The color and texture texton occurrences are aggregated
using a linear mixing method over each superpixel and the segmentation is
finally achieved using a simple yet effective majority voting strategy.
Evaluations on two public image datasets from videos collected by the
Department of Transport and Main Roads (DTMR), Queensland, Australia, and a
public roadside grass dataset show high accuracy of the proposed approach. We
also demonstrate the effectiveness of the approach for vegetation segmentation
in real-world scenarios.",['cs.CV'],233,142
Multiresolution Graph Variational Autoencoder,"In this paper, we propose Multiresolution Graph Networks (MGN) and
Multiresolution Graph Variational Autoencoders (MGVAE) to learn and generate
graphs in a multiresolution and equivariant manner. At each resolution level,
MGN employs higher order message passing to encode the graph while learning to
partition it into mutually exclusive clusters and coarsening into a lower
resolution. MGVAE constructs a hierarchical generative model based on MGN to
variationally autoencode the hierarchy of coarsened graphs. Our proposed
framework is end-to-end permutation equivariant with respect to node ordering.
Our methods have been successful with several generative tasks including link
prediction on citation graphs, unsupervised molecular representation learning
to predict molecular properties, molecular generation, general graph generation
and graph-based image generation.","['cs.LG', 'cs.SI', 'physics.chem-ph']",119,83
EPSANet: An Efficient Pyramid Squeeze Attention Block on Convolutional Neural Network,"Recently, it has been demonstrated that the performance of a deep
convolutional neural network can be effectively improved by embedding an
attention module into it. In this work, a novel lightweight and effective
attention method named Pyramid Squeeze Attention (PSA) module is proposed. By
replacing the 3x3 convolution with the PSA module in the bottleneck blocks of
the ResNet, a novel representational block named Efficient Pyramid Squeeze
Attention (EPSA) is obtained. The EPSA block can be easily added as a
plug-and-play component into a well-established backbone network, and
significant improvements on model performance can be achieved. Hence, a simple
and efficient backbone architecture named EPSANet is developed in this work by
stacking these ResNet-style EPSA blocks. Correspondingly, a stronger
multi-scale representation ability can be offered by the proposed EPSANet for
various computer vision tasks including but not limited to, image
classification, object detection, instance segmentation, etc. Without bells and
whistles, the performance of the proposed EPSANet outperforms most of the
state-of-the-art channel attention methods. As compared to the SENet-50, the
Top-1 accuracy is improved by 1.93% on ImageNet dataset, a larger margin of
+2.7 box AP for object detection and an improvement of +1.7 mask AP for
instance segmentation by using the Mask-RCNN on MS-COCO dataset are obtained.
Our source code is available at:https://github.com/murufeng/EPSANet.",['cs.CV'],234,143
FaiR-IoT: Fairness-aware Human-in-the-Loop Reinforcement Learning for Harnessing Human Variability in Personalized IoT,"Thanks to the rapid growth in wearable technologies, monitoring complex human
context becomes feasible, paving the way to develop human-in-the-loop IoT
systems that naturally evolve to adapt to the human and environment state
autonomously. Nevertheless, a central challenge in designing such personalized
IoT applications arises from human variability. Such variability stems from the
fact that different humans exhibit different behaviors when interacting with
IoT applications (intra-human variability), the same human may change the
behavior over time when interacting with the same IoT application (inter-human
variability), and human behavior may be affected by the behaviors of other
people in the same environment (multi-human variability). To that end, we
propose FaiR-IoT, a general reinforcement learning-based framework for adaptive
and fairness-aware human-in-the-loop IoT applications. In FaiR-IoT, three
levels of reinforcement learning agents interact to continuously learn human
preferences and maximize the system's performance and fairness while taking
into account the intra-, inter-, and multi-human variability. We validate the
proposed framework on two applications, namely (i) Human-in-the-Loop Automotive
Advanced Driver Assistance Systems and (ii) Human-in-the-Loop Smart House.
Results obtained on these two applications validate the generality of FaiR-IoT
and its ability to provide a personalized experience while enhancing the
system's performance by 40%-60% compared to non-personalized systems and
enhancing the fairness of the multi-human systems by 1.5 orders of magnitude.","['cs.LG', 'cs.MA']",243,129
Pose Guided Person Image Generation with Hidden p-Norm Regression,"In this paper, we propose a novel approach to solve the pose guided person
image generation task. We assume that the relation between pose and appearance
information can be described by a simple matrix operation in hidden space.
Based on this assumption, our method estimates a pose-invariant feature matrix
for each identity, and uses it to predict the target appearance conditioned on
the target pose. The estimation process is formulated as a p-norm regression
problem in hidden space. By utilizing the differentiation of the solution of
this regression problem, the parameters of the whole framework can be trained
in an end-to-end manner. While most previous works are only applicable to the
supervised training and single-shot generation scenario, our method can be
easily adapted to unsupervised training and multi-shot generation. Extensive
experiments on the challenging Market-1501 dataset show that our method yields
competitive performance in all the aforementioned variant scenarios.",['cs.CV'],156,103
An interpretable neural network model through piecewise linear approximation,"Most existing interpretable methods explain a black-box model in a post-hoc
manner, which uses simpler models or data analysis techniques to interpret the
predictions after the model is learned. However, they (a) may derive
contradictory explanations on the same predictions given different methods and
data samples, and (b) focus on using simpler models to provide higher
descriptive accuracy at the sacrifice of prediction accuracy. To address these
issues, we propose a hybrid interpretable model that combines a piecewise
linear component and a nonlinear component. The first component describes the
explicit feature contributions by piecewise linear approximation to increase
the expressiveness of the model. The other component uses a multi-layer
perceptron to capture feature interactions and implicit nonlinearity, and
increase the prediction performance. Different from the post-hoc approaches,
the interpretability is obtained once the model is learned in the form of
feature shapes. We also provide a variant to explore higher-order interactions
among features to demonstrate that the proposed model is flexible for
adaptation. Experiments demonstrate that the proposed model can achieve good
interpretability by describing feature shapes while maintaining
state-of-the-art accuracy.","['cs.LG', 'cs.AI', 'stat.ML']",189,112
EfficientLPS: Efficient LiDAR Panoptic Segmentation,"Panoptic segmentation of point clouds is a crucial task that enables
autonomous vehicles to comprehend their vicinity using their highly accurate
and reliable LiDAR sensors. Existing top-down approaches tackle this problem by
either combining independent task-specific networks or translating methods from
the image domain ignoring the intricacies of LiDAR data and thus often
resulting in sub-optimal performance. In this paper, we present the novel
top-down Efficient LiDAR Panoptic Segmentation (EfficientLPS) architecture that
addresses multiple challenges in segmenting LiDAR point clouds including
distance-dependent sparsity, severe occlusions, large scale-variations, and
re-projection errors. EfficientLPS comprises of a novel shared backbone that
encodes with strengthened geometric transformation modeling capacity and
aggregates semantically rich range-aware multi-scale features. It incorporates
new scale-invariant semantic and instance segmentation heads along with the
panoptic fusion module which is supervised by our proposed panoptic periphery
loss function. Additionally, we formulate a regularized pseudo labeling
framework to further improve the performance of EfficientLPS by training on
unlabelled data. We benchmark our proposed model on two large-scale LiDAR
datasets: nuScenes, for which we also provide ground truth annotations, and
SemanticKITTI. Notably, EfficientLPS sets the new state-of-the-art on both
these datasets.","['cs.CV', 'cs.LG', 'cs.RO']",202,144
Bayesian task embedding for few-shot Bayesian optimization,"We describe a method for Bayesian optimization by which one may incorporate
data from multiple systems whose quantitative interrelationships are unknown a
priori. All general (nonreal-valued) features of the systems are associated
with continuous latent variables that enter as inputs into a single metamodel
that simultaneously learns the response surfaces of all of the systems.
Bayesian inference is used to determine appropriate beliefs regarding the
latent variables. We explain how the resulting probabilistic metamodel may be
used for Bayesian optimization tasks and demonstrate its implementation on a
variety of synthetic and real-world examples, comparing its performance under
zero-, one-, and few-shot settings against traditional Bayesian optimization,
which usually requires substantially more data from the system of interest.","['cs.LG', 'stat.ML']",120,85
Energy-Based Models for Code Generation under Compilability Constraints,"Neural language models can be successfully trained on source code, leading to
applications such as code completion. However, their versatile autoregressive
self-supervision objective overlooks important global sequence-level features
that are present in the data such as syntactic correctness or compilability. In
this work, we pose the problem of learning to generate compilable code as
constraint satisfaction. We define an Energy-Based Model (EBM) representing a
pre-trained generative model with an imposed constraint of generating only
compilable sequences. We then use the KL-Adaptive Distributional Policy
Gradient algorithm (Khalifa et al., 2021) to train a generative model
approximating the EBM. We conduct experiments showing that our proposed
approach is able to improve compilability rates without sacrificing diversity
and complexity of the generated samples.","['cs.LG', 'cs.CL', 'cs.NE', 'cs.SE', 'I.2.2; I.2.7; I.2.6; I.5.1']",125,99
Dilated-Scale-Aware Attention ConvNet For Multi-Class Object Counting,"Object counting aims to estimate the number of objects in images. The leading
counting approaches focus on the single category counting task and achieve
impressive performance. Note that there are multiple categories of objects in
real scenes. Multi-class object counting expands the scope of application of
object counting task. The multi-target detection task can achieve multi-class
object counting in some scenarios. However, it requires the dataset annotated
with bounding boxes. Compared with the point annotations in mainstream object
counting issues, the coordinate box-level annotations are more difficult to
obtain. In this paper, we propose a simple yet efficient counting network based
on point-level annotations. Specifically, we first change the traditional
output channel from one to the number of categories to achieve multiclass
counting. Since all categories of objects use the same feature extractor in our
proposed framework, their features will interfere mutually in the shared
feature space. We further design a multi-mask structure to suppress harmful
interaction among objects. Extensive experiments on the challenging benchmarks
illustrate that the proposed method achieves state-of-the-art counting
performance.",['cs.CV'],183,116
Deep Probabilistic Graphical Modeling,"Probabilistic graphical modeling (PGM) provides a framework for formulating
an interpretable generative process of data and expressing uncertainty about
unknowns, but it lacks flexibility. Deep learning (DL) is an alternative
framework for learning from data that has achieved great empirical success in
recent years. DL offers great flexibility, but it lacks the interpretability
and calibration of PGM. This thesis develops deep probabilistic graphical
modeling (DPGM.) DPGM consists in leveraging DL to make PGM more flexible. DPGM
brings about new methods for learning from data that exhibit the advantages of
both PGM and DL.
  We use DL within PGM to build flexible models endowed with an interpretable
latent structure. One model class we develop extends exponential family PCA
using neural networks to improve predictive performance while enforcing the
interpretability of the latent factors. Another model class we introduce
enables accounting for long-term dependencies when modeling sequential data,
which is a challenge when using purely DL or PGM approaches. Finally, DPGM
successfully solves several outstanding problems of probabilistic topic models,
a widely used family of models in PGM.
  DPGM also brings about new algorithms for learning with complex data. We
develop reweighted expectation maximization, an algorithm that unifies several
existing maximum likelihood-based algorithms for learning models parameterized
by neural networks. This unifying view is made possible using expectation
maximization, a canonical inference algorithm in PGM. We also develop
entropy-regularized adversarial learning, a learning paradigm that deviates
from the traditional maximum likelihood approach used in PGM. From the DL
perspective, entropy-regularized adversarial learning provides a solution to
the long-standing mode collapse problem of generative adversarial networks, a
widely used DL approach.","['stat.ML', 'cs.LG']",273,145
Graph Structured Prediction Energy Networks,"For joint inference over multiple variables, a variety of structured
prediction techniques have been developed to model correlations among variables
and thereby improve predictions. However, many classical approaches suffer from
one of two primary drawbacks: they either lack the ability to model high-order
correlations among variables while maintaining computationally tractable
inference, or they do not allow to explicitly model known correlations. To
address this shortcoming, we introduce `Graph Structured Prediction Energy
Networks,' for which we develop inference techniques that allow to both model
explicit local and implicit higher-order correlations while maintaining
tractability of inference. We apply the proposed method to tasks from the
natural language processing and computer vision domain and demonstrate its
general utility.","['cs.LG', 'stat.ML']",117,86
Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning,"This paper addresses a new problem of understanding human gaze communication
in social videos from both atomic-level and event-level, which is significant
for studying human social interactions. To tackle this novel and challenging
problem, we contribute a large-scale video dataset, VACATION, which covers
diverse daily social scenes and gaze communication behaviors with complete
annotations of objects and human faces, human attention, and communication
structures and labels in both atomic-level and event-level. Together with
VACATION, we propose a spatio-temporal graph neural network to explicitly
represent the diverse gaze interactions in the social scenes and to infer
atomic-level gaze communication by message passing. We further propose an event
network with encoder-decoder structure to predict the event-level gaze
communication. Our experiments demonstrate that the proposed model improves
various baselines significantly in predicting the atomic-level and event-level
gaze",['cs.CV'],145,84
Using Cross-Loss Influence Functions to Explain Deep Network Representations,"As machine learning is increasingly deployed in the real world, it is ever
more vital that we understand the decision-criteria of the models we train.
Recently, researchers have shown that influence functions, a statistical
measure of sample impact, may be extended to approximate the effects of
training samples on classification accuracy for deep neural networks. However,
prior work only applies to supervised learning setups where training and
testing share an objective function. Despite the rise in unsupervised learning,
self-supervised learning, and model pre-training, there are currently no
suitable technologies for estimating influence of deep networks that do not
train and test on the same objective. To overcome this limitation, we provide
the first theoretical and empirical demonstration that influence functions can
be extended to handle mismatched training and testing settings. Our result
enables us to compute the influence of unsupervised and self-supervised
training examples with respect to a supervised test objective. We demonstrate
this technique on a synthetic dataset as well as two Skip-gram language model
examples to examine cluster membership and sources of unwanted bias.",['cs.LG'],181,117
Target Transformed Regression for Accurate Tracking,"Accurate tracking is still a challenging task due to appearance variations,
pose and view changes, and geometric deformations of target in videos. Recent
anchor-free trackers provide an efficient regression mechanism but fail to
produce precise bounding box estimation. To address these issues, this paper
repurposes a Transformer-alike regression branch, termed as Target Transformed
Regression (TREG), for accurate anchor-free tracking. The core to our TREG is
to model pair-wise relation between elements in target template and search
region, and use the resulted target enhanced visual representation for accurate
bounding box regression. This target contextualized representation is able to
enhance the target relevant information to help precisely locate the box
boundaries, and deal with the object deformation to some extent due to its
local and dense matching mechanism. In addition, we devise a simple online
template update mechanism to select reliable templates, increasing the
robustness for appearance variations and geometric deformations of target in
time. Experimental results on visual tracking benchmarks including VOT2018,
VOT2019, OTB100, GOT10k, NFS, UAV123, LaSOT and TrackingNet demonstrate that
TREG obtains the state-of-the-art performance, achieving a success rate of
0.640 on LaSOT, while running at around 30 FPS. The code and models will be
made available at https://github.com/MCG-NJU/TREG.",['cs.CV'],213,147
Time Adaptive Reinforcement Learning,"Reinforcement learning (RL) allows to solve complex tasks such as Go often
with a stronger performance than humans. However, the learned behaviors are
usually fixed to specific tasks and unable to adapt to different contexts. Here
we consider the case of adapting RL agents to different time restrictions, such
as finishing a task with a given time limit that might change from one task
execution to the next. We define such problems as Time Adaptive Markov Decision
Processes and introduce two model-free, value-based algorithms: the Independent
Gamma-Ensemble and the n-Step Ensemble. In difference to classical approaches,
they allow a zero-shot adaptation between different time restrictions. The
proposed approaches represent general mechanisms to handle time adaptive tasks
making them compatible with many existing RL methods, algorithms, and
scenarios.","['cs.LG', 'cs.AI', 'stat.ML', 'I.2.6']",132,95
Deep Mixtures of Unigrams for uncovering Topics in Textual Data,"Mixtures of Unigrams are one of the simplest and most efficient tools for
clustering textual data, as they assume that documents related to the same
topic have similar distributions of terms, naturally described by Multinomials.
When the classification task is particularly challenging, such as when the
document-term matrix is high-dimensional and extremely sparse, a more composite
representation can provide better insight on the grouping structure. In this
work, we developed a deep version of mixtures of Unigrams for the unsupervised
classification of very short documents with a large number of terms, by
allowing for models with further deeper latent layers; the proposal is derived
in a Bayesian framework. The behaviour of the Deep Mixtures of Unigrams is
empirically compared with that of other traditional and state-of-the-art
methods, namely $k$-means with cosine distance, $k$-means with Euclidean
distance on data transformed according to Semantic Analysis, Partition Around
Medoids, Mixture of Gaussians on semantic-based transformed data, hierarchical
clustering according to Ward's method with cosine dissimilarity, Latent
Dirichlet Allocation, Mixtures of Unigrams estimated via the EM algorithm,
Spectral Clustering and Affinity Propagation clustering. The performance is
evaluated in terms of both correct classification rate and Adjusted Rand Index.
Simulation studies and real data analysis prove that going deep in clustering
such data highly improves the classification accuracy.","['stat.ML', 'cs.LG']",222,143
Diff-Net: Image Feature Difference based High-Definition Map Change Detection,"Up-to-date High-Definition (HD) maps are essential for self-driving cars. To
achieve constantly updated HD maps, we present a deep neural network (DNN),
Diff-Net, to detect changes in them. Compared to traditional methods based on
object detectors, the essential design in our work is a parallel feature
difference calculation structure that infers map changes by comparing features
extracted from the camera and rasterized images. To generate these rasterized
images, we project map elements onto images in the camera view, yielding
meaningful map representations that can be consumed by a DNN accordingly. As we
formulate the change detection task as an object detection problem, we leverage
the anchor-based structure that predicts bounding boxes with different change
status categories. Furthermore, rather than relying on single frame input, we
introduce a spatio-temporal fusion module that fuses features from history
frames into the current, thus improving the overall performance. Finally, we
comprehensively validate our method's effectiveness using freshly collected
datasets. Results demonstrate that our Diff-Net achieves better performance
than the baseline methods and is ready to be integrated into a map production
pipeline maintaining an up-to-date HD map.","['cs.CV', 'cs.RO']",194,131
MsCGAN: Multi-scale Conditional Generative Adversarial Networks for Person Image Generation,"To synthesize high-quality person images with arbitrary poses is challenging.
In this paper, we propose a novel Multi-scale Conditional Generative
Adversarial Networks (MsCGAN), aiming to convert the input conditional person
image to a synthetic image of any given target pose, whose appearance and the
texture are consistent with the input image. MsCGAN is a multi-scale
adversarial network consisting of two generators and two discriminators. One
generator transforms the conditional person image into a coarse image of the
target pose globally, and the other is to enhance the detailed quality of the
synthetic person image through a local reinforcement network. The outputs of
the two generators are then merged into a synthetic, discriminant and
high-resolution image. On the other hand, the synthetic image is downsampled to
multiple resolutions as the input to multi-scale discriminator networks. The
proposed multi-scale generators and discriminators handling different levels of
visual features can benefit to synthesizing high-resolution person images with
realistic appearance and texture. Experiments are conducted on the Market-1501
and DeepFashion datasets to evaluate the proposed model, and both qualitative
and quantitative results demonstrate the superior performance of the proposed
MsCGAN.",['cs.CV'],194,103
Neural Sign Language Translation based on Human Keypoint Estimation,"We propose a sign language translation system based on human keypoint
estimation. It is well-known that many problems in the field of computer vision
require a massive amount of dataset to train deep neural network models. The
situation is even worse when it comes to the sign language translation problem
as it is far more difficult to collect high-quality training data. In this
paper, we introduce the KETI (short for Korea Electronics Technology Institute)
sign language dataset which consists of 14,672 videos of high resolution and
quality. Considering the fact that each country has a different and unique sign
language, the KETI sign language dataset can be the starting line for further
research on the Korean sign language translation. Using the KETI sign language
dataset, we develop a neural network model for translating sign videos into
natural language sentences by utilizing the human keypoints extracted from a
face, hands, and body parts. The obtained human keypoint vector is normalized
by the mean and standard deviation of the keypoints and used as input to our
translation model based on the sequence-to-sequence architecture. As a result,
we show that our approach is robust even when the size of the training data is
not sufficient. Our translation model achieves 93.28% (55.28%, respectively)
translation accuracy on the validation set (test set, respectively) for 105
sentences that can be used in emergency situations. We compare several types of
our neural sign translation models based on different attention mechanisms in
terms of classical metrics for measuring the translation performance.",['cs.CV'],260,143
Exploiting Web Images for Weakly Supervised Object Detection,"In recent years, the performance of object detection has advanced
significantly with the evolving deep convolutional neural networks. However,
the state-of-the-art object detection methods still rely on accurate bounding
box annotations that require extensive human labelling. Object detection
without bounding box annotations, i.e, weakly supervised detection methods, are
still lagging far behind. As weakly supervised detection only uses image level
labels and does not require the ground truth of bounding box location and label
of each object in an image, it is generally very difficult to distill knowledge
of the actual appearances of objects. Inspired by curriculum learning, this
paper proposes an easy-to-hard knowledge transfer scheme that incorporates easy
web images to provide prior knowledge of object appearance as a good starting
point. While exploiting large-scale free web imagery, we introduce a
sophisticated labour free method to construct a web dataset with good diversity
in object appearance. After that, semantic relevance and distribution relevance
are introduced and utilized in the proposed curriculum training scheme. Our
end-to-end learning with the constructed web data achieves remarkable
improvement across most object classes especially for the classes that are
often considered hard in other works.",['cs.CV'],200,130
Learning More Universal Representations for Transfer-Learning,"A representation is supposed universal if it encodes any element of the
visual world (e.g., objects, scenes) in any configuration (e.g., scale,
context). While not expecting pure universal representations, the goal in the
literature is to improve the universality level, starting from a representation
with a certain level. To do so, the state-of-the-art consists in learning
CNN-based representations on a diversified training problem (e.g., ImageNet
modified by adding annotated data). While it effectively increases
universality, such approach still requires a large amount of efforts to satisfy
the needs in annotated data. In this work, we propose two methods to improve
universality, but pay special attention to limit the need of annotated data. We
also propose a unified framework of the methods based on the diversifying of
the training problem. Finally, to better match Atkinson's cognitive study about
universal human representations, we proposed to rely on the transfer-learning
scheme as well as a new metric to evaluate universality. This latter, aims us
to demonstrates the interest of our methods on 10 target-problems, relating to
the classification task and a variety of visual domains.","['cs.CV', 'cs.LG']",192,120
Improving Supervised Phase Identification Through the Theory of Information Losses,"This paper considers the problem of Phase Identification in power
distribution systems. In particular, it focuses on improving supervised
learning accuracies by focusing on exploiting some of the problem's information
theoretic properties. This focus, along with recent advances in Information
Theoretic Machine Learning (ITML), helps us to create two new techniques. The
first transforms a bound on information losses into a data selection technique.
This is important because phase identification data labels are difficult to
obtain in practice. The second interprets the properties of distribution
systems in the terms of ITML. This allows us to obtain an improvement in the
representation learned by any classifier applied to the problem. We tested
these two techniques experimentally on real datasets and have found that they
yield phenomenal performance in every case. In the most extreme case, they
improve phase identification accuracy from $51.7\%$ to $97.3\%$. Furthermore,
since many problems share the physical properties of phase identification
exploited in this paper, the techniques can be applied to a wide range of
similar problems.","['cs.LG', 'stat.ML']",173,115
RSM-GAN: A Convolutional Recurrent GAN for Anomaly Detection in Contaminated Seasonal Multivariate Time Series,"Robust anomaly detection is a requirement for monitoring complex modern
systems with applications such as cyber-security, fraud prevention, and
maintenance. These systems generate multiple correlated time series that are
highly seasonal and noisy. This paper presents a novel unsupervised deep
learning architecture for multivariate time series anomaly detection, called
Robust Seasonal Multivariate Generative Adversarial Network (RSM-GAN). It
extends recent advancements in GANs with adoption of convolutional-LSTM layers
and an attention mechanism to produce state-of-the-art performance. We conduct
extensive experiments to demonstrate the strength of our architecture in
adjusting for complex seasonality patterns and handling severe levels of
training data contamination. We also propose a novel anomaly score assignment
and causal inference framework. We compare RSM-GAN with existing classical and
deep-learning based anomaly detection models, and the results show that our
architecture is associated with the lowest false positive rate and improves
precision by 30% and 16% in real-world and synthetic data, respectively.
Furthermore, we report the superiority of RSM-GAN regarding accurate root cause
identification and NAB scores in all data settings.","['cs.LG', 'stat.ML']",181,126
Bayesian Image Reconstruction using Deep Generative Models,"Machine learning models are commonly trained end-to-end and in a supervised
setting, using paired (input, output) data. Examples include recent
super-resolution methods that train on pairs of (low-resolution,
high-resolution) images. However, these end-to-end approaches require
re-training every time there is a distribution shift in the inputs (e.g., night
images vs daylight) or relevant latent variables (e.g., camera blur or hand
motion). In this work, we leverage state-of-the-art (SOTA) generative models
(here StyleGAN2) for building powerful image priors, which enable application
of Bayes' theorem for many downstream reconstruction tasks. Our method,
Bayesian Reconstruction through Generative Models (BRGM), uses a single
pre-trained generator model to solve different image restoration tasks, i.e.,
super-resolution and in-painting, by combining it with different forward
corruption models. We keep the weights of the generator model fixed, and
reconstruct the image by estimating the Bayesian maximum a-posteriori (MAP)
estimate over the input latent vector that generated the reconstructed image.
We further use variational inference to approximate the posterior distribution
over the latent vectors, from which we sample multiple solutions. We
demonstrate BRGM on three large and diverse datasets: (i) 60,000 images from
the Flick Faces High Quality dataset (ii) 240,000 chest X-rays from MIMIC III
and (iii) a combined collection of 5 brain MRI datasets with 7,329 scans.
Across all three datasets and without any dataset-specific hyperparameter
tuning, our simple approach yields performance competitive with current
task-specific state-of-the-art methods on super-resolution and in-painting,
while being more generalisable and without requiring any training. Our source
code and pre-trained models are available online:
https://razvanmarinescu.github.io/brgm/.","['cs.CV', 'cs.LG', 'cs.NE', 'eess.IV', 'stat.ML']",288,190
Tracking by Animation: Unsupervised Learning of Multi-Object Attentive Trackers,"Online Multi-Object Tracking (MOT) from videos is a challenging computer
vision task which has been extensively studied for decades. Most of the
existing MOT algorithms are based on the Tracking-by-Detection (TBD) paradigm
combined with popular machine learning approaches which largely reduce the
human effort to tune algorithm parameters. However, the commonly used
supervised learning approaches require the labeled data (e.g., bounding boxes),
which is expensive for videos. Also, the TBD framework is usually suboptimal
since it is not end-to-end, i.e., it considers the task as detection and
tracking, but not jointly. To achieve both label-free and end-to-end learning
of MOT, we propose a Tracking-by-Animation framework, where a differentiable
neural model first tracks objects from input frames and then animates these
objects into reconstructed frames. Learning is then driven by the
reconstruction error through backpropagation. We further propose a
Reprioritized Attentive Tracking to improve the robustness of data association.
Experiments conducted on both synthetic and real video datasets show the
potential of the proposed model. Our project page is publicly available at:
https://github.com/zhen-he/tracking-by-animation","['cs.CV', 'cs.LG', 'stat.ML']",191,130
Image inpainting using frequency domain priors,"In this paper, we present a novel image inpainting technique using frequency
domain information. Prior works on image inpainting predict the missing pixels
by training neural networks using only the spatial domain information. However,
these methods still struggle to reconstruct high-frequency details for real
complex scenes, leading to a discrepancy in color, boundary artifacts,
distorted patterns, and blurry textures. To alleviate these problems, we
investigate if it is possible to obtain better performance by training the
networks using frequency domain information (Discrete Fourier Transform) along
with the spatial domain information. To this end, we propose a frequency-based
deconvolution module that enables the network to learn the global context while
selectively reconstructing the high-frequency components. We evaluate our
proposed method on the publicly available datasets CelebA, Paris Streetview,
and DTD texture dataset, and show that our method outperforms current
state-of-the-art image inpainting techniques both qualitatively and
quantitatively.","['cs.CV', 'eess.IV']",152,106
Multi-output Gaussian Processes for Uncertainty-aware Recommender Systems,"Recommender systems are often designed based on a collaborative filtering
approach, where user preferences are predicted by modelling interactions
between users and items. Many common approaches to solve the collaborative
filtering task are based on learning representations of users and items,
including simple matrix factorization, Gaussian process latent variable models,
and neural-network based embeddings. While matrix factorization approaches fail
to model nonlinear relations, neural networks can potentially capture such
complex relations with unprecedented predictive power and are highly scalable.
However, neither of them is able to model predictive uncertainties. In
contrast, Gaussian Process based models can generate a predictive distribution,
but cannot scale to large amounts of data. In this manuscript, we propose a
novel approach combining the representation learning paradigm of collaborative
filtering with multi-output Gaussian processes in a joint framework to generate
uncertainty-aware recommendations. We introduce an efficient strategy for model
training and inference, resulting in a model that scales to very large and
sparse datasets and achieves competitive performance in terms of classical
metrics quantifying the reconstruction error. In addition to accurately
predicting user preferences, our model also provides meaningful uncertainty
estimates about that prediction.","['cs.LG', 'stat.ML']",191,128
"Balancing thermal comfort datasets: We GAN, but should we?","Thermal comfort assessment for the built environment has become more
available to analysts and researchers due to the proliferation of sensors and
subjective feedback methods. These data can be used for modeling comfort
behavior to support design and operations towards energy efficiency and
well-being. By nature, occupant subjective feedback is imbalanced as indoor
conditions are designed for comfort, and responses indicating otherwise are
less common. This situation creates a scenario for the machine learning
workflow where class balancing as a pre-processing step might be valuable for
developing predictive thermal comfort classification models with
high-performance. This paper investigates the various thermal comfort dataset
class balancing techniques from the literature and proposes a modified
conditional Generative Adversarial Network (GAN), $\texttt{comfortGAN}$, to
address this imbalance scenario. These approaches are applied to three publicly
available datasets, ranging from 30 and 67 participants to a global collection
of thermal comfort datasets, with 1,474; 2,067; and 66,397 data points,
respectively. This work finds that a classification model trained on a balanced
dataset, comprised of real and generated samples from $\texttt{comfortGAN}$,
has higher performance (increase between 4% and 17% in classification accuracy)
than other augmentation methods tested. However, when classes representing
discomfort are merged and reduced to three, better imbalanced performance is
expected, and the additional increase in performance by $\texttt{comfortGAN}$
shrinks to 1-2%. These results illustrate that class balancing for thermal
comfort modeling is beneficial using advanced techniques such as GANs, but its
value is diminished in certain scenarios. A discussion is provided to assist
potential users in determining which scenarios this process is useful and which
method works best.","['cs.LG', 'stat.ML']",274,174
PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud,"In this paper, we propose PointRCNN for 3D object detection from raw point
cloud. The whole framework is composed of two stages: stage-1 for the bottom-up
3D proposal generation and stage-2 for refining proposals in the canonical
coordinates to obtain the final detection results. Instead of generating
proposals from RGB image or projecting point cloud to bird's view or voxels as
previous methods do, our stage-1 sub-network directly generates a small number
of high-quality 3D proposals from point cloud in a bottom-up manner via
segmenting the point cloud of the whole scene into foreground points and
background. The stage-2 sub-network transforms the pooled points of each
proposal to canonical coordinates to learn better local spatial features, which
is combined with global semantic features of each point learned in stage-1 for
accurate box refinement and confidence prediction. Extensive experiments on the
3D detection benchmark of KITTI dataset show that our proposed architecture
outperforms state-of-the-art methods with remarkable margins by using only
point cloud as input. The code is available at
https://github.com/sshaoshuai/PointRCNN.",['cs.CV'],188,117
Histological images segmentation of mucous glands,"Mucous glands lesions analysis and assessing of malignant potential of colon
polyps are very important tasks of surgical pathology. However, differential
diagnosis of colon polyps often seems impossible by classical methods and it is
necessary to involve computer methods capable of assessing minimal differences
to extend the capabilities of the classical pathology examination. Accurate
segmentation of mucous glands from histology images is a crucial step to obtain
reliable morphometric criteria for quantitative diagnostic methods. We review
major trends in histological images segmentation and design a new convolutional
neural network for mucous gland segmentation.",['cs.CV'],93,67
Predicting Depth from Semantic Segmentation using Game Engine Dataset,"Depth perception is fundamental for robots to understand the surrounding
environment. As the view of cognitive neuroscience, visual depth perception
methods are divided into three categories, namely binocular, active, and
pictorial. The first two categories have been studied for decades in detail.
However, research for the exploration of the third category is still in its
infancy and has got momentum by the advent of deep learning methods in recent
years. In cognitive neuroscience, it is known that pictorial depth perception
mechanisms are dependent on the perception of seen objects. Inspired by this
fact, in this thesis, we investigated the relation of perception of objects and
depth estimation convolutional neural networks. For this purpose, we developed
new network structures based on a simple depth estimation network that only
used a single image at its input. Our proposed structures use both an image and
a semantic label of the image as their input. We used semantic labels as the
output of object perception. The obtained results of performance comparison
between the developed network and original network showed that our novel
structures can improve the performance of depth estimation by 52\% of relative
error of distance in the examined cases. Most of the experimental studies were
carried out on synthetic datasets that were generated by game engines to
isolate the performance comparison from the effect of inaccurate depth and
semantic labels of non-synthetic datasets. It is shown that particular
synthetic datasets may be used for training of depth networks in cases that an
appropriate dataset is not available. Furthermore, we showed that in these
cases, usage of semantic labels improves the robustness of the network against
domain shift from synthetic training data to non-synthetic test data.","['cs.CV', 'cs.LG', 'cs.RO', '68T45', 'I.4.6; I.4.8']",285,156
Early Anomaly Detection in Time Series: A Hierarchical Approach for Predicting Critical Health Episodes,"The early detection of anomalous events in time series data is essential in
many domains of application. In this paper we deal with critical health events,
which represent a significant cause of mortality in intensive care units of
hospitals. The timely prediction of these events is crucial for mitigating
their consequences and improving healthcare. One of the most common approaches
to tackle early anomaly detection problems is standard classification methods.
In this paper we propose a novel method that uses a layered learning
architecture to address these tasks. One key contribution of our work is the
idea of pre-conditional events, which denote arbitrary but computable relaxed
versions of the event of interest. We leverage this idea to break the original
problem into two hierarchical layers, which we hypothesize are easier to solve.
The results suggest that the proposed approach leads to a better performance
relative to state of the art approaches for critical health episode prediction.","['stat.ML', 'cs.LG']",157,105
Deep Face Video Inpainting via UV Mapping,"This paper addresses the problem of face video inpainting. Existing video
inpainting methods target primarily at natural scenes with repetitive patterns.
They do not make use of any prior knowledge of the face to help retrieve
correspondences for the corrupted face. They therefore only achieve sub-optimal
results, particularly for faces under large pose and expression variations
where face components appear very differently across frames. In this paper, we
propose a two-stage deep learning method for face video inpainting. We employ
3DMM as our 3D face prior to transform a face between the image space and the
UV (texture) space. In Stage I, we perform face inpainting in the UV space.
This helps to largely remove the influence of face poses and expressions and
makes the learning task much easier with well aligned face features. We
introduce a frame-wise attention module to fully exploit correspondences in
neighboring frames to assist the inpainting task. In Stage II, we transform the
inpainted face regions back to the image space and perform face video
refinement that inpaints any background regions not covered in Stage I and also
refines the inpainted face regions. Extensive experiments have been carried out
which show our method can significantly outperform methods based merely on 2D
information, especially for faces under large pose and expression variations.",['cs.CV'],219,130
From Handcrafted to Deep Features for Pedestrian Detection: A Survey,"Pedestrian detection is an important but challenging problem in computer
vision, especially in human-centric tasks. Over the past decade, significant
improvement has been witnessed with the help of handcrafted features and deep
features. Here we present a comprehensive survey on recent advances in
pedestrian detection. First, we provide a detailed review of single-spectral
pedestrian detection that includes handcrafted features based methods and deep
features based approaches. For handcrafted features based methods, we present
an extensive review of approaches and find that handcrafted features with large
freedom degrees in shape and space have better performance. In the case of deep
features based approaches, we split them into pure CNN based methods and those
employing both handcrafted and CNN based features. We give the statistical
analysis and tendency of these methods, where feature enhanced, part-aware, and
post-processing methods have attracted main attention. In addition to
single-spectral pedestrian detection, we also review multi-spectral pedestrian
detection, which provides more robust features for illumination variance.
Furthermore, we introduce some related datasets and evaluation metrics, and
compare some representative methods. We conclude this survey by emphasizing
open problems that need to be addressed and highlighting various future
directions. Researchers can track an up-to-date list at
https://github.com/JialeCao001/PedSurvey.",['cs.CV'],212,134
Image Based Fashion Product Recommendation with Deep Learning,"We develop a two-stage deep learning framework that recommends fashion images
based on other input images of similar style. For that purpose, a neural
network classifier is used as a data-driven, visually-aware feature extractor.
The latter then serves as input for similarity-based recommendations using a
ranking algorithm. Our approach is tested on the publicly available Fashion
dataset. Initialization strategies using transfer learning from larger product
databases are presented. Combined with more traditional content-based
recommendation systems, our framework can help to increase robustness and
performance, for example, by better matching a particular customer style.",['cs.CV'],98,81
Developing a gender classification approach in human face images using modified local binary patterns and tani-moto based nearest neighbor algorithm,"Human identification is a much attention problem in computer vision. Gender
classification plays an important role in human identification as preprocess
step. So far, various methods have been proposed to solve this problem.
Absolutely, classification accuracy is the main challenge for researchers in
gender classification. But, some challenges such as rotation, gray scale
variations, pose, illumination changes may be occurred in smart phone image
capturing. In this respect, a multi step approach is proposed in this paper to
classify genders in human face images based on improved local binary patters
(MLBP). LBP is a texture descriptor, which extract local contrast and local
spatial structure information. Some issues such as noise sensitivity, rotation
sensitivity and low discriminative features can be considered as disadvantages
of the basic LBP. MLBP handle disadvantages using a new theory to categorize
extracted binary patterns of basic LBP. The proposed approach includes two
stages. First of all, a feature vector is extracted for human face images based
on MLBP. Next, non linear classifiers can be used to classify gender. In this
paper nearest neighborhood classifier is evaluated based on Tani-Moto metric as
distance measure. In the result part, two databases, self-collected and ICPR
are used as human face database. Results are compared by some state-ofthe-art
algorithms in this literature that shows the high quality of the proposed
approach in terms of accuracy rate. Some of other main advantages of the
proposed approach are rotation invariant, low noise sensitivity, size invariant
and low computational complexity. The proposed approach decreases the
computational complexity of smartphone applications because of reducing the
number of database comparisons. It can also improve performance of the
synchronous applications in the smarphones because of memory and CPU usage
reduction.",['cs.CV'],288,168
Deep Multimodal Learning: An Effective Method for Video Classification,"Videos have become ubiquitous on the Internet. And video analysis can provide
lots of information for detecting and recognizing objects as well as help
people understand human actions and interactions with the real world. However,
facing data as huge as TB level, effective methods should be applied. Recurrent
neural network (RNN) architecture has wildly been used on many sequential
learning problems such as Language Model, Time-Series Analysis, etc. In this
paper, we propose some variations of RNN such as stacked bidirectional LSTM/GRU
network with attention mechanism to categorize large-scale video data. We also
explore different multimodal fusion methods. Our model combines both visual and
audio information on both video and frame level and received great result.
Ensemble methods are also applied. Because of its multimodal characteristics,
we decide to call this method Deep Multimodal Learning(DML). Our DML-based
model was trained on Google Cloud and our own server and was tested in a
well-known video classification competition on Kaggle held by Google.","['cs.CV', 'cs.MM']",167,124
Colorization Transformer,"We present the Colorization Transformer, a novel approach for diverse high
fidelity image colorization based on self-attention. Given a grayscale image,
the colorization proceeds in three steps. We first use a conditional
autoregressive transformer to produce a low resolution coarse coloring of the
grayscale image. Our architecture adopts conditional transformer layers to
effectively condition grayscale input. Two subsequent fully parallel networks
upsample the coarse colored low resolution image into a finely colored high
resolution image. Sampling from the Colorization Transformer produces diverse
colorings whose fidelity outperforms the previous state-of-the-art on
colorising ImageNet based on FID results and based on a human evaluation in a
Mechanical Turk test. Remarkably, in more than 60% of cases human evaluators
prefer the highest rated among three generated colorings over the ground truth.
The code and pre-trained checkpoints for Colorization Transformer are publicly
available at
https://github.com/google-research/google-research/tree/master/coltran","['cs.CV', 'cs.AI', 'cs.LG']",155,102
Federated Knowledge Distillation,"Distributed learning frameworks often rely on exchanging model parameters
across workers, instead of revealing their raw data. A prime example is
federated learning that exchanges the gradients or weights of each neural
network model. Under limited communication resources, however, such a method
becomes extremely costly particularly for modern deep neural networks having a
huge number of model parameters. In this regard, federated distillation (FD) is
a compelling distributed learning solution that only exchanges the model
outputs whose dimensions are commonly much smaller than the model sizes (e.g.,
10 labels in the MNIST dataset). The goal of this chapter is to provide a deep
understanding of FD while demonstrating its communication efficiency and
applicability to a variety of tasks. To this end, towards demystifying the
operational principle of FD, the first part of this chapter provides a novel
asymptotic analysis for two foundational algorithms of FD, namely knowledge
distillation (KD) and co-distillation (CD), by exploiting the theory of neural
tangent kernel (NTK). Next, the second part elaborates on a baseline
implementation of FD for a classification task, and illustrates its performance
in terms of accuracy and communication efficiency compared to FL. Lastly, to
demonstrate the applicability of FD to various distributed learning tasks and
environments, the third part presents two selected applications, namely FD over
asymmetric uplink-and-downlink wireless channels and FD for reinforcement
learning.","['cs.LG', 'cs.DC', 'cs.IT', 'cs.NI', 'math.IT']",227,143
CalCROP21: A Georeferenced multi-spectral dataset of Satellite Imagery and Crop Labels,"Mapping and monitoring crops is a key step towards sustainable
intensification of agriculture and addressing global food security. A dataset
like ImageNet that revolutionized computer vision applications can accelerate
development of novel crop mapping techniques. Currently, the United States
Department of Agriculture (USDA) annually releases the Cropland Data Layer
(CDL) which contains crop labels at 30m resolution for the entire United States
of America. While CDL is state of the art and is widely used for a number of
agricultural applications, it has a number of limitations (e.g., pixelated
errors, labels carried over from previous errors and absence of input imagery
along with class labels). In this work, we create a new semantic segmentation
benchmark dataset, which we call CalCROP21, for the diverse crops in the
Central Valley region of California at 10m spatial resolution using a Google
Earth Engine based robust image processing pipeline and a novel attention based
spatio-temporal semantic segmentation algorithm STATT. STATT uses re-sampled
(interpolated) CDL labels for training, but is able to generate a better
prediction than CDL by leveraging spatial and temporal patterns in Sentinel2
multi-spectral image series to effectively capture phenologic differences
amongst crops and uses attention to reduce the impact of clouds and other
atmospheric disturbances. We also present a comprehensive evaluation to show
that STATT has significantly better results when compared to the resampled CDL
labels. We have released the dataset and the processing pipeline code for
generating the benchmark dataset.",['cs.CV'],244,157
Conditional Video Generation Using Action-Appearance Captions,"The field of automatic video generation has received a boost thanks to the
recent Generative Adversarial Networks (GANs). However, most existing methods
cannot control the contents of the generated video using a text caption, losing
their usefulness to a large extent. This particularly affects human videos due
to their great variety of actions and appearances. This paper presents
Conditional Flow and Texture GAN (CFT-GAN), a GAN-based video generation method
from action-appearance captions. We propose a novel way of generating video by
encoding a caption (e.g., ""a man in blue jeans is playing golf"") in a two-stage
generation pipeline. Our CFT-GAN uses such caption to generate an optical flow
(action) and a texture (appearance) for each frame. As a result, the output
video reflects the content specified in the caption in a plausible way.
Moreover, to train our method, we constructed a new dataset for human video
generation with captions. We evaluated the proposed method qualitatively and
quantitatively via an ablation study and a user study. The results demonstrate
that CFT-GAN is able to successfully generate videos containing the action and
appearances indicated in the captions.",['cs.CV'],192,118
High-Fidelity and Arbitrary Face Editing,"Cycle consistency is widely used for face editing. However, we observe that
the generator tends to find a tricky way to hide information from the original
image to satisfy the constraint of cycle consistency, making it impossible to
maintain the rich details (e.g., wrinkles and moles) of non-editing areas. In
this work, we propose a simple yet effective method named HifaFace to address
the above-mentioned problem from two perspectives. First, we relieve the
pressure of the generator to synthesize rich details by directly feeding the
high-frequency information of the input image into the end of the generator.
Second, we adopt an additional discriminator to encourage the generator to
synthesize rich details. Specifically, we apply wavelet transformation to
transform the image into multi-frequency domains, among which the
high-frequency parts can be used to recover the rich details. We also notice
that a fine-grained and wider-range control for the attribute is of great
importance for face editing. To achieve this goal, we propose a novel attribute
regression loss. Powered by the proposed framework, we achieve high-fidelity
and arbitrary face editing, outperforming other state-of-the-art approaches.",['cs.CV'],194,116
Asymmetric Learning Vector Quantization for Efficient Nearest Neighbor Classification in Dynamic Time Warping Spaces,"The nearest neighbor method together with the dynamic time warping (DTW)
distance is one of the most popular approaches in time series classification.
This method suffers from high storage and computation requirements for large
training sets. As a solution to both drawbacks, this article extends learning
vector quantization (LVQ) from Euclidean spaces to DTW spaces. The proposed LVQ
scheme uses asymmetric weighted averaging as update rule. Empirical results
exhibited superior performance of asymmetric generalized LVQ (GLVQ) over other
state-of-the-art prototype generation methods for nearest neighbor
classification.","['cs.LG', 'stat.ML']",89,71
Saliency Methods for Explaining Adversarial Attacks,"The classification decisions of neural networks can be misled by small
imperceptible perturbations. This work aims to explain the misled
classifications using saliency methods. The idea behind saliency methods is to
explain the classification decisions of neural networks by creating so-called
saliency maps. Unfortunately, a number of recent publications have shown that
many of the proposed saliency methods do not provide insightful explanations. A
prominent example is Guided Backpropagation (GuidedBP), which simply performs
(partial) image recovery. However, our numerical analysis shows the saliency
maps created by GuidedBP do indeed contain class-discriminative information. We
propose a simple and efficient way to enhance the saliency maps. The proposed
enhanced GuidedBP shows the state-of-the-art performance to explain adversary
classifications.",['cs.CV'],121,80
Augmented Transformer with Adaptive Graph for Temporal Action Proposal Generation,"Temporal action proposal generation (TAPG) is a fundamental and challenging
task in video understanding, especially in temporal action detection. Most
previous works focus on capturing the local temporal context and can well
locate simple action instances with clean frames and clear boundaries. However,
they generally fail in complicated scenarios where interested actions involve
irrelevant frames and background clutters, and the local temporal context
becomes less effective. To deal with these problems, we present an augmented
transformer with adaptive graph network (ATAG) to exploit both long-range and
local temporal contexts for TAPG. Specifically, we enhance the vanilla
transformer by equipping a snippet actionness loss and a front block, dubbed
augmented transformer, and it improves the abilities of capturing long-range
dependencies and learning robust feature for noisy action instances.Moreover,
an adaptive graph convolutional network (GCN) is proposed to build local
temporal context by mining the position information and difference between
adjacent features. The features from the two modules carry rich semantic
information of the video, and are fused for effective sequential proposal
generation. Extensive experiments are conducted on two challenging datasets,
THUMOS14 and ActivityNet1.3, and the results demonstrate that our method
outperforms state-of-the-art TAPG methods. Our code will be released soon.",['cs.CV'],206,136
Bridging the gap between AI and Healthcare sides: towards developing clinically relevant AI-powered diagnosis systems,"Despite the success of Convolutional Neural Network-based Computer-Aided
Diagnosis research, its clinical applications remain challenging. Accordingly,
developing medical Artificial Intelligence (AI) fitting into a clinical
environment requires identifying/bridging the gap between AI and Healthcare
sides. Since the biggest problem in Medical Imaging lies in data paucity,
confirming the clinical relevance for diagnosis of research-proven image
augmentation techniques is essential. Therefore, we hold a clinically valuable
AI-envisioning workshop among Japanese Medical Imaging experts, physicians, and
generalists in Healthcare/Informatics. Then, a questionnaire survey for
physicians evaluates our pathology-aware Generative Adversarial Network
(GAN)-based image augmentation projects in terms of Data Augmentation and
physician training. The workshop reveals the intrinsic gap between
AI/Healthcare sides and solutions on Why (i.e., clinical
significance/interpretation) and How (i.e., data acquisition, commercial
deployment, and safety/feeling safe). This analysis confirms our
pathology-aware GANs' clinical relevance as a clinical decision support system
and non-expert physician training tool. Our findings would play a key role in
connecting inter-disciplinary research and clinical applications, not limited
to the Japanese medical context and pathology-aware GANs.","['cs.CV', 'cs.LG', 'eess.IV']",187,124
Distant Transfer Learning via Deep Random Walk,"Transfer learning, which is to improve the learning performance in the target
domain by leveraging useful knowledge from the source domain, often requires
that those two domains are very close, which limits its application scope.
Recently, distant transfer learning has been studied to transfer knowledge
between two distant or even totally unrelated domains via auxiliary domains
that are usually unlabeled as a bridge in the spirit of human transitive
inference that it is possible to connect two completely unrelated concepts
together through gradual knowledge transfer. In this paper, we study distant
transfer learning by proposing a DeEp Random Walk basEd distaNt Transfer
(DERWENT) method. Different from existing distant transfer learning models that
implicitly identify the path of knowledge transfer between the source and
target instances through auxiliary instances, the proposed DERWENT model can
explicitly learn such paths via the deep random walk technique. Specifically,
based on sequences identified by the random walk technique on a data graph
where source and target data have no direct edges, the proposed DERWENT model
enforces adjacent data points in a squence to be similar, makes the ending data
point be represented by other data points in the same sequence, and considers
weighted training losses of source data. Empirical studies on several benchmark
datasets demonstrate that the proposed DERWENT algorithm yields the
state-of-the-art performance.","['cs.LG', 'stat.ML']",222,132
Short-term Hourly Streamflow Prediction with Graph Convolutional GRU Networks,"The frequency and impact of floods are expected to increase due to climate
change. It is crucial to predict streamflow, consequently flooding, in order to
prepare and mitigate its consequences in terms of property damage and
fatalities. This paper presents a Graph Convolutional GRUs based model to
predict the next 36 hours of streamflow for a sensor location using the
upstream river network. As shown in experiment results, the model presented in
this study provides better performance than the persistence baseline and a
Convolutional Bidirectional GRU network for the selected study area in
short-term streamflow prediction.","['cs.LG', 'eess.SP']",97,70
A Hybrid Framework for Tumor Saliency Estimation,"Automatic tumor segmentation of breast ultrasound (BUS) image is quite
challenging due to the complicated anatomic structure of breast and poor image
quality. Most tumor segmentation approaches achieve good performance on BUS
images collected in controlled settings; however, the performance degrades
greatly with BUS images from different sources. Tumor saliency estimation (TSE)
has attracted increasing attention to solving the problem by modeling
radiologists' attention mechanism. In this paper, we propose a novel hybrid
framework for TSE, which integrates both high-level domain-knowledge and robust
low-level saliency assumptions and can overcome drawbacks caused by direct
mapping in traditional TSE approaches. The new framework integrated the
Neutro-Connectedness (NC) map, the adaptive-center, the correlation and the
layer structure-based weighted map. The experimental results demonstrate that
the proposed approach outperforms state-of-the-art TSE methods.",['cs.CV'],137,102
GAN-based Projector for Faster Recovery with Convergence Guarantees in Linear Inverse Problems,"A Generative Adversarial Network (GAN) with generator $G$ trained to model
the prior of images has been shown to perform better than sparsity-based
regularizers in ill-posed inverse problems. Here, we propose a new method of
deploying a GAN-based prior to solve linear inverse problems using projected
gradient descent (PGD). Our method learns a network-based projector for use in
the PGD algorithm, eliminating expensive computation of the Jacobian of $G$.
Experiments show that our approach provides a speed-up of $60\text{-}80\times$
over earlier GAN-based recovery methods along with better accuracy. Our main
theoretical result is that if the measurement matrix is moderately conditioned
on the manifold range($G$) and the projector is $\delta$-approximate, then the
algorithm is guaranteed to reach $O(\delta)$ reconstruction error in
$O(log(1/\delta))$ steps in the low noise regime. Additionally, we propose a
fast method to design such measurement matrices for a given $G$. Extensive
experiments demonstrate the efficacy of this method by requiring
$5\text{-}10\times$ fewer measurements than random Gaussian measurement
matrices for comparable recovery performance. Because the learning of the GAN
and projector is decoupled from the measurement operator, our GAN-based
projector and recovery algorithm are applicable without retraining to all
linear inverse problems, as confirmed by experiments on compressed sensing,
super-resolution, and inpainting.","['cs.LG', 'eess.IV', 'stat.ML']",224,137
Reading Between the Pixels: Photographic Steganography for Camera Display Messaging,"We exploit human color metamers to send light-modulated messages less visible
to the human eye, but recoverable by cameras. These messages are a key
component to camera-display messaging, such as handheld smartphones capturing
information from electronic signage. Each color pixel in the display image is
modified by a particular color gradient vector. The challenge is to find the
color gradient that maximizes camera response, while minimizing human response.
The mismatch in human spectral and camera sensitivity curves creates an
opportunity for hidden messaging. Our approach does not require knowledge of
these sensitivity curves, instead we employ a data-driven method. We learn an
ellipsoidal partitioning of the six-dimensional space of colors and color
gradients. This partitioning creates metamer sets defined by the base color at
the display pixel and the color gradient direction for message encoding. We
sample from the resulting metamer sets to find color steps for each base color
to embed a binary message into an arbitrary image with reduced visible
artifacts. Unlike previous methods that rely on visually obtrusive intensity
modulation, we embed with color so that the message is more hidden. Ordinary
displays and cameras are used without the need for expensive LEDs or high speed
devices. The primary contribution of this work is a framework to map the pixels
in an arbitrary image to a metamer pair for steganographic photo messaging.","['cs.CV', 'cs.GR', 'cs.MM', 'cs.NI', 'I.4.8']",229,136
Neural Additive Vector Autoregression Models for Causal Discovery in Time Series Data,"Causal structure discovery in complex dynamical systems is an important
challenge for many scientific domains. Although data from (interventional)
experiments is usually limited, large amounts of observational time series data
sets are usually available. Current methods that learn causal structure from
time series often assume linear relationships. Hence, they may fail in
realistic settings that contain nonlinear relations between the variables. We
propose Neural Additive Vector Autoregression (NAVAR) models, a neural approach
to causal structure learning that can discover nonlinear relationships. We
train deep neural networks that extract the (additive) Granger causal
influences from the time evolution in multi-variate time series. The method
achieves state-of-the-art results on various benchmark data sets for causal
discovery, while providing clear interpretations of the mapped causal
relations.",['cs.LG'],127,91
Deep Learning for Automatic Quality Grading of Mangoes: Methods and Insights,"The quality grading of mangoes is a crucial task for mango growers as it
vastly affects their profit. However, until today, this process still relies on
laborious efforts of humans, who are prone to fatigue and errors. To remedy
this, the paper approaches the grading task with various convolutional neural
networks (CNN), a tried-and-tested deep learning technology in computer vision.
The models involved include Mask R-CNN (for background removal), the numerous
past winners of the ImageNet challenge, namely AlexNet, VGGs, and ResNets; and,
a family of self-defined convolutional autoencoder-classifiers (ConvAE-Clfs)
inspired by the claimed benefit of multi-task learning in classification tasks.
Transfer learning is also adopted in this work via utilizing the ImageNet
pretrained weights. Besides elaborating on the preprocessing techniques,
training details, and the resulting performance, we go one step further to
provide explainable insights into the model's working with the help of saliency
maps and principal component analysis (PCA). These insights provide a succinct,
meaningful glimpse into the intricate deep learning black box, fostering trust,
and can also be presented to humans in real-world use cases for reviewing the
grading results.",['cs.CV'],192,138
Towards Better Forecasting by Fusing Near and Distant Future Visions,"Multivariate time series forecasting is an important yet challenging problem
in machine learning. Most existing approaches only forecast the series value of
one future moment, ignoring the interactions between predictions of future
moments with different temporal distance. Such a deficiency probably prevents
the model from getting enough information about the future, thus limiting the
forecasting accuracy. To address this problem, we propose Multi-Level Construal
Neural Network (MLCNN), a novel multi-task deep learning framework. Inspired by
the Construal Level Theory of psychology, this model aims to improve the
predictive performance by fusing forecasting information (i.e., future visions)
of different future time. We first use the Convolution Neural Network to
extract multi-level abstract representations of the raw data for near and
distant future predictions. We then model the interplay between multiple
predictive tasks and fuse their future visions through a modified
Encoder-Decoder architecture. Finally, we combine traditional Autoregression
model with the neural network to solve the scale insensitive problem.
Experiments on three real-world datasets show that our method achieves
statistically significant improvements compared to the most state-of-the-art
baseline methods, with average 4.59% reduction on RMSE metric and average 6.87%
reduction on MAE metric.","['cs.LG', 'stat.ML']",202,139
PVT: Point-Voxel Transformer for 3D Deep Learning,"In this paper, we present an efficient and high-performance neural
architecture, termed Point-Voxel Transformer (PVT)for 3D deep learning, which
deeply integrates both 3D voxel-based and point-based self-attention
computation to learn more discriminative features from 3D data. Specifically,
we conduct multi-head self-attention (MSA) computation in voxels to obtain the
efficient learning pattern and the coarse-grained local features while
performing self-attention in points to provide finer-grained information about
the global context. In addition, to reduce the cost of MSA computation with
high efficiency, we design a cyclic shifted boxing scheme by limiting the MSA
computation to non-overlapping local box and also preserving cross-box
connection. Evaluated on classification benchmark, our method not only achieves
state-of-the-art accuracy of 94.0% (no voting) but outperforms previous
Transformer-based models with 7x measured speedup on average. On part and
semantic segmentation, our model also obtains strong performance(86.5% and
68.2% mIoU, respectively). For 3D object detection task, we replace the
primitives in Frustrum PointNet with PVT block and achieve an improvement of
8.6% AP.","['cs.CV', 'cs.AI', 'cs.GR']",187,132
Learning Parallel Dense Correspondence from Spatio-Temporal Descriptors for Efficient and Robust 4D Reconstruction,"This paper focuses on the task of 4D shape reconstruction from a sequence of
point clouds. Despite the recent success achieved by extending deep implicit
representations into 4D space, it is still a great challenge in two respects,
i.e. how to design a flexible framework for learning robust spatio-temporal
shape representations from 4D point clouds, and develop an efficient mechanism
for capturing shape dynamics. In this work, we present a novel pipeline to
learn a temporal evolution of the 3D human shape through spatially continuous
transformation functions among cross-frame occupancy fields. The key idea is to
parallelly establish the dense correspondence between predicted occupancy
fields at different time steps via explicitly learning continuous displacement
vector fields from robust spatio-temporal shape representations. Extensive
comparisons against previous state-of-the-arts show the superior accuracy of
our approach for 4D human reconstruction in the problems of 4D shape
auto-encoding and completion, and a much faster network inference with about 8
times speedup demonstrates the significant efficiency of our approach. The
trained models and implementation code are available at
https://github.com/tangjiapeng/LPDC-Net.",['cs.CV'],187,130
Optimal Combination Forecasts on Retail Multi-Dimensional Sales Data,"Time series data in the retail world are particularly rich in terms of
dimensionality, and these dimensions can be aggregated in groups or
hierarchies. Valuable information is nested in these complex structures, which
helps to predict the aggregated time series data. From a portfolio of brands
under HUUB's monitoring, we selected two to explore their sales behaviour,
leveraging the grouping properties of their product structure. Using
statistical models, namely SARIMA, to forecast each level of the hierarchy, an
optimal combination approach was used to generate more consistent forecasts in
the higher levels. Our results show that the proposed methods can indeed
capture nested information in the more granular series, helping to improve the
forecast accuracy of the aggregated series. The Weighted Least Squares (WLS)
method surpasses all other methods proposed in the study, including the Minimum
Trace (MinT) reconciliation.","['stat.ML', 'cs.LG', 'stat.AP', 'stat.ME']",140,101
Measuring Compositionality in Representation Learning,"Many machine learning algorithms represent input data with vector embeddings
or discrete codes. When inputs exhibit compositional structure (e.g. objects
built from parts or procedures from subroutines), it is natural to ask whether
this compositional structure is reflected in the the inputs' learned
representations. While the assessment of compositionality in languages has
received significant attention in linguistics and adjacent fields, the machine
learning literature lacks general-purpose tools for producing graded
measurements of compositional structure in more general (e.g. vector-valued)
representation spaces. We describe a procedure for evaluating compositionality
by measuring how well the true representation-producing model can be
approximated by a model that explicitly composes a collection of inferred
representational primitives. We use the procedure to provide formal and
empirical characterizations of compositional structure in a variety of
settings, exploring the relationship between compositionality and learning
dynamics, human judgments, representational similarity, and generalization.","['cs.LG', 'cs.CL', 'stat.ML']",148,100
"""Train one, Classify one, Teach one"" -- Cross-surgery transfer learning for surgical step recognition","Prior work demonstrated the ability of machine learning to automatically
recognize surgical workflow steps from videos. However, these studies focused
on only a single type of procedure. In this work, we analyze, for the first
time, surgical step recognition on four different laparoscopic surgeries:
Cholecystectomy, Right Hemicolectomy, Sleeve Gastrectomy, and Appendectomy.
Inspired by the traditional apprenticeship model, in which surgical training is
based on the Halstedian method, we paraphrase the ""see one, do one, teach one""
approach for the surgical intelligence domain as ""train one, classify one,
teach one"". In machine learning, this approach is often referred to as transfer
learning. To analyze the impact of transfer learning across different
laparoscopic procedures, we explore various time-series architectures and
examine their performance on each target domain. We introduce a new
architecture, the Time-Series Adaptation Network (TSAN), an architecture
optimized for transfer learning of surgical step recognition, and we show how
TSAN can be pre-trained using self-supervised learning on a Sequence Sorting
task. Such pre-training enables TSAN to learn workflow steps of a new
laparoscopic procedure type from only a small number of labeled samples from
the target procedure. Our proposed architecture leads to better performance
compared to other possible architectures, reaching over 90% accuracy when
transferring from laparoscopic Cholecystectomy to the other three procedure
types.","['cs.CV', 'cs.AI']",219,130
Hybrid Quantum-Classical Graph Convolutional Network,"The high energy physics (HEP) community has a long history of dealing with
large-scale datasets. To manage such voluminous data, classical machine
learning and deep learning techniques have been employed to accelerate physics
discovery. Recent advances in quantum machine learning (QML) have indicated the
potential of applying these techniques in HEP. However, there are only limited
results in QML applications currently available. In particular, the challenge
of processing sparse data, common in HEP datasets, has not been extensively
studied in QML models. This research provides a hybrid quantum-classical graph
convolutional network (QGCNN) for learning HEP data. The proposed framework
demonstrates an advantage over classical multilayer perceptron and
convolutional neural networks in the aspect of number of parameters. Moreover,
in terms of testing accuracy, the QGCNN shows comparable performance to a
quantum convolutional neural network on the same HEP dataset while requiring
less than $50\%$ of the parameters. Based on numerical simulation results,
studying the application of graph convolutional operations and other QML models
may prove promising in advancing HEP research and other scientific fields.","['cs.LG', 'cs.CV', 'hep-ex', 'physics.data-an', 'quant-ph']",176,112
Image-to-Image Translation with Conditional Adversarial Networks,"We investigate conditional adversarial networks as a general-purpose solution
to image-to-image translation problems. These networks not only learn the
mapping from input image to output image, but also learn a loss function to
train this mapping. This makes it possible to apply the same generic approach
to problems that traditionally would require very different loss formulations.
We demonstrate that this approach is effective at synthesizing photos from
label maps, reconstructing objects from edge maps, and colorizing images, among
other tasks. Indeed, since the release of the pix2pix software associated with
this paper, a large number of internet users (many of them artists) have posted
their own experiments with our system, further demonstrating its wide
applicability and ease of adoption without the need for parameter tweaking. As
a community, we no longer hand-engineer our mapping functions, and this work
suggests we can achieve reasonable results without hand-engineering our loss
functions either.",['cs.CV'],155,112
Learning Non-Parametric Invariances from Data with Permanent Random Connectomes,"One of the fundamental problems in supervised classification and in machine
learning in general, is the modelling of non-parametric invariances that exist
in data. Most prior art has focused on enforcing priors in the form of
invariances to parametric nuisance transformations that are expected to be
present in data. Learning non-parametric invariances directly from data remains
an important open problem. In this paper, we introduce a new architectural
layer for convolutional networks which is capable of learning general
invariances from data itself. This layer can learn invariance to non-parametric
transformations and interestingly, motivates and incorporates permanent random
connectomes, thereby being called Permanent Random Connectome Non-Parametric
Transformation Networks (PRC-NPTN). PRC-NPTN networks are initialized with
random connections (not just weights) which are a small subset of the
connections in a fully connected convolution layer. Importantly, these
connections in PRC-NPTNs once initialized remain permanent throughout training
and testing. Permanent random connectomes make these architectures loosely more
biologically plausible than many other mainstream network architectures which
require highly ordered structures. We motivate randomly initialized connections
as a simple method to learn invariance from data itself while invoking
invariance towards multiple nuisance transformations simultaneously. We find
that these randomly initialized permanent connections have positive effects on
generalization, outperform much larger ConvNet baselines and the recently
proposed Non-Parametric Transformation Network (NPTN) on benchmarks that
enforce learning invariances from the data itself.","['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",233,141
ADSaS: Comprehensive Real-time Anomaly Detection System,"Since with massive data growth, the need for autonomous and generic anomaly
detection system is increased. However, developing one stand-alone generic
anomaly detection system that is accurate and fast is still a challenge. In
this paper, we propose conventional time-series analysis approaches, the
Seasonal Autoregressive Integrated Moving Average (SARIMA) model and Seasonal
Trend decomposition using Loess (STL), to detect complex and various anomalies.
Usually, SARIMA and STL are used only for stationary and periodic time-series,
but by combining, we show they can detect anomalies with high accuracy for data
that is even noisy and non-periodic. We compared the algorithm to Long Short
Term Memory (LSTM), a deep-learning-based algorithm used for anomaly detection
system. We used a total of seven real-world datasets and four artificial
datasets with different time-series properties to verify the performance of the
proposed algorithm.","['cs.LG', 'stat.ML']",145,95
Randomized Transferable Machine,"Feature-based transfer is one of the most effective methodologies for
transfer learning. Existing studies usually assume that the learned new feature
representation is truly \emph{domain-invariant}, and thus directly train a
transfer model $\mathcal{M}$ on source domain. In this paper, we consider a
more realistic scenario where the new feature representation is suboptimal and
small divergence still exists across domains. We propose a new learning
strategy with a transfer model called Randomized Transferable Machine (RTM).
More specifically, we work on source data with the new feature representation
learned from existing feature-based transfer methods. The key idea is to
enlarge source training data populations by randomly corrupting source data
using some noises, and then train a transfer model $\widetilde{\mathcal{M}}$
that performs well on all the corrupted source data populations. In principle,
the more corruptions are made, the higher the probability of the target data
can be covered by the constructed source populations, and thus better transfer
performance can be achieved by $\widetilde{\mathcal{M}}$. An ideal case is with
infinite corruptions, which however is infeasible in reality. We develop a
marginalized solution with linear regression model and dropout noise. With a
marginalization trick, we can train an RTM that is equivalently to training
using infinite source noisy populations without truly conducting any
corruption. More importantly, such an RTM has a closed-form solution, which
enables very fast and efficient training. Extensive experiments on various
real-world transfer tasks show that RTM is a promising transfer model.","['cs.LG', 'cs.AI']",251,143
Fast Learning and Prediction for Object Detection using Whitened CNN Features,"We combine features extracted from pre-trained convolutional neural networks
(CNNs) with the fast, linear Exemplar-LDA classifier to get the advantages of
both: the high detection performance of CNNs, automatic feature engineering,
fast model learning from few training samples and efficient sliding-window
detection. The Adaptive Real-Time Object Detection System (ARTOS) has been
refactored broadly to be used in combination with Caffe for the experimental
studies reported in this work.",['cs.CV'],72,61
Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction,"To overcome the limitations of Neural Programmer-Interpreters (NPI) in its
universality and learnability, we propose the incorporation of combinator
abstraction into neural programing and a new NPI architecture to support this
abstraction, which we call Combinatory Neural Programmer-Interpreter (CNPI).
Combinator abstraction dramatically reduces the number and complexity of
programs that need to be interpreted by the core controller of CNPI, while
still allowing the CNPI to represent and interpret arbitrary complex programs
by the collaboration of the core with the other components. We propose a small
set of four combinators to capture the most pervasive programming patterns. Due
to the finiteness and simplicity of this combinator set and the offloading of
some burden of interpretation from the core, we are able construct a CNPI that
is universal with respect to the set of all combinatorizable programs, which is
adequate for solving most algorithmic tasks. Moreover, besides supervised
training on execution traces, CNPI can be trained by policy gradient
reinforcement learning with appropriately designed curricula.",['cs.LG'],166,104
Investigating Code-Mixed Modern Standard Arabic-Egyptian to English Machine Translation,"Recent progress in neural machine translation (NMT) has made it possible to
translate successfully between monolingual language pairs where large parallel
data exist, with pre-trained models improving performance even further.
Although there exists work on translating in code-mixed settings (where one of
the pairs includes text from two or more languages), it is still unclear what
recent success in NMT and language modeling exactly means for translating
code-mixed text. We investigate one such context, namely MT from code-mixed
Modern Standard Arabic and Egyptian Arabic (MSAEA) into English. We develop
models under different conditions, employing both (i) standard end-to-end
sequence-to-sequence (S2S) Transformers trained from scratch and (ii)
pre-trained S2S language models (LMs). We are able to acquire reasonable
performance using only MSA-EN parallel data with S2S models trained from
scratch. We also find LMs fine-tuned on data from various Arabic dialects to
help the MSAEA-EN task. Our work is in the context of the Shared Task on
Machine Translation in Code-Switching. Our best model achieves $\bf25.72$ BLEU,
placing us first on the official shared task evaluation for MSAEA-EN.","['cs.LG', 'cs.CL']",192,124
Predicting Rigid Body Dynamics using Dual Quaternion Recurrent Neural Networks with Quaternion Attention,"We propose a novel neural network architecture based on dual quaternions
which allow for a compact representation of informations with a main focus on
describing rigid body movements. To cover the dynamic behavior inherent to
rigid body movements, we propose recurrent architectures in the neural network.
To further model the interactions between individual rigid bodies as well as
external inputs efficiently, we incorporate a novel attention mechanism
employing dual quaternion algebra. The introduced architecture is trainable by
means of gradient based algorithms. We apply our approach to a parcel
prediction problem where a rigid body with an initial position, orientation,
velocity and angular velocity moves through a fixed simulation environment
which exhibits rich interactions between the parcel and the boundaries.",['cs.LG'],120,83
Generative Flow via Invertible nxn Convolution,"Flow-based generative models have recently become one of the most efficient
approaches to model data generation. Indeed, they are constructed with a
sequence of invertible and tractable transformations. Glow first introduced a
simple type of generative flow using an invertible $1 \times 1$ convolution.
However, the $1 \times 1$ convolution suffers from limited flexibility compared
to the standard convolutions. In this paper, we propose a novel invertible $n
\times n$ convolution approach that overcomes the limitations of the invertible
$1 \times 1$ convolution. In addition, our proposed network is not only
tractable and invertible but also uses fewer parameters than standard
convolutions. The experiments on CIFAR-10, ImageNet and Celeb-HQ datasets, have
shown that our invertible $n \times n$ convolution helps to improve the
performance of generative models significantly.",['cs.CV'],131,85
A Survey of Model Compression and Acceleration for Deep Neural Networks,"Deep neural networks (DNNs) have recently achieved great success in many
visual recognition tasks. However, existing deep neural network models are
computationally expensive and memory intensive, hindering their deployment in
devices with low memory resources or in applications with strict latency
requirements. Therefore, a natural thought is to perform model compression and
acceleration in deep networks without significantly decreasing the model
performance. During the past five years, tremendous progress has been made in
this area. In this paper, we review the recent techniques for compacting and
accelerating DNN models. In general, these techniques are divided into four
categories: parameter pruning and quantization, low-rank factorization,
transferred/compact convolutional filters, and knowledge distillation. Methods
of parameter pruning and quantization are described first, after that the other
techniques are introduced. For each category, we also provide insightful
analysis about the performance, related applications, advantages, and
drawbacks. Then we go through some very recent successful methods, for example,
dynamic capacity networks and stochastic depths networks. After that, we survey
the evaluation matrices, the main datasets used for evaluating the model
performance, and recent benchmark efforts. Finally, we conclude this paper,
discuss remaining the challenges and possible directions for future work.","['cs.LG', 'cs.CV']",197,140
Compressive Transformers for Long-Range Sequence Modelling,"We present the Compressive Transformer, an attentive sequence model which
compresses past memories for long-range sequence learning. We find the
Compressive Transformer obtains state-of-the-art language modelling results in
the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc
respectively. We also find it can model high-frequency speech effectively and
can be used as a memory mechanism for RL, demonstrated on an object matching
task. To promote the domain of long-range sequence learning, we propose a new
open-vocabulary language modelling benchmark derived from books, PG-19.","['cs.LG', 'stat.ML']",96,72
Morphology on categorical distributions,"The categorical distribution is a natural representation of uncertainty in
multi-class segmentations. In the two-class case the categorical distribution
reduces to the Bernoulli distribution, for which grayscale morphology provides
a range of useful operations. In the general case, applying morphological
operations on uncertain multi-class segmentations is not straightforward as an
image of categorical distributions is not a complete lattice. Although
morphology on color images has received wide attention, this is not so for
color-coded or categorical images and even less so for images of categorical
distributions. In this work, we establish a set of requirements for morphology
on categorical distributions by combining classic morphology with a
probabilistic view. We then define operators respecting these requirements,
introduce protected operations on categorical distributions and illustrate the
utility of these operators on two example tasks: modeling annotator bias in
brain tumor segmentations and segmenting vesicle instances from the predictions
of a multi-class U-Net.",['cs.CV'],156,90
Image Retrieval for Structure-from-Motion via Graph Convolutional Network,"Conventional image retrieval techniques for Structure-from-Motion (SfM)
suffer from the limit of effectively recognizing repetitive patterns and cannot
guarantee to create just enough match pairs with high precision and high
recall. In this paper, we present a novel retrieval method based on Graph
Convolutional Network (GCN) to generate accurate pairwise matches without
costly redundancy. We formulate image retrieval task as a node binary
classification problem in graph data: a node is marked as positive if it shares
the scene overlaps with the query image. The key idea is that we find that the
local context in feature space around a query image contains rich information
about the matchable relation between this image and its neighbors. By
constructing a subgraph surrounding the query image as input data, we adopt a
learnable GCN to exploit whether nodes in the subgraph have overlapping regions
with the query photograph. Experiments demonstrate that our method performs
remarkably well on the challenging dataset of highly ambiguous and duplicated
scenes. Besides, compared with state-of-the-art matchable retrieval methods,
the proposed approach significantly reduces useless attempted matches without
sacrificing the accuracy and completeness of reconstruction.",['cs.CV'],191,131
Quantization and Deployment of Deep Neural Networks on Microcontrollers,"Embedding Artificial Intelligence onto low-power devices is a challenging
task that has been partly overcome with recent advances in machine learning and
hardware design. Presently, deep neural networks can be deployed on embedded
targets to perform different tasks such as speech recognition,object detection
or Human Activity Recognition. However, there is still room for optimization of
deep neural networks onto embedded devices. These optimizations mainly address
power consumption,memory and real-time constraints, but also an easier
deployment at the edge. Moreover, there is still a need for a better
understanding of what can be achieved for different use cases. This work
focuses on quantization and deployment of deep neural networks onto low-power
32-bit microcontrollers. The quantization methods, relevant in the context of
an embedded execution onto a microcontroller, are first outlined. Then, a new
framework for end-to-end deep neural networks training, quantization and
deployment is presented. This framework, called MicroAI, is designed as an
alternative to existing inference engines (TensorFlow Lite for Microcontrollers
and STM32Cube.AI). Our framework can indeed be easily adjusted and/or extended
for specific use cases. Execution using single precision 32-bit floating-point
as well as fixed-point on 8- and 16-bit integers are supported. The proposed
quantization method is evaluated with three different datasets (UCI-HAR, Spoken
MNIST and GTSRB). Finally, a comparison study between MicroAI and both existing
embedded inference engines is provided in terms of memory and power efficiency.
On-device evaluation is done using ARM Cortex-M4F-based microcontrollers (Ambiq
Apollo3 and STM32L452RE).","['cs.LG', 'eess.SP']",259,162
Adversarial Attacks and Detection on Reinforcement Learning-Based Interactive Recommender Systems,"Adversarial attacks pose significant challenges for detecting adversarial
attacks at an early stage. We propose attack-agnostic detection on
reinforcement learning-based interactive recommendation systems. We first craft
adversarial examples to show their diverse distributions and then augment
recommendation systems by detecting potential attacks with a deep
learning-based classifier based on the crafted data. Finally, we study the
attack strength and frequency of adversarial examples and evaluate our model on
standard datasets with multiple crafting methods. Our extensive experiments
show that most adversarial attacks are effective, and both attack strength and
attack frequency impact the attack performance. The strategically-timed attack
achieves comparative attack performance with only 1/3 to 1/2 attack frequency.
Besides, our black-box detector trained with one crafting method has the
generalization ability over several crafting methods.","['cs.LG', 'cs.CR', 'cs.IR']",133,89
Tracking Multiple Moving Objects Using Unscented Kalman Filtering Techniques,"It is an important task to reliably detect and track multiple moving objects
for video surveillance and monitoring. However, when occlusion occurs in
nonlinear motion scenarios, many existing methods often fail to continuously
track multiple moving objects of interest. In this paper we propose an
effective approach for detection and tracking of multiple moving objects with
occlusion. Moving targets are initially detected using a simple yet efficient
block matching technique, providing rough location information for multiple
object tracking. More accurate location information is then estimated for each
moving object by a nonlinear tracking algorithm. Considering the ambiguity
caused by the occlusion among multiple moving objects, we apply an unscented
Kalman filtering (UKF) technique for reliable object detection and tracking.
Different from conventional Kalman filtering (KF), which cannot achieve the
optimal estimation in nonlinear tracking scenarios, UKF can be used to track
both linear and nonlinear motions due to the unscented transform. Further, it
estimates the velocity information for each object to assist to the object
detection algorithm, effectively delineating multiple moving objects of
occlusion. The experimental results demonstrate that the proposed method can
correctly detect and track multiple moving objects with nonlinear motion
patterns and occlusions.",['cs.CV'],196,113
Deep Generative Video Compression,"The usage of deep generative models for image compression has led to
impressive performance gains over classical codecs while neural video
compression is still in its infancy. Here, we propose an end-to-end, deep
generative modeling approach to compress temporal sequences with a focus on
video. Our approach builds upon variational autoencoder (VAE) models for
sequential data and combines them with recent work on neural image compression.
The approach jointly learns to transform the original sequence into a
lower-dimensional representation as well as to discretize and entropy code this
representation according to predictions of the sequential VAE. Rate-distortion
evaluations on small videos from public data sets with varying complexity and
diversity show that our model yields competitive results when trained on
generic video content. Extreme compression performance is achieved when
training the model on specialized content.","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']",139,97
Indoor dense depth map at drone hovering,"Autonomous Micro Aerial Vehicles (MAVs) gained tremendous attention in recent
years. Autonomous flight in indoor requires a dense depth map for navigable
space detection which is the fundamental component for autonomous navigation.
In this paper, we address the problem of reconstructing dense depth while a
drone is hovering (small camera motion) in indoor scenes using already
estimated cameras and sparse point cloud obtained from a vSLAM. We start by
segmenting the scene based on sudden depth variation using sparse 3D points and
introduce a patch-based local plane fitting via energy minimization which
combines photometric consistency and co-planarity with neighbouring patches.
The method also combines a plane sweep technique for image segments having
almost no sparse point for initialization. Experiments show, the proposed
method produces better depth for indoor in artificial lighting condition,
low-textured environment compared to earlier literature in small motion.",['cs.CV'],144,108
Automatic Colon Polyp Detection using Region based Deep CNN and Post Learning Approaches,"Automatic detection of colonic polyps is still an unsolved problem due to the
large variation of polyps in terms of shape, texture, size, and color, and the
existence of various polyp-like mimics during colonoscopy. In this study, we
apply a recent region based convolutional neural network (CNN) approach for the
automatic detection of polyps in images and videos obtained from colonoscopy
examinations. We use a deep-CNN model (Inception Resnet) as a transfer learning
scheme in the detection system. To overcome the polyp detection obstacles and
the small number of polyp images, we examine image augmentation strategies for
training deep networks. We further propose two efficient post-learning methods
such as, automatic false positive learning and off-line learning, both of which
can be incorporated with the region based detection system for reliable polyp
detection. Using the large size of colonoscopy databases, experimental results
demonstrate that the suggested detection systems show better performance
compared to other systems in the literature. Furthermore, we show improved
detection performance using the proposed post-learning schemes for colonoscopy
videos.","['cs.CV', 'cs.AI']",176,109
Discrete Wavelet Transform and Gradient Difference based approach for text localization in videos,"The text detection and localization is important for video analysis and
understanding. The scene text in video contains semantic information and thus
can contribute significantly to video retrieval and understanding. However,
most of the approaches detect scene text in still images or single video frame.
Videos differ from images in temporal redundancy. This paper proposes a novel
hybrid method to robustly localize the texts in natural scene images and videos
based on fusion of discrete wavelet transform and gradient difference. A set of
rules and geometric properties have been devised to localize the actual text
regions. Then, morphological operation is performed to generate the text
regions and finally the connected component analysis is employed to localize
the text in a video frame. The experimental results obtained on publicly
available standard ICDAR 2003 and Hua dataset illustrate that the proposed
method can accurately detect and localize texts of various sizes, fonts and
colors. The experimentation on huge collection of video databases reveal the
suitability of the proposed method to video databases.",['cs.CV'],170,101
ACM-Net: Action Context Modeling Network for Weakly-Supervised Temporal Action Localization,"Weakly-supervised temporal action localization aims to localize action
instances temporal boundary and identify the corresponding action category with
only video-level labels. Traditional methods mainly focus on foreground and
background frames separation with only a single attention branch and class
activation sequence. However, we argue that apart from the distinctive
foreground and background frames there are plenty of semantically ambiguous
action context frames. It does not make sense to group those context frames to
the same background class since they are semantically related to a specific
action category. Consequently, it is challenging to suppress action context
frames with only a single class activation sequence. To address this issue, in
this paper, we propose an action-context modeling network termed ACM-Net, which
integrates a three-branch attention module to measure the likelihood of each
temporal point being action instance, context, or non-action background,
simultaneously. Then based on the obtained three-branch attention values, we
construct three-branch class activation sequences to represent the action
instances, contexts, and non-action backgrounds, individually. To evaluate the
effectiveness of our ACM-Net, we conduct extensive experiments on two benchmark
datasets, THUMOS-14 and ActivityNet-1.3. The experiments show that our method
can outperform current state-of-the-art methods, and even achieve comparable
performance with fully-supervised methods. Code can be found at
https://github.com/ispc-lab/ACM-Net","['cs.CV', 'cs.MM']",230,139
eGAN: Unsupervised approach to class imbalance using transfer learning,"Class imbalance is an inherent problem in many machine learning
classification tasks. This often leads to trained models that are unusable for
any practical purpose. In this study we explore an unsupervised approach to
address these imbalances by leveraging transfer learning from pre-trained image
classification models to encoder-based Generative Adversarial Network (eGAN).
To the best of our knowledge, this is the first work to tackle this problem
using GAN without needing to augment with synthesized fake images.
  In the proposed approach we use the discriminator network to output a
negative or positive score. We classify as minority, test samples with negative
scores and as majority those with positive scores. Our approach eliminates
epistemic uncertainty in model predictions, as the P(minority) + P(majority)
need not sum up to 1. The impact of transfer learning and combinations of
different pre-trained image classification models at the generator and
discriminator is also explored. Best result of 0.69 F1-score was obtained on
CIFAR-10 classification task with imbalance ratio of 1:2500.
  Our approach also provides a mechanism of thresholding the specificity or
sensitivity of our machine learning system. Keywords: Class imbalance, Transfer
Learning, GAN, nash equilibrium","['cs.LG', 'cs.AI', 'cs.CV']",198,129
Point Cloud Completion by Skip-attention Network with Hierarchical Folding,"Point cloud completion aims to infer the complete geometries for missing
regions of 3D objects from incomplete ones. Previous methods usually predict
the complete point cloud based on the global shape representation extracted
from the incomplete input. However, the global representation often suffers
from the information loss of structure details on local regions of incomplete
point cloud. To address this problem, we propose Skip-Attention Network
(SA-Net) for 3D point cloud completion. Our main contributions lie in the
following two-folds. First, we propose a skip-attention mechanism to
effectively exploit the local structure details of incomplete point clouds
during the inference of missing parts. The skip-attention mechanism selectively
conveys geometric information from the local regions of incomplete point clouds
for the generation of complete ones at different resolutions, where the
skip-attention reveals the completion process in an interpretable way. Second,
in order to fully utilize the selected geometric information encoded by
skip-attention mechanism at different resolutions, we propose a novel
structure-preserving decoder with hierarchical folding for complete shape
generation. The hierarchical folding preserves the structure of complete point
cloud generated in upper layer by progressively detailing the local regions,
using the skip-attentioned geometry at the same resolution. We conduct
comprehensive experiments on ShapeNet and KITTI datasets, which demonstrate
that the proposed SA-Net outperforms the state-of-the-art point cloud
completion methods.",['cs.CV'],230,121
OpenMatch: Open-set Consistency Regularization for Semi-supervised Learning with Outliers,"Semi-supervised learning (SSL) is an effective means to leverage unlabeled
data to improve a model's performance. Typical SSL methods like FixMatch assume
that labeled and unlabeled data share the same label space. However, in
practice, unlabeled data can contain categories unseen in the labeled set,
i.e., outliers, which can significantly harm the performance of SSL algorithms.
To address this problem, we propose a novel Open-set Semi-Supervised Learning
(OSSL) approach called OpenMatch. Learning representations of inliers while
rejecting outliers is essential for the success of OSSL. To this end, OpenMatch
unifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers.
The OVA-classifier outputs the confidence score of a sample being an inlier,
providing a threshold to detect outliers. Another key contribution is an
open-set soft-consistency regularization loss, which enhances the smoothness of
the OVA-classifier with respect to input transformations and greatly improves
outlier detection. OpenMatch achieves state-of-the-art performance on three
datasets, and even outperforms a fully supervised model in detecting outliers
unseen in unlabeled data on CIFAR10.",['cs.CV'],180,116
Multiview Hessian Regularization for Image Annotation,"The rapid development of computer hardware and Internet technology makes
large scale data dependent models computationally tractable, and opens a bright
avenue for annotating images through innovative machine learning algorithms.
Semi-supervised learning (SSL) has consequently received intensive attention in
recent years and has been successfully deployed in image annotation. One
representative work in SSL is Laplacian regularization (LR), which smoothes the
conditional distribution for classification along the manifold encoded in the
graph Laplacian, however, it has been observed that LR biases the
classification function towards a constant function which possibly results in
poor generalization. In addition, LR is developed to handle uniformly
distributed data (or single view data), although instances or objects, such as
images and videos, are usually represented by multiview features, such as
color, shape and texture. In this paper, we present multiview Hessian
regularization (mHR) to address the above two problems in LR-based image
annotation. In particular, mHR optimally combines multiple Hessian
regularizations, each of which is obtained from a particular view of instances,
and steers the classification function which varies linearly along the data
manifold. We apply mHR to kernel least squares and support vector machines as
two examples for image annotation. Extensive experiments on the PASCAL VOC'07
dataset validate the effectiveness of mHR by comparing it with baseline
algorithms, including LR and HR.","['cs.LG', 'cs.CV', 'stat.ML']",221,144
Learning non-Gaussian graphical models via Hessian scores and triangular transport,"Undirected probabilistic graphical models represent the conditional
dependencies, or Markov properties, of a collection of random variables.
Knowing the sparsity of such a graphical model is valuable for modeling
multivariate distributions and for efficiently performing inference. While the
problem of learning graph structure from data has been studied extensively for
certain parametric families of distributions, most existing methods fail to
consistently recover the graph structure for non-Gaussian data. Here we propose
an algorithm for learning the Markov structure of continuous and non-Gaussian
distributions. To characterize conditional independence, we introduce a score
based on integrated Hessian information from the joint log-density, and we
prove that this score upper bounds the conditional mutual information for a
general class of distributions. To compute the score, our algorithm SING
estimates the density using a deterministic coupling, induced by a triangular
transport map, and iteratively exploits sparse structure in the map to reveal
sparsity in the graph. For certain non-Gaussian datasets, we show that our
algorithm recovers the graph structure even with a biased approximation to the
density. Among other examples, we apply sing to learn the dependencies between
the states of a chaotic dynamical system with local interactions.","['stat.ML', 'cs.LG', 'stat.CO']",198,118
Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective,"Federated Learning (FL) proposed in recent years has received significant
attention from researchers in that it can bring separate data sources together
and build machine learning models in a collaborative but private manner. Yet,
in most applications of FL, such as keyboard prediction, labeling data requires
virtually no additional efforts, which is not generally the case. In reality,
acquiring large-scale labeled datasets can be extremely costly, which motivates
research works that exploit unlabeled data to help build machine learning
models. However, to the best of our knowledge, few existing works aim to
utilize unlabeled data to enhance federated learning, which leaves a
potentially promising research topic. In this paper, we identify the need to
exploit unlabeled data in FL, and survey possible research fields that can
contribute to the goal.","['cs.LG', 'stat.ML']",131,92
DiffGCN: Graph Convolutional Networks via Differential Operators and Algebraic Multigrid Pooling,"Graph Convolutional Networks (GCNs) have shown to be effective in handling
unordered data like point clouds and meshes. In this work we propose novel
approaches for graph convolution, pooling and unpooling, inspired from finite
differences and algebraic multigrid frameworks. We form a parameterized
convolution kernel based on discretized differential operators, leveraging the
graph mass, gradient and Laplacian. This way, the parameterization does not
depend on the graph structure, only on the meaning of the network convolutions
as differential operators. To allow hierarchical representations of the input,
we propose pooling and unpooling operations that are based on algebraic
multigrid methods, which are mainly used to solve partial differential
equations on unstructured grids. To motivate and explain our method, we compare
it to standard convolutional neural networks, and show their similarities and
relations in the case of a regular grid. Our proposed method is demonstrated in
various experiments like classification and part-segmentation, achieving on par
or better than state of the art results. We also analyze the computational cost
of our method compared to other GCNs.","['cs.CV', 'cs.GR', 'cs.LG']",175,122
Enhancing VAEs for Collaborative Filtering: Flexible Priors & Gating Mechanisms,"Neural network based models for collaborative filtering have started to gain
attention recently. One branch of research is based on using deep generative
models to model user preferences where variational autoencoders were shown to
produce state-of-the-art results. However, there are some potentially
problematic characteristics of the current variational autoencoder for CF. The
first is the too simplistic prior that VAEs incorporate for learning the latent
representations of user preference. The other is the model's inability to learn
deeper representations with more than one hidden layer for each network. Our
goal is to incorporate appropriate techniques to mitigate the aforementioned
problems of variational autoencoder CF and further improve the recommendation
performance. Our work is the first to apply flexible priors to collaborative
filtering and show that simple priors (in original VAEs) may be too restrictive
to fully model user preferences and setting a more flexible prior gives
significant gains. We experiment with the VampPrior, originally proposed for
image generation, to examine the effect of flexible priors in CF. We also show
that VampPriors coupled with gating mechanisms outperform SOTA results
including the Variational Autoencoder for Collaborative Filtering by meaningful
margins on 2 popular benchmark datasets (MovieLens & Netflix).","['stat.ML', 'cs.IR', 'cs.LG']",200,128
CapsGAN: Using Dynamic Routing for Generative Adversarial Networks,"In this paper, we propose a novel technique for generating images in the 3D
domain from images with high degree of geometrical transformations. By
coalescing two popular concurrent methods that have seen rapid ascension to the
machine learning zeitgeist in recent years: GANs (Goodfellow et. al.) and
Capsule networks (Sabour, Hinton et. al.) - we present: \textbf{CapsGAN}. We
show that CapsGAN performs better than or equal to traditional CNN based GANs
in generating images with high geometric transformations using rotated MNIST.
In the process, we also show the efficacy of using capsules architecture in the
GANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the
performance control and training stability by experimenting with using
Wasserstein distance (gradient clipping, penalty) and Spectral Normalization.
The experimental findings of this paper should propel the application of
capsules and GANs in the still exciting and nascent domain of 3D image
generation, and plausibly video (frame) generation.","['cs.CV', 'cs.LG']",154,101
Multi-source Transfer Learning with Ensemble for Financial Time Series Forecasting,"Although transfer learning is proven to be effective in computer vision and
natural language processing applications, it is rarely investigated in
forecasting financial time series. Majority of existing works on transfer
learning are based on single-source transfer learning due to the availability
of open-access large-scale datasets. However, in financial domain, the lengths
of individual time series are relatively short and single-source transfer
learning models are less effective. Therefore, in this paper, we investigate
multi-source deep transfer learning for financial time series. We propose two
multi-source transfer learning methods namely Weighted Average Ensemble for
Transfer Learning (WAETL) and Tree-structured Parzen Estimator Ensemble
Selection (TPEES). The effectiveness of our approach is evaluated on financial
time series extracted from stock markets. Experiment results reveal that TPEES
outperforms other baseline methods on majority of multi-source transfer tasks.",['cs.LG'],141,91
Estimating Vector Fields from Noisy Time Series,"While there has been a surge of recent interest in learning differential
equation models from time series, methods in this area typically cannot cope
with highly noisy data. We break this problem into two parts: (i) approximating
the unknown vector field (or right-hand side) of the differential equation, and
(ii) dealing with noise. To deal with (i), we describe a neural network
architecture consisting of tensor products of one-dimensional neural shape
functions. For (ii), we propose an alternating minimization scheme that
switches between vector field training and filtering steps, together with
multiple trajectories of training data. We find that the neural shape function
architecture retains the approximation properties of dense neural networks,
enables effective computation of vector field error, and allows for graphical
interpretability, all for data/systems in any finite dimension $d$. We also
study the combination of either our neural shape function method or existing
differential equation learning methods with alternating minimization and
multiple trajectories. We find that retrofitting any learning method in this
way boosts the method's robustness to noise. While in their raw form the
methods struggle with 1% Gaussian noise, after retrofitting, they learn
accurate vector fields from data with 10% Gaussian noise.","['stat.ML', 'cs.LG', 'cs.SY', 'eess.SY', 'math.DS', 'math.OC']",201,122
FP-Stereo: Hardware-Efficient Stereo Vision for Embedded Applications,"Fast and accurate depth estimation, or stereo matching, is essential in
embedded stereo vision systems, requiring substantial design effort to achieve
an appropriate balance among accuracy, speed and hardware cost. To reduce the
design effort and achieve the right balance, we propose FP-Stereo for building
high-performance stereo matching pipelines on FPGAs automatically. FP-Stereo
consists of an open-source hardware-efficient library, allowing designers to
obtain the desired implementation instantly. Diverse methods are supported in
our library for each stage of the stereo matching pipeline and a series of
techniques are developed to exploit the parallelism and reduce the resource
overhead. To improve the usability, FP-Stereo can generate synthesizable C code
of the FPGA accelerator with our optimized HLS templates automatically. To
guide users for the right design choice meeting specific application
requirements, detailed comparisons are performed on various configurations of
our library to investigate the accuracy/speed/cost trade-off. Experimental
results also show that FP-Stereo outperforms the state-of-the-art FPGA design
from all aspects, including 6.08% lower error, 2x faster speed, 30% less
resource usage and 40% less energy consumption. Compared to GPU designs,
FP-Stereo achieves the same accuracy at a competitive speed while consuming
much less energy.","['cs.CV', 'cs.AR', 'cs.RO']",208,134
Better than the Best: Gradient-based Improper Reinforcement Learning for Network Scheduling,"We consider the problem of scheduling in constrained queueing networks with a
view to minimizing packet delay. Modern communication systems are becoming
increasingly complex, and are required to handle multiple types of traffic with
widely varying characteristics such as arrival rates and service times. This,
coupled with the need for rapid network deployment, render a bottom up approach
of first characterizing the traffic and then devising an appropriate scheduling
protocol infeasible.
  In contrast, we formulate a top down approach to scheduling where, given an
unknown network and a set of scheduling policies, we use a policy gradient
based reinforcement learning algorithm that produces a scheduler that performs
better than the available atomic policies. We derive convergence results and
analyze finite time performance of the algorithm. Simulation results show that
the algorithm performs well even when the arrival rates are nonstationary and
can stabilize the system even when the constituent policies are unstable.",['cs.LG'],152,102
Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data,"The translation equivariance of convolutional layers enables convolutional
neural networks to generalize well on image problems. While translation
equivariance provides a powerful inductive bias for images, we often
additionally desire equivariance to other transformations, such as rotations,
especially for non-image data. We propose a general method to construct a
convolutional layer that is equivariant to transformations from any specified
Lie group with a surjective exponential map. Incorporating equivariance to a
new group requires implementing only the group exponential and logarithm maps,
enabling rapid prototyping. Showcasing the simplicity and generality of our
method, we apply the same model architecture to images, ball-and-stick
molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the
equivariance of our models is especially impactful, leading to exact
conservation of linear and angular momentum.","['stat.ML', 'cs.LG']",130,88
Generating Geological Facies Models with Fidelity to Diversity and Statistics of Training Images using Improved Generative Adversarial Networks,"This paper presents a methodology and workflow that overcome the limitations
of the conventional Generative Adversarial Networks (GANs) for geological
facies modeling. It attempts to improve the training stability and guarantee
the diversity of the generated geology through interpretable latent vectors.
The resulting samples are ensured to have the equal probability (or an unbiased
distribution) as from the training dataset. This is critical when applying GANs
to generate unbiased and representative geological models that can be further
used to facilitate objective uncertainty evaluation and optimal decision-making
in oil field exploration and development.
  We proposed and implemented a new variant of GANs called Info-WGAN for the
geological facies modeling that combines Information Maximizing Generative
Adversarial Network (InfoGAN) with Wasserstein distance and Gradient Penalty
(GP) for learning interpretable latent codes as well as generating stable and
unbiased distribution from the training data. Different from the original GAN
design, InfoGAN can use the training images with full, partial, or no labels to
perform disentanglement of the complex sedimentary types exhibited in the
training dataset to achieve the variety and diversity of the generated samples.
This is accomplished by adding additional categorical variables that provide
disentangled semantic representations besides the mere randomized latent vector
used in the original GANs. By such means, a regularization term is used to
maximize the mutual information between such latent categorical codes and the
generated geological facies in the loss function.
  Furthermore, the resulting unbiased sampling by Info-WGAN makes the data
conditioning much easier than the conventional GANs in geological modeling
because of the variety and diversity as well as the equal probability of the
unconditional sampling by the generator.","['cs.LG', 'eess.IV', 'physics.comp-ph', 'stat.ML']",274,151
Stopping Criterion for the Mean Shift Iterative Algorithm,"Image segmentation is a critical step in computer vision tasks constituting
an essential issue for pattern recognition and visual interpretation. In this
paper, we propose a new stopping criterion for the mean shift iterative
algorithm by using images defined in Zn ring, with the goal of reaching a
better segmentation. We carried out also a study on the weak and strong of
equivalence classes between two images. An analysis on the convergence with
this new stopping criterion is carried out too.","['cs.CV', 'math.RA']",81,60
An efficient supervised dictionary learning method for audio signal recognition,"Machine hearing or listening represents an emerging area. Conventional
approaches rely on the design of handcrafted features specialized to a specific
audio task and that can hardly generalized to other audio fields. For example,
Mel-Frequency Cepstral Coefficients (MFCCs) and its variants were successfully
applied to computational auditory scene recognition while Chroma vectors are
good at music chord recognition. Unfortunately, these predefined features may
be of variable discrimination power while extended to other tasks or even
within the same task due to different nature of clips. Motivated by this need
of a principled framework across domain applications for machine listening, we
propose a generic and data-driven representation learning approach. For this
sake, a novel and efficient supervised dictionary learning method is presented.
The method learns dissimilar dictionaries, one per each class, in order to
extract heterogeneous information for classification. In other words, we are
seeking to minimize the intra-class homogeneity and maximize class
separability. This is made possible by promoting pairwise orthogonality between
class specific dictionaries and controlling the sparsity structure of the audio
clip's decomposition over these dictionaries. The resulting optimization
problem is non-convex and solved using a proximal gradient descent method.
Experiments are performed on both computational auditory scene (East Anglia and
Rouen) and synthetic music chord recognition datasets. Obtained results show
that our method is capable to reach state-of-the-art hand-crafted features for
both applications.",['cs.CV'],235,162
Local Nonstationarity for Efficient Bayesian Optimization,"Bayesian optimization has shown to be a fundamental global optimization
algorithm in many applications: ranging from automatic machine learning,
robotics, reinforcement learning, experimental design, simulations, etc. The
most popular and effective Bayesian optimization relies on a surrogate model in
the form of a Gaussian process due to its flexibility to represent a prior over
function. However, many algorithms and setups relies on the stationarity
assumption of the Gaussian process. In this paper, we present a novel
nonstationary strategy for Bayesian optimization that is able to outperform the
state of the art in Bayesian optimization both in stationary and nonstationary
problems.","['cs.LG', 'stat.ML']",100,68
What can computational models learn from human selective attention? A review from an audiovisual crossmodal perspective,"Selective attention plays an essential role in information acquisition and
utilization from the environment. In the past 50 years, research on selective
attention has been a central topic in cognitive science. Compared with unimodal
studies, crossmodal studies are more complex but necessary to solve real-world
challenges in both human experiments and computational modeling. Although an
increasing number of findings on crossmodal selective attention have shed light
on humans' behavioral patterns and neural underpinnings, a much better
understanding is still necessary to yield the same benefit for computational
intelligent agents. This article reviews studies of selective attention in
unimodal visual and auditory and crossmodal audiovisual setups from the
multidisciplinary perspectives of psychology and cognitive neuroscience, and
evaluates different ways to simulate analogous mechanisms in computational
models and robotics. We discuss the gaps between these fields in this
interdisciplinary review and provide insights about how to use psychological
findings and theories in artificial intelligence from different perspectives.","['cs.CV', 'cs.LG']",156,109
C2CL: Contact to Contactless Fingerprint Matching,"Matching contactless fingerprints or finger photos to contact-based
fingerprint impressions has received increased attention in the wake of
COVID-19 due to the superior hygiene of the contactless acquisition and the
widespread availability of low cost mobile phones capable of capturing photos
of fingerprints with sufficient resolution for verification purposes. This
paper presents an end-to-end automated system, called C2CL, comprised of a
mobile finger photo capture app, preprocessing, and matching algorithms to
handle the challenges inhibiting previous cross-matching methods; namely i) low
ridge-valley contrast of contactless fingerprints, ii) varying roll, pitch,
yaw, and distance of the finger to the camera, iii) non-linear distortion of
contact-based fingerprints, and vi) different image qualities of smartphone
cameras. Our preprocessing algorithm segments, enhances, scales, and unwarps
contactless fingerprints, while our matching algorithm extracts both minutiae
and texture representations. A sequestered dataset of 9,888 contactless 2D
fingerprints and corresponding contact-based fingerprints from 206 subjects (2
thumbs and 2 index fingers for each subject) acquired using our mobile capture
app is used to evaluate the cross-database performance of our proposed
algorithm. Furthermore, additional experimental results on 3 publicly available
datasets demonstrate, for the first time, contact to contactless fingerprint
matching accuracy that is comparable to existing contact to contact fingerprint
matching systems (TAR in the range of 96.67% to 98.15% at FAR=0.01%).","['cs.CV', 'cs.LG', 'eess.IV']",230,149
Video Contents Understanding using Deep Neural Networks,"We propose a novel application of Transfer Learning to classify video-frame
sequences over multiple classes. This is a pre-weighted model that does not
require to train a fresh CNN. This representation is achieved with the advent
of ""deep neural network"" (DNN), which is being studied these days by many
researchers. We utilize the classical approaches for video classification task
using object detection techniques for comparison, such as ""Google Video
Intelligence API"" and this study will run experiments as to how those
architectures would perform in foggy or rainy weather conditions. Experimental
evaluation on video collections shows that the new proposed classifier achieves
superior performance over existing solutions.",['cs.CV'],109,92
Generative Language Modeling for Automated Theorem Proving,"We explore the application of transformer-based language models to automated
theorem proving. This work is motivated by the possibility that a major
limitation of automated theorem provers compared to humans -- the generation of
original mathematical terms -- might be addressable via generation from
language models. We present an automated prover and proof assistant, GPT-f, for
the Metamath formalization language, and analyze its performance. GPT-f found
new short proofs that were accepted into the main Metamath library, which is to
our knowledge, the first time a deep-learning based system has contributed
proofs that were adopted by a formal mathematics community.","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",102,72
Enhanced Regularizers for Attributional Robustness,"Deep neural networks are the default choice of learning models for computer
vision tasks. Extensive work has been carried out in recent years on explaining
deep models for vision tasks such as classification. However, recent work has
shown that it is possible for these models to produce substantially different
attribution maps even when two very similar images are given to the network,
raising serious questions about trustworthiness. To address this issue, we
propose a robust attribution training strategy to improve attributional
robustness of deep neural networks. Our method carefully analyzes the
requirements for attributional robustness and introduces two new regularizers
that preserve a model's attribution map during attacks. Our method surpasses
state-of-the-art attributional robustness methods by a margin of approximately
3% to 9% in terms of attribution robustness measures on several datasets
including MNIST, FMNIST, Flower and GTSRB.","['cs.CV', 'cs.AI']",142,101
POLA: Online Time Series Prediction by Adaptive Learning Rates,"Online prediction for streaming time series data has practical use for many
real-world applications where downstream decisions depend on accurate forecasts
for the future. Deployment in dynamic environments requires models to adapt
quickly to changing data distributions without overfitting. We propose POLA
(Predicting Online by Learning rate Adaptation) to automatically regulate the
learning rate of recurrent neural network models to adapt to changing time
series patterns across time. POLA meta-learns the learning rate of the
stochastic gradient descent (SGD) algorithm by assimilating the prequential or
interleaved-test-then-train evaluation scheme for online prediction. We
evaluate POLA on two real-world datasets across three commonly-used recurrent
neural network models. POLA demonstrates overall comparable or better
predictive performance over other online prediction methods.","['cs.LG', 'stat.ML']",125,85
Survey of XAI in digital pathology,"Artificial intelligence (AI) has shown great promise for diagnostic imaging
assessments. However, the application of AI to support medical diagnostics in
clinical routine comes with many challenges. The algorithms should have high
prediction accuracy but also be transparent, understandable and reliable. Thus,
explainable artificial intelligence (XAI) is highly relevant for this domain.
We present a survey on XAI within digital pathology, a medical imaging
sub-discipline with particular characteristics and needs. The review includes
several contributions. Firstly, we give a thorough overview of current XAI
techniques of potential relevance for deep learning methods in pathology
imaging, and categorise them from three different aspects. In doing so, we
incorporate uncertainty estimation methods as an integral part of the XAI
landscape. We also connect the technical methods to the specific prerequisites
in digital pathology and present findings to guide future research efforts. The
survey is intended for both technical researchers and medical professionals,
one of the objectives being to establish a common ground for cross-disciplinary
discussions.","['cs.CV', 'cs.LG', 'eess.IV']",165,117
Survey: Transformer based Video-Language Pre-training,"Inspired by the success of transformer-based pre-training methods on natural
language tasks and further computer vision tasks, researchers have begun to
apply transformer to video processing. This survey aims to give a comprehensive
overview on transformer-based pre-training methods for Video-Language learning.
We first briefly introduce the transformer tructure as the background
knowledge, including attention mechanism, position encoding etc. We then
describe the typical paradigm of pre-training & fine-tuning on Video-Language
processing in terms of proxy tasks, downstream tasks and commonly used video
datasets. Next, we categorize transformer models into Single-Stream and
Multi-Stream structures, highlight their innovations and compare their
performances. Finally, we analyze and discuss the current challenges and
possible future research directions for Video-Language pre-training.",['cs.CV'],127,86
Deep Learning in the Automotive Industry: Recent Advances and Application Examples,"One of the most exciting technology breakthroughs in the last few years has
been the rise of deep learning. State-of-the-art deep learning models are being
widely deployed in academia and industry, across a variety of areas, from image
analysis to natural language processing. These models have grown from fledgling
research subjects to mature techniques in real-world use. The increasing scale
of data, computational power and the associated algorithmic innovations are the
main drivers for the progress we see in this field. These developments also
have a huge potential for the automotive industry and therefore the interest in
deep learning-based technology is growing. A lot of the product innovations,
such as self-driving cars, parking and lane-change assist or safety functions,
such as autonomous emergency braking, are powered by deep learning algorithms.
Deep learning is poised to offer gains in performance and functionality for
most ADAS (Advanced Driver Assistance System) solutions. Virtual sensing for
vehicle dynamics application, vehicle inspection/heath monitoring, automated
driving and data-driven product development are key areas that are expected to
get the most attention. This article provides an overview of the recent
advances and some associated challenges in deep learning techniques in the
context of automotive applications.","['cs.LG', 'cs.RO', 'eess.SP', 'stat.ML']",207,135
Wav2Letter: an End-to-End ConvNet-based Speech Recognition System,"This paper presents a simple end-to-end model for speech recognition,
combining a convolutional network based acoustic model and a graph decoding. It
is trained to output letters, with transcribed speech, without the need for
force alignment of phonemes. We introduce an automatic segmentation criterion
for training from sequence annotation without alignment that is on par with CTC
while being simpler. We show competitive results in word error rate on the
Librispeech corpus with MFCC features, and promising results from raw waveform.","['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']",83,64
CAD-Net: A Context-Aware Detection Network for Objects in Remote Sensing Imagery,"Accurate and robust detection of multi-class objects in optical remote
sensing images is essential to many real-world applications such as urban
planning, traffic control, searching and rescuing, etc. However,
state-of-the-art object detection techniques designed for images captured using
ground-level sensors usually experience a sharp performance drop when directly
applied to remote sensing images, largely due to the object appearance
differences in remote sensing images in term of sparse texture, low contrast,
arbitrary orientations, large scale variations, etc. This paper presents a
novel object detection network (CAD-Net) that exploits attention-modulated
features as well as global and local contexts to address the new challenges in
detecting objects from remote sensing images. The proposed CAD-Net learns
global and local contexts of objects by capturing their correlations with the
global scene (at scene-level) and the local neighboring objects or features (at
object-level), respectively. In addition, it designs a spatial-and-scale-aware
attention module that guides the network to focus on more informative regions
and features as well as more appropriate feature scales. Experiments over two
publicly available object detection datasets for remote sensing images
demonstrate that the proposed CAD-Net achieves superior detection performance.
The implementation codes will be made publicly available for facilitating
future researches.",['cs.CV'],214,132
Perceptual Evaluation of Adversarial Attacks for CNN-based Image Classification,"Deep neural networks (DNNs) have recently achieved state-of-the-art
performance and provide significant progress in many machine learning tasks,
such as image classification, speech processing, natural language processing,
etc. However, recent studies have shown that DNNs are vulnerable to adversarial
attacks. For instance, in the image classification domain, adding small
imperceptible perturbations to the input image is sufficient to fool the DNN
and to cause misclassification. The perturbed image, called \textit{adversarial
example}, should be visually as close as possible to the original image.
However, all the works proposed in the literature for generating adversarial
examples have used the $L_{p}$ norms ($L_{0}$, $L_{2}$ and $L_{\infty}$) as
distance metrics to quantify the similarity between the original image and the
adversarial example. Nonetheless, the $L_{p}$ norms do not correlate with human
judgment, making them not suitable to reliably assess the perceptual
similarity/fidelity of adversarial examples. In this paper, we present a
database for visual fidelity assessment of adversarial examples. We describe
the creation of the database and evaluate the performance of fifteen
state-of-the-art full-reference (FR) image fidelity assessment metrics that
could substitute $L_{p}$ norms. The database as well as subjective scores are
publicly available to help designing new metrics for adversarial examples and
to facilitate future research works.","['cs.LG', 'cs.CR', 'cs.CV', 'eess.IV', 'stat.ML']",219,128
Distribution-aware Margin Calibration for Medical Image Segmentation,"The Jaccard index, also known as Intersection-over-Union (IoU score), is one
of the most critical evaluation metrics in medical image segmentation. However,
directly optimizing the mean IoU (mIoU) score over multiple objective classes
is an open problem. Although some algorithms have been proposed to optimize its
surrogates, there is no guarantee provided for their generalization ability. In
this paper, we present a novel data-distribution-aware margin calibration
method for a better generalization of the mIoU over the whole
data-distribution, underpinned by a rigid lower bound. This scheme ensures a
better segmentation performance in terms of IoU scores in practice. We evaluate
the effectiveness of the proposed margin calibration method on two medical
image segmentation datasets, showing substantial improvements of IoU scores
over other learning schemes using deep segmentation models.",['cs.CV'],133,94
Occlusion-aware Visual Tracker using Spatial Structural Information and Dominant Features,"To overcome the problem of occlusion in visual tracking, this paper proposes
an occlusion-aware tracking algorithm. The proposed algorithm divides the
object into discrete image patches according to the pixel distribution of the
object by means of clustering. To avoid the drifting of the tracker to false
targets, the proposed algorithm extracts the dominant features, such as color
histogram or histogram of oriented gradient orientation, from these image
patches, and uses them as cues for tracking. To enhance the robustness of the
tracker, the proposed algorithm employs an implicit spatial structure between
these patches as another cue for tracking; Afterwards, the proposed algorithm
incorporates these components into the particle filter framework, which results
in a robust and precise tracker. Experimental results on color image sequences
with different resolutions show that the proposed tracker outperforms the
comparison algorithms on handling occlusion in visual tracking.","['cs.CV', 'I.2.10']",144,85
Motion-Attentive Transition for Zero-Shot Video Object Segmentation,"In this paper, we present a novel Motion-Attentive Transition Network
(MATNet) for zero-shot video object segmentation, which provides a new way of
leveraging motion information to reinforce spatio-temporal object
representation. An asymmetric attention block, called Motion-Attentive
Transition (MAT), is designed within a two-stream encoder, which transforms
appearance features into motion-attentive representations at each convolutional
stage. In this way, the encoder becomes deeply interleaved, allowing for
closely hierarchical interactions between object motion and appearance. This is
superior to the typical two-stream architecture, which treats motion and
appearance separately in each stream and often suffers from overfitting to
appearance information. Additionally, a bridge network is proposed to obtain a
compact, discriminative and scale-sensitive representation for multi-level
encoder features, which is further fed into a decoder to achieve segmentation
results. Extensive experiments on three challenging public benchmarks (i.e.
DAVIS-16, FBMS and Youtube-Objects) show that our model achieves compelling
performance against the state-of-the-arts.","['cs.CV', 'cs.LG', 'eess.IV']",164,114
Exploiting Hierarchy for Learning and Transfer in KL-regularized RL,"As reinforcement learning agents are tasked with solving more challenging and
diverse tasks, the ability to incorporate prior knowledge into the learning
system and to exploit reusable structure in solution space is likely to become
increasingly important. The KL-regularized expected reward objective
constitutes one possible tool to this end. It introduces an additional
component, a default or prior behavior, which can be learned alongside the
policy and as such partially transforms the reinforcement learning problem into
one of behavior modelling. In this work we consider the implications of this
framework in cases where both the policy and default behavior are augmented
with latent variables. We discuss how the resulting hierarchical structures can
be used to implement different inductive biases and how their modularity can
benefit transfer. Empirically we find that they can lead to faster learning and
transfer on a range of continuous control tasks.","['cs.LG', 'stat.ML']",146,103
A Ray-based Approach for Boundary Estimation of Fiber Bundles Derived from Diffusion Tensor Imaging,"Diffusion Tensor Imaging (DTI) is a non-invasive imaging technique that
allows estimation of the location of white matter tracts in-vivo, based on the
measurement of water diffusion properties. For each voxel, a second-order
tensor can be calculated by using diffusion-weighted sequences (DWI) that are
sensitive to the random motion of water molecules. Given at least 6
diffusion-weighted images with different gradients and one unweighted image,
the coefficients of the symmetric diffusion tensor matrix can be calculated.
Deriving the eigensystem of the tensor, the eigenvectors and eigenvalues can be
calculated to describe the three main directions of diffusion and its
magnitude. Using DTI data, fiber bundles can be determined, to gain information
about eloquent brain structures. Especially in neurosurgery, information about
location and dimension of eloquent structures like the corticospinal tract or
the visual pathways is of major interest. Therefore, the fiber bundle boundary
has to be determined. In this paper, a novel ray-based approach for boundary
estimation of tubular structures is presented.",['cs.CV'],168,107
PERMDNN: Efficient Compressed DNN Architecture with Permuted Diagonal Matrices,"Deep neural network (DNN) has emerged as the most important and popular
artificial intelligent (AI) technique. The growth of model size poses a key
energy efficiency challenge for the underlying computing platform. Thus, model
compression becomes a crucial problem. However, the current approaches are
limited by various drawbacks. Specifically, network sparsification approach
suffers from irregularity, heuristic nature and large indexing overhead. On the
other hand, the recent structured matrix-based approach (i.e., CirCNN) is
limited by the relatively complex arithmetic computation (i.e., FFT), less
flexible compression ratio, and its inability to fully utilize input sparsity.
To address these drawbacks, this paper proposes PermDNN, a novel approach to
generate and execute hardware-friendly structured sparse DNN models using
permuted diagonal matrices. Compared with unstructured sparsification approach,
PermDNN eliminates the drawbacks of indexing overhead, non-heuristic
compression effects and time-consuming retraining. Compared with circulant
structure-imposing approach, PermDNN enjoys the benefits of higher reduction in
computational complexity, flexible compression ratio, simple arithmetic
computation and full utilization of input sparsity. We propose PermDNN
architecture, a multi-processing element (PE) fully-connected (FC)
layer-targeted computing engine. The entire architecture is highly scalable and
flexible, and hence it can support the needs of different applications with
different model configurations. We implement a 32-PE design using CMOS 28nm
technology. Compared with EIE, PermDNN achieves 3.3x~4.8x higher throughout,
5.9x~8.5x better area efficiency and 2.8x~4.0x better energy efficiency on
different workloads. Compared with CirCNN, PermDNN achieves 11.51x higher
throughput and 3.89x better energy efficiency.","['cs.CV', 'cs.AR']",262,171
Toward Transformer-Based Object Detection,"Transformers have become the dominant model in natural language processing,
owing to their ability to pretrain on massive amounts of data, then transfer to
smaller, more specific tasks via fine-tuning. The Vision Transformer was the
first major attempt to apply a pure transformer model directly to images as
input, demonstrating that as compared to convolutional networks,
transformer-based architectures can achieve competitive results on benchmark
classification tasks. However, the computational complexity of the attention
operator means that we are limited to low-resolution inputs. For more complex
tasks such as detection or segmentation, maintaining a high input resolution is
crucial to ensure that models can properly identify and reflect fine details in
their output. This naturally raises the question of whether or not
transformer-based architectures such as the Vision Transformer are capable of
performing tasks other than classification. In this paper, we determine that
Vision Transformers can be used as a backbone by a common detection task head
to produce competitive COCO results. The model that we propose, ViT-FRCNN,
demonstrates several known properties associated with transformers, including
large pretraining capacity and fast fine-tuning performance. We also
investigate improvements over a standard detection backbone, including superior
performance on out-of-domain images, better performance on large objects, and a
lessened reliance on non-maximum suppression. We view ViT-FRCNN as an important
stepping stone toward a pure-transformer solution of complex vision tasks such
as object detection.","['cs.CV', 'cs.AI', 'cs.LG']",240,151
GateNet: Gating-Enhanced Deep Network for Click-Through Rate Prediction,"Advertising and feed ranking are essential to many Internet companies such as
Facebook. Among many real-world advertising and feed ranking systems, click
through rate (CTR) prediction plays a central role. In recent years, many
neural network based CTR models have been proposed and achieved success such as
Factorization-Machine Supported Neural Networks, DeepFM and xDeepFM. Many of
them contain two commonly used components: embedding layer and MLP hidden
layers. On the other side, gating mechanism is also widely applied in many
research fields such as computer vision(CV) and natural language
processing(NLP). Some research has proved that gating mechanism improves the
trainability of non-convex deep neural networks. Inspired by these
observations, we propose a novel model named GateNet which introduces either
the feature embedding gate or the hidden gate to the embedding layer or hidden
layers of DNN CTR models, respectively. The feature embedding gate provides a
learnable feature gating module to select salient latent information from the
feature-level. The hidden gate helps the model to implicitly capture the
high-order interaction more effectively. Extensive experiments conducted on
three real-world datasets demonstrate its effectiveness to boost the
performance of various state-of-the-art models such as FM, DeepFM and xDeepFM
on all datasets.",['cs.LG'],209,141
Medical Concept Representation Learning from Electronic Health Records and its Application on Heart Failure Prediction,"Objective: To transform heterogeneous clinical data from electronic health
records into clinically meaningful constructed features using data driven
method that rely, in part, on temporal relations among data. Materials and
Methods: The clinically meaningful representations of medical concepts and
patients are the key for health analytic applications. Most of existing
approaches directly construct features mapped to raw data (e.g., ICD or CPT
codes), or utilize some ontology mapping such as SNOMED codes. However, none of
the existing approaches leverage EHR data directly for learning such concept
representation. We propose a new way to represent heterogeneous medical
concepts (e.g., diagnoses, medications and procedures) based on co-occurrence
patterns in longitudinal electronic health records. The intuition behind the
method is to map medical concepts that are co-occuring closely in time to
similar concept vectors so that their distance will be small. We also derive a
simple method to construct patient vectors from the related medical concept
vectors. Results: For qualitative evaluation, we study similar medical concepts
across diagnosis, medication and procedure. In quantitative evaluation, our
proposed representation significantly improves the predictive modeling
performance for onset of heart failure (HF), where classification methods (e.g.
logistic regression, neural network, support vector machine and K-nearest
neighbors) achieve up to 23% improvement in area under the ROC curve (AUC)
using this proposed representation. Conclusion: We proposed an effective method
for patient and medical concept representation learning. The resulting
representation can map relevant concepts together and also improves predictive
modeling performance.","['cs.LG', 'cs.NE']",249,156
Performing Deep Recurrent Double Q-Learning for Atari Games,"Currently, many applications in Machine Learning are based on define new
models to extract more information about data, In this case Deep Reinforcement
Learning with the most common application in video games like Atari, Mario, and
others causes an impact in how to computers can learning by himself with only
information called rewards obtained from any action. There is a lot of
algorithms modeled and implemented based on Deep Recurrent Q-Learning proposed
by DeepMind used in AlphaZero and Go. In this document, We proposed Deep
Recurrent Double Q-Learning that is an implementation of Deep Reinforcement
Learning using Double Q-Learning algorithms and Recurrent Networks like LSTM
and DRQN.","['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",110,75
HMOR: Hierarchical Multi-Person Ordinal Relations for Monocular Multi-Person 3D Pose Estimation,"Remarkable progress has been made in 3D human pose estimation from a
monocular RGB camera. However, only a few studies explored 3D multi-person
cases. In this paper, we attempt to address the lack of a global perspective of
the top-down approaches by introducing a novel form of supervision -
Hierarchical Multi-person Ordinal Relations (HMOR). The HMOR encodes
interaction information as the ordinal relations of depths and angles
hierarchically, which captures the body-part and joint level semantic and
maintains global consistency at the same time. In our approach, an integrated
top-down model is designed to leverage these ordinal relations in the learning
process. The integrated model estimates human bounding boxes, human depths, and
root-relative 3D poses simultaneously, with a coarse-to-fine architecture to
improve the accuracy of depth estimation. The proposed method significantly
outperforms state-of-the-art methods on publicly available multi-person 3D pose
datasets. In addition to superior performance, our method costs lower
computation complexity and fewer model parameters.",['cs.CV'],167,116
Transforming and Projecting Images into Class-conditional Generative Networks,"We present a method for projecting an input image into the space of a
class-conditional generative neural network. We propose a method that optimizes
for transformation to counteract the model biases in generative neural
networks. Specifically, we demonstrate that one can solve for image
translation, scale, and global color transformation, during the projection
optimization to address the object-center bias and color bias of a Generative
Adversarial Network. This projection process poses a difficult optimization
problem, and purely gradient-based optimizations fail to find good solutions.
We describe a hybrid optimization strategy that finds good projections by
estimating transformations and class parameters. We show the effectiveness of
our method on real images and further demonstrate how the corresponding
projections lead to better editability of these images.",['cs.CV'],127,84
Self-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism,"Physics-Informed Neural Networks (PINNs) have emerged recently as a promising
application of deep neural networks to the numerical solution of nonlinear
partial differential equations (PDEs). However, the original PINN algorithm is
known to suffer from stability and accuracy problems in cases where the
solution has sharp spatio-temporal transitions. These stiff PDEs require an
unreasonably large number of collocation points to be solved accurately. It has
been recognized that adaptive procedures are needed to force the neural network
to fit accurately the stubborn spots in the solution of stiff PDEs. To
accomplish this, previous approaches have used fixed weights hard-coded over
regions of the solution deemed to be important. In this paper, we propose a
fundamentally new method to train PINNs adaptively, where the adaptation
weights are fully trainable, so the neural network learns by itself which
regions of the solution are difficult and is forced to focus on them, which is
reminiscent of soft multiplicative-mask attention mechanism used in computer
vision. The basic idea behind these Self-Adaptive PINNs is to make the weights
increase where the corresponding loss is higher, which is accomplished by
training the network to simultaneously minimize the losses and maximize the
weights, i.e., to find a saddle point in the cost surface. We show that this is
formally equivalent to solving a PDE-constrained optimization problem using a
penalty-based method, though in a way where the monotonically-nondecreasing
penalty coefficients are trainable. Numerical experiments with an Allen-Cahn
stiff PDE, the Self-Adaptive PINN outperformed other state-of-the-art PINN
algorithms in L2 error by a wide margin, while using a smaller number of
training epochs. An Appendix contains additional results with Burger's and
Helmholtz PDEs, which confirmed the trends observed in the Allen-Cahn
experiments.","['cs.LG', 'stat.ML']",299,184
Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++,"Manually labeling datasets with object masks is extremely time consuming. In
this work, we follow the idea of Polygon-RNN to produce polygonal annotations
of objects interactively using humans-in-the-loop. We introduce several
important improvements to the model: 1) we design a new CNN encoder
architecture, 2) show how to effectively train the model with Reinforcement
Learning, and 3) significantly increase the output resolution using a Graph
Neural Network, allowing the model to accurately annotate high-resolution
objects in images. Extensive evaluation on the Cityscapes dataset shows that
our model, which we refer to as Polygon-RNN++, significantly outperforms the
original model in both automatic (10% absolute and 16% relative improvement in
mean IoU) and interactive modes (requiring 50% fewer clicks by annotators). We
further analyze the cross-domain scenario in which our model is trained on one
dataset, and used out of the box on datasets from varying domains. The results
show that Polygon-RNN++ exhibits powerful generalization capabilities,
achieving significant improvements over existing pixel-wise methods. Using
simple online fine-tuning we further achieve a high reduction in annotation
time for new datasets, moving a step closer towards an interactive annotation
tool to be used in practice.",['cs.CV'],201,137
Towards Compact Single Image Super-Resolution via Contrastive Self-distillation,"Convolutional neural networks (CNNs) are highly successful for
super-resolution (SR) but often require sophisticated architectures with heavy
memory cost and computational overhead, significantly restricts their practical
deployments on resource-limited devices. In this paper, we proposed a novel
contrastive self-distillation (CSD) framework to simultaneously compress and
accelerate various off-the-shelf SR models. In particular, a channel-splitting
super-resolution network can first be constructed from a target teacher network
as a compact student network. Then, we propose a novel contrastive loss to
improve the quality of SR images and PSNR/SSIM via explicit knowledge transfer.
Extensive experiments demonstrate that the proposed CSD scheme effectively
compresses and accelerates several standard SR models such as EDSR, RCAN and
CARN. Code is available at https://github.com/Booooooooooo/CSD.",['cs.CV'],129,102
Don't Sweep your Learning Rate under the Rug: A Closer Look at Cross-modal Transfer of Pretrained Transformers,"Self-supervised pre-training of large-scale transformer models on text
corpora followed by finetuning has achieved state-of-the-art on a number of
natural language processing tasks. Recently, Lu et al. (2021, arXiv:2103.05247)
claimed that frozen pretrained transformers (FPTs) match or outperform training
from scratch as well as unfrozen (fine-tuned) pretrained transformers in a set
of transfer tasks to other modalities. In our work, we find that this result
is, in fact, an artifact of not tuning the learning rates. After carefully
redesigning the empirical setup, we find that when tuning learning rates
properly, pretrained transformers do outperform or match training from scratch
in all of our tasks, but only as long as the entire model is finetuned. Thus,
while transfer from pretrained language models to other modalities does indeed
provide gains and hints at exciting possibilities for future work, properly
tuning hyperparameters is important for arriving at robust findings.","['cs.LG', 'cs.AI']",155,104
Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials,"Most state-of-the-art techniques for multi-class image segmentation and
labeling use conditional random fields defined over pixels or image regions.
While region-level models often feature dense pairwise connectivity,
pixel-level models are considerably larger and have only permitted sparse graph
structures. In this paper, we consider fully connected CRF models defined on
the complete set of pixels in an image. The resulting graphs have billions of
edges, making traditional inference algorithms impractical. Our main
contribution is a highly efficient approximate inference algorithm for fully
connected CRF models in which the pairwise edge potentials are defined by a
linear combination of Gaussian kernels. Our experiments demonstrate that dense
connectivity at the pixel level substantially improves segmentation and
labeling accuracy.","['cs.CV', 'cs.AI', 'cs.LG']",122,88
Learning to Control Direct Current Motor for Steering in Real Time via Reinforcement Learning,"Model free techniques have been successful at optimal control of complex
systems at an expense of copious amounts of data and computation. However, it
is often desired to obtain a control policy in a short period of time with
minimal data use and computational burden. To this end, we make use of the NFQ
algorithm for steering position control of a golf cart in both a real hardware
and a simulated environment that was built from real-world interaction. The
controller learns to apply a sequence of voltage signals in the presence of
environmental uncertainties and inherent non-linearities that challenge the the
control task. We were able to increase the rate of successful control under
four minutes in simulation and under 11 minutes in real hardware.","['cs.LG', 'cs.RO']",127,86
A Progressive Sub-Network Searching Framework for Dynamic Inference,"Many techniques have been developed, such as model compression, to make Deep
Neural Networks (DNNs) inference more efficiently. Nevertheless, DNNs still
lack excellent run-time dynamic inference capability to enable users trade-off
accuracy and computation complexity (i.e., latency on target hardware) after
model deployment, based on dynamic requirements and environments. Such research
direction recently draws great attention, where one realization is to train the
target DNN through a multiple-term objective function, which consists of
cross-entropy terms from multiple sub-nets. Our investigation in this work show
that the performance of dynamic inference highly relies on the quality of
sub-net sampling. With objective to construct a dynamic DNN and search multiple
high quality sub-nets with minimal searching cost, we propose a progressive
sub-net searching framework, which is embedded with several effective
techniques, including trainable noise ranking, channel group and fine-tuning
threshold setting, sub-nets re-selection. The proposed framework empowers the
target DNN with better dynamic inference capability, which outperforms prior
works on both CIFAR-10 and ImageNet dataset via comprehensive experiments on
different network structures. Taken ResNet18 as an example, our proposed method
achieves much better dynamic inference accuracy compared with prior popular
Universally-Slimmable-Network by 4.4%-maximally and 2.3%-averagely in ImageNet
dataset with the same model size.",['cs.CV'],220,153
Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval,"The rapid growth of user-generated videos on the Internet has intensified the
need for text-based video retrieval systems. Traditional methods mainly favor
the concept-based paradigm on retrieval with simple queries, which are usually
ineffective for complex queries that carry far more complex semantics.
Recently, embedding-based paradigm has emerged as a popular approach. It aims
to map the queries and videos into a shared embedding space where
semantically-similar texts and videos are much closer to each other. Despite
its simplicity, it forgoes the exploitation of the syntactic structure of text
queries, making it suboptimal to model the complex queries.
  To facilitate video retrieval with complex queries, we propose a
Tree-augmented Cross-modal Encoding method by jointly learning the linguistic
structure of queries and the temporal representation of videos. Specifically,
given a complex user query, we first recursively compose a latent semantic tree
to structurally describe the text query. We then design a tree-augmented query
encoder to derive structure-aware query representation and a temporal attentive
video encoder to model the temporal characteristics of videos. Finally, both
the query and videos are mapped into a joint embedding space for matching and
ranking. In this approach, we have a better understanding and modeling of the
complex queries, thereby achieving a better video retrieval performance.
Extensive experiments on large scale video retrieval benchmark datasets
demonstrate the effectiveness of our approach.",['cs.CV'],233,131
Temporal graph-based approach for behavioural entity classification,"Graph-based analyses have gained a lot of relevance in the past years due to
their high potential in describing complex systems by detailing the actors
involved, their relations and their behaviours. Nevertheless, in scenarios
where these aspects are evolving over time, it is not easy to extract valuable
information or to characterize correctly all the actors. In this study, a two
phased approach for exploiting the potential of graph structures in the
cybersecurity domain is presented. The main idea is to convert a network
classification problem into a graph-based behavioural one. We extract these
graph structures that can represent the evolution of both normal and attack
entities and apply a temporal dissection approach in order to highlight their
micro-dynamics. Further, three clustering techniques are applied to the normal
entities in order to aggregate similar behaviours, mitigate the imbalance
problem and reduce noisy data. Our approach suggests the implementation of two
promising deep learning paradigms for entity classification based on Graph
Convolutional Networks.","['cs.LG', 'cs.IT', 'math.IT']",165,110
Aligning Partially Overlapping Point Sets: an Inner Approximation Algorithm,"Aligning partially overlapping point sets where there is no prior information
about the value of the transformation is a challenging problem in computer
vision. To achieve this goal, we first reduce the objective of the robust point
matching algorithm to a function of a low dimensional variable. The resulting
function, however, is only concave over a finite region including the feasible
region. To cope with this issue, we employ the inner approximation optimization
algorithm which only operates within the region where the objective function is
concave. Our algorithm does not need regularization on transformation, and thus
can handle the situation where there is no prior information about the values
of the transformations. Our method is also $\epsilon-$globally optimal and thus
is guaranteed to be robust. Moreover, its most computationally expensive
subroutine is a linear assignment problem which can be efficiently solved.
Experimental results demonstrate the better robustness of the proposed method
over state-of-the-art algorithms. Our method is also efficient when the number
of transformation parameters is small.",['cs.CV'],171,102
Cost-effective Object Detection: Active Sample Mining with Switchable Selection Criteria,"Though quite challenging, leveraging large-scale unlabeled or partially
labeled data in learning systems (e.g., model/classifier training) has
attracted increasing attentions due to its fundamental importance. To address
this problem, many active learning (AL) methods have been proposed that employ
up-to-date detectors to retrieve representative minority samples according to
predefined confidence or uncertainty thresholds. However, these AL methods
cause the detectors to ignore the remaining majority samples (i.e., those with
low uncertainty or high prediction confidence). In this work, by developing a
principled active sample mining (ASM) framework, we demonstrate that
cost-effectively mining samples from these unlabeled majority data is key to
training more powerful object detectors while minimizing user effort.
Specifically, our ASM framework involves a switchable sample selection
mechanism for determining whether an unlabeled sample should be manually
annotated via AL or automatically pseudo-labeled via a novel self-learning
process. The proposed process can be compatible with mini-batch based training
(i.e., using a batch of unlabeled or partially labeled data as a one-time
input) for object detection. In addition, a few samples with low-confidence
predictions are selected and annotated via AL. Notably, our method is suitable
for object categories that are not seen in the unlabeled data during the
learning process. Extensive experiments clearly demonstrate that our ASM
framework can achieve performance comparable to that of alternative methods but
with significantly fewer annotations.",['cs.CV'],236,147
Unsupervised learning of disentangled representations in deep restricted kernel machines with orthogonality constraints,"We introduce Constr-DRKM, a deep kernel method for the unsupervised learning
of disentangled data representations. We propose augmenting the original deep
restricted kernel machine formulation for kernel PCA by orthogonality
constraints on the latent variables to promote disentanglement and to make it
possible to carry out optimization without first defining a stabilized
objective. After illustrating an end-to-end training procedure based on a
quadratic penalty optimization algorithm with warm start, we quantitatively
evaluate the proposed method's effectiveness in disentangled feature learning.
We demonstrate on four benchmark datasets that this approach performs similarly
overall to $\beta$-VAE on a number of disentanglement metrics when few training
points are available, while being less sensitive to randomness and
hyperparameter selection than $\beta$-VAE. We also present a deterministic
initialization of Constr-DRKM's training algorithm that significantly improves
the reproducibility of the results. Finally, we empirically evaluate and
discuss the role of the number of layers in the proposed methodology, examining
the influence of each principal component in every layer and showing that
components in lower layers act as local feature detectors capturing the broad
trends of the data distribution, while components in deeper layers use the
representation learned by previous layers and more accurately reproduce
higher-level features.","['cs.LG', 'stat.ML']",209,135
Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment,"Reinforcement learning from large-scale offline datasets provides us with the
ability to learn policies without potentially unsafe or impractical
exploration. Significant progress has been made in the past few years in
dealing with the challenge of correcting for differing behavior between the
data collection and learned policies. However, little attention has been paid
to potentially changing dynamics when transferring a policy to the online
setting, where performance can be up to 90% reduced for existing methods. In
this paper we address this problem with Augmented World Models (AugWM). We
augment a learned dynamics model with simple transformations that seek to
capture potential changes in physical properties of the robot, leading to more
robust policies. We not only train our policy in this new setting, but also
provide it with the sampled augmentation as a context, allowing it to adapt to
changes in the environment. At test time we learn the context in a
self-supervised fashion by approximating the augmentation which corresponds to
the new environment. We rigorously evaluate our approach on over 100 different
changed dynamics settings, and show that this simple approach can significantly
improve the zero-shot generalization of a recent state-of-the-art baseline,
often achieving successful policies where the baseline fails.","['cs.LG', 'cs.AI']",208,138
Fusing Bird View LIDAR Point Cloud and Front View Camera Image for Deep Object Detection,"We propose a new method for fusing a LIDAR point cloud and camera-captured
images in the deep convolutional neural network (CNN). The proposed method
constructs a new layer called non-homogeneous pooling layer to transform
features between bird view map and front view map. The sparse LIDAR point cloud
is used to construct the mapping between the two maps. The pooling layer allows
efficient fusion of the bird view and front view features at any stage of the
network. This is favorable for the 3D-object detection using camera-LIDAR
fusion in autonomous driving scenarios. A corresponding deep CNN is designed
and tested on the KITTI bird view object detection dataset, which produces 3D
bounding boxes from the bird view map. The fusion method shows particular
benefit for detection of pedestrians in the bird view compared to other
fusion-based object detection networks.","['cs.CV', 'cs.LG']",144,80
Point Transformer,"Self-attention networks have revolutionized natural language processing and
are making impressive strides in image analysis tasks such as image
classification and object detection. Inspired by this success, we investigate
the application of self-attention networks to 3D point cloud processing. We
design self-attention layers for point clouds and use these to construct
self-attention networks for tasks such as semantic scene segmentation, object
part segmentation, and object classification. Our Point Transformer design
improves upon prior work across domains and tasks. For example, on the
challenging S3DIS dataset for large-scale semantic scene segmentation, the
Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the
strongest prior model by 3.3 absolute percentage points and crossing the 70%
mIoU threshold for the first time.",['cs.CV'],128,83
Integrating Deep Learning and Augmented Reality to Enhance Situational Awareness in Firefighting Environments,"We present a new four-pronged approach to build firefighter's situational
awareness for the first time in the literature. We construct a series of deep
learning frameworks built on top of one another to enhance the safety,
efficiency, and successful completion of rescue missions conducted by
firefighters in emergency first response settings. First, we used a deep
Convolutional Neural Network (CNN) system to classify and identify objects of
interest from thermal imagery in real-time. Next, we extended this CNN
framework for object detection, tracking, segmentation with a Mask RCNN
framework, and scene description with a multimodal natural language
processing(NLP) framework. Third, we built a deep Q-learning-based agent,
immune to stress-induced disorientation and anxiety, capable of making clear
navigation decisions based on the observed and stored facts in live-fire
environments. Finally, we used a low computational unsupervised learning
technique called tensor decomposition to perform meaningful feature extraction
for anomaly detection in real-time. With these ad-hoc deep learning structures,
we built the artificial intelligence system's backbone for firefighters'
situational awareness. To bring the designed system into usage by firefighters,
we designed a physical structure where the processed results are used as inputs
in the creation of an augmented reality capable of advising firefighters of
their location and key features around them, which are vital to the rescue
operation at hand, as well as a path planning feature that acts as a virtual
guide to assist disoriented first responders in getting back to safety. When
combined, these four approaches present a novel approach to information
understanding, transfer, and synthesis that could dramatically improve
firefighter response and efficacy and reduce life loss.",['cs.CV'],278,175
Evolving Fuzzy Image Segmentation with Self-Configuration,"Current image segmentation techniques usually require that the user tune
several parameters in order to obtain maximum segmentation accuracy, a
computationally inefficient approach, especially when a large number of images
must be processed sequentially in daily practice. The use of evolving fuzzy
systems for designing a method that automatically adjusts parameters to segment
medical images according to the quality expectation of expert users has been
proposed recently (Evolving fuzzy image segmentation EFIS). However, EFIS
suffers from a few limitations when used in practice mainly due to some fixed
parameters. For instance, EFIS depends on auto-detection of the object of
interest for feature calculation, a task that is highly application-dependent.
This shortcoming limits the applicability of EFIS, which was proposed with the
ultimate goal of offering a generic but adjustable segmentation scheme. In this
paper, a new version of EFIS is proposed to overcome these limitations. The new
EFIS, called self-configuring EFIS (SC-EFIS), uses available training data to
self-estimate the parameters that are fixed in EFIS. As well, the proposed
SC-EFIS relies on a feature selection process that does not require
auto-detection of an ROI. The proposed SC-EFIS was evaluated using the same
segmentation algorithms and the same dataset as for EFIS. The results show that
SC-EFIS can provide the same results as EFIS but with a higher level of
automation.",['cs.CV'],229,136
Accurate RGB-D Salient Object Detection via Collaborative Learning,"Benefiting from the spatial cues embedded in depth images, recent progress on
RGB-D saliency detection shows impressive ability on some challenge scenarios.
However, there are still two limitations. One hand is that the pooling and
upsampling operations in FCNs might cause blur object boundaries. On the other
hand, using an additional depth-network to extract depth features might lead to
high computation and storage cost. The reliance on depth inputs during testing
also limits the practical applications of current RGB-D models. In this paper,
we propose a novel collaborative learning framework where edge, depth and
saliency are leveraged in a more efficient way, which solves those problems
tactfully. The explicitly extracted edge information goes together with
saliency to give more emphasis to the salient regions and object boundaries.
Depth and saliency learning is innovatively integrated into the high-level
feature learning process in a mutual-benefit manner. This strategy enables the
network to be free of using extra depth networks and depth inputs to make
inference. To this end, it makes our model more lightweight, faster and more
versatile. Experiment results on seven benchmark datasets show its superior
performance.",['cs.CV'],191,136
Semantic Segmentation by Improved Generative Adversarial Networks,"While most existing segmentation methods usually combined the powerful
feature extraction capabilities of CNNs with Conditional Random Fields (CRFs)
post-processing, the result always limited by the fault of CRFs . Due to the
notoriously slow calculation speeds and poor efficiency of CRFs, in recent
years, CRFs post-processing has been gradually eliminated. In this paper, an
improved Generative Adversarial Networks (GANs) for image semantic segmentation
task (semantic segmentation by GANs, Seg-GAN) is proposed to facilitate further
segmentation research. In addition, we introduce Convolutional CRFs (ConvCRFs)
as an effective improvement solution for the image semantic segmentation task.
Towards the goal of differentiating the segmentation results from the ground
truth distribution and improving the details of the output images, the proposed
discriminator network is specially designed in a full convolutional manner
combined with cascaded ConvCRFs. Besides, the adversarial loss aggressively
encourages the output image to be close to the distribution of the ground
truth. Our method not only learns an end-to-end mapping from input image to
corresponding output image, but also learns a loss function to train this
mapping. The experiments show that our method achieves better performance than
state-of-the-art methods.",['cs.CV'],195,123
Neural Semantic Encoders,"We present a memory augmented neural network for natural language
understanding: Neural Semantic Encoders. NSE is equipped with a novel memory
update rule and has a variable sized encoding memory that evolves over time and
maintains the understanding of input sequences through read}, compose and write
operations. NSE can also access multiple and shared memories. In this paper, we
demonstrated the effectiveness and the flexibility of NSE on five different
natural language tasks: natural language inference, question answering,
sentence classification, document sentiment analysis and machine translation
where NSE achieved state-of-the-art performance when evaluated on publically
available benchmarks. For example, our shared-memory model showed an
encouraging result on neural machine translation, improving an attention-based
baseline by approximately 1.0 BLEU.","['cs.LG', 'cs.CL', 'stat.ML']",124,94
What graph neural networks cannot learn: depth vs width,"This paper studies the expressive power of graph neural networks falling
within the message-passing framework (GNNmp). Two results are presented. First,
GNNmp are shown to be Turing universal under sufficient conditions on their
depth, width, node attributes, and layer expressiveness. Second, it is
discovered that GNNmp can lose a significant portion of their power when their
depth and width is restricted. The proposed impossibility statements stem from
a new technique that enables the repurposing of seminal results from
distributed computing and leads to lower bounds for an array of decision,
optimization, and estimation problems involving graphs. Strikingly, several of
these problems are deemed impossible unless the product of a GNNmp's depth and
width exceeds a polynomial of the graph size; this dependence remains
significant even for tasks that appear simple or when considering
approximation.","['cs.LG', 'stat.ML']",136,96
A Simple Yet Effective Method for Video Temporal Grounding with Cross-Modality Attention,"The task of language-guided video temporal grounding is to localize the
particular video clip corresponding to a query sentence in an untrimmed video.
Though progress has been made continuously in this field, some issues still
need to be resolved. First, most of the existing methods rely on the
combination of multiple complicated modules to solve the task. Second, due to
the semantic gaps between the two different modalities, aligning the
information at different granularities (local and global) between the video and
the language is significant, which is less addressed. Last, previous works do
not consider the inevitable annotation bias due to the ambiguities of action
boundaries. To address these limitations, we propose a simple two-branch
Cross-Modality Attention (CMA) module with intuitive structure design, which
alternatively modulates two modalities for better matching the information both
locally and globally. In addition, we introduce a new task-specific regression
loss function, which improves the temporal grounding accuracy by alleviating
the impact of annotation bias. We conduct extensive experiments to validate our
method, and the results show that just with this simple model, it can
outperform the state of the arts on both Charades-STA and ActivityNet Captions
datasets.",['cs.CV'],198,137
Multi-graph Fusion for Multi-view Spectral Clustering,"A panoply of multi-view clustering algorithms has been developed to deal with
prevalent multi-view data. Among them, spectral clustering-based methods have
drawn much attention and demonstrated promising results recently. Despite
progress, there are still two fundamental questions that stay unanswered to
date. First, how to fuse different views into one graph. More often than not,
the similarities between samples may be manifested differently by different
views. Many existing algorithms either simply take the average of multiple
views or just learn a common graph. These simple approaches fail to consider
the flexible local manifold structures of all views. Hence, the rich
heterogeneous information is not fully exploited. Second, how to learn the
explicit cluster structure. Most existing methods don't pay attention to the
quality of the graphs and perform graph learning and spectral clustering
separately. Those unreliable graphs might lead to suboptimal clustering
results. To fill these gaps, in this paper, we propose a novel multi-view
spectral clustering model which performs graph fusion and spectral clustering
simultaneously. The fusion graph approximates the original graph of each
individual view but maintains an explicit cluster structure. Experiments on
four widely used data sets confirm the superiority of the proposed method.","['cs.LG', 'cs.CV', 'stat.ML']",202,142
Self-supervised Learning for Large-scale Item Recommendations,"Large scale recommender models find most relevant items from huge catalogs,
and they play a critical role in modern search and recommendation systems. To
model the input space with large-vocab categorical features, a typical
recommender model learns a joint embedding space through neural networks for
both queries and items from user feedback data. However, with millions to
billions of items in the corpus, users tend to provide feedback for a very
small set of them, causing a power-law distribution. This makes the feedback
data for long-tail items extremely sparse.
  Inspired by the recent success in self-supervised representation learning
research in both computer vision and natural language understanding, we propose
a multi-task self-supervised learning (SSL) framework for large-scale item
recommendations. The framework is designed to tackle the label sparsity problem
by learning better latent relationship of item features. Specifically, SSL
improves item representation learning as well as serving as additional
regularization to improve generalization. Furthermore, we propose a novel data
augmentation method that utilizes feature correlations within the proposed
framework.
  We evaluate our framework using two real-world datasets with 500M and 1B
training examples respectively. Our results demonstrate the effectiveness of
SSL regularization and show its superior performance over the state-of-the-art
regularization techniques. We also have already launched the proposed
techniques to a web-scale commercial app-to-app recommendation system, with
significant improvements top-tier business metrics demonstrated in A/B
experiments on live traffic. Our online results also verify our hypothesis that
our framework indeed improves model performance even more on slices that lack
supervision.","['cs.LG', 'cs.IR', 'stat.ML']",267,174
A discriminative approach for finding and characterizing positivity violations using decision trees,"The assumption of positivity in causal inference (also known as common
support and co-variate overlap) is necessary to obtain valid causal estimates.
Therefore, confirming it holds in a given dataset is an important first step of
any causal analysis. Most common methods to date are insufficient for
discovering non-positivity, as they do not scale for modern high-dimensional
covariate spaces, or they cannot pinpoint the subpopulation violating
positivity. To overcome these issues, we suggest to harness decision trees for
detecting violations. By dividing the covariate space into mutually exclusive
regions, each with maximized homogeneity of treatment groups, decision trees
can be used to automatically detect subspaces violating positivity. By
augmenting the method with an additional random forest model, we can quantify
the robustness of the violation within each subspace. This solution is scalable
and provides an interpretable characterization of the subspaces in which
violations occur. We provide a visualization of the stratification rules that
define each subpopulation, combined with the severity of positivity violation
within it. We also provide an interactive version of the visualization that
allows a deeper dive into the properties of each subspace.","['stat.ML', 'cs.LG']",188,122
Promises and Pitfalls of Black-Box Concept Learning Models,"Machine learning models that incorporate concept learning as an intermediate
step in their decision making process can match the performance of black-box
predictive models while retaining the ability to explain outcomes in human
understandable terms. However, we demonstrate that the concept representations
learned by these models encode information beyond the pre-defined concepts, and
that natural mitigation strategies do not fully work, rendering the
interpretation of the downstream prediction misleading. We describe the
mechanism underlying the information leakage and suggest recourse for
mitigating its effects.","['cs.LG', 'cs.AI']",86,69
Ensemble deep learning: A review,"Ensemble learning combines several individual models to obtain better
generalization performance. Currently, deep learning models with multilayer
processing architecture is showing better performance as compared to the
shallow or traditional classification models. Deep ensemble learning models
combine the advantages of both the deep learning models as well as the ensemble
learning such that the final model has better generalization performance. This
paper reviews the state-of-art deep ensemble models and hence serves as an
extensive summary for the researchers. The ensemble models are broadly
categorised into ensemble models like bagging, boosting and stacking, negative
correlation based deep ensemble models, explicit/implicit ensembles,
homogeneous /heterogeneous ensemble, decision fusion strategies, unsupervised,
semi-supervised, reinforcement learning and online/incremental, multilabel
based deep ensemble models. Application of deep ensemble models in different
domains is also briefly discussed. Finally, we conclude this paper with some
future recommendations and research directions.","['cs.LG', 'cs.AI', 'cs.CV']",146,94
RetrievalFuse: Neural 3D Scene Reconstruction with a Database,"3D reconstruction of large scenes is a challenging problem due to the
high-complexity nature of the solution space, in particular for generative
neural networks. In contrast to traditional generative learned models which
encode the full generative process into a neural network and can struggle with
maintaining local details at the scene level, we introduce a new method that
directly leverages scene geometry from the training database. First, we learn
to synthesize an initial estimate for a 3D scene, constructed by retrieving a
top-k set of volumetric chunks from the scene database. These candidates are
then refined to a final scene generation with an attention-based refinement
that can effectively select the most consistent set of geometry from the
candidates and combine them together to create an output scene, facilitating
transfer of coherent structures and local detail from train scene geometry. We
demonstrate our neural scene reconstruction with a database for the tasks of 3D
super resolution and surface reconstruction from sparse point clouds, showing
that our approach enables generation of more coherent, accurate 3D scenes,
improving on average by over 8% in IoU over state-of-the-art scene
reconstruction.",['cs.CV'],192,117
Control-Tutored Reinforcement Learning: an application to the Herding Problem,"In this extended abstract we introduce a novel control-tutored Q-learning
approach (CTQL) as part of the ongoing effort in developing model-based and
safe RL for continuous state spaces. We validate our approach by applying it to
a challenging multi-agent herding control problem.","['cs.LG', 'cs.AI']",46,43
A Systematic Evaluation of Recent Deep Learning Architectures for Fine-Grained Vehicle Classification,"Fine-grained vehicle classification is the task of classifying make, model,
and year of a vehicle. This is a very challenging task, because vehicles of
different types but similar color and viewpoint can often look much more
similar than vehicles of same type but differing color and viewpoint. Vehicle
make, model, and year in com- bination with vehicle color - are of importance
in several applications such as vehicle search, re-identification, tracking,
and traffic analysis. In this work we investigate the suitability of several
recent landmark convolutional neural network (CNN) architectures, which have
shown top results on large scale image classification tasks, for the task of
fine-grained classification of vehicles. We compare the performance of the
networks VGG16, several ResNets, Inception architectures, the recent DenseNets,
and MobileNet. For classification we use the Stanford Cars-196 dataset which
features 196 different types of vehicles. We investigate several aspects of CNN
training, such as data augmentation and training from scratch vs. fine-tuning.
Importantly, we introduce no aspects in the architectures or training process
which are specific to vehicle classification. Our final model achieves a
state-of-the-art classification accuracy of 94.6% outperforming all related
works, even approaches which are specifically tailored for the task, e.g. by
including vehicle part detections.",['cs.CV'],213,129
Image Synthesis for Data Augmentation in Medical CT using Deep Reinforcement Learning,"Deep learning has shown great promise for CT image reconstruction, in
particular to enable low dose imaging and integrated diagnostics. These merits,
however, stand at great odds with the low availability of diverse image data
which are needed to train these neural networks. We propose to overcome this
bottleneck via a deep reinforcement learning (DRL) approach that is integrated
with a style-transfer (ST) methodology, where the DRL generates the anatomical
shapes and the ST synthesizes the texture detail. We show that our method bears
high promise for generating novel and anatomically accurate high resolution CT
images at large and diverse quantities. Our approach is specifically designed
to work with even small image datasets which is desirable given the often low
amount of image data many researchers have available to them.",['cs.CV'],131,92
Stochastic Aggregation in Graph Neural Networks,"Graph neural networks (GNNs) manifest pathologies including over-smoothing
and limited discriminating power as a result of suboptimally expressive
aggregating mechanisms. We herein present a unifying framework for stochastic
aggregation (STAG) in GNNs, where noise is (adaptively) injected into the
aggregation process from the neighborhood to form node embeddings. We provide
theoretical arguments that STAG models, with little overhead, remedy both of
the aforementioned problems. In addition to fixed-noise models, we also propose
probabilistic versions of STAG models and a variational inference framework to
learn the noise posterior. We conduct illustrative experiments clearly
targeting oversmoothing and multiset aggregation limitations. Furthermore, STAG
enhances general performance of GNNs demonstrated by competitive performance in
common citation and molecule graph benchmark datasets.","['stat.ML', 'cs.AI', 'cs.LG']",119,90
ClickBAIT-v2: Training an Object Detector in Real-Time,"Modern deep convolutional neural networks (CNNs) for image classification and
object detection are often trained offline on large static datasets. Some
applications, however, will require training in real-time on live video streams
with a human-in-the-loop. We refer to this class of problem as time-ordered
online training (ToOT). These problems will require a consideration of not only
the quantity of incoming training data, but the human effort required to
annotate and use it. We demonstrate and evaluate a system tailored to training
an object detector on a live video stream with minimal input from a human
operator. We show that we can obtain bounding box annotation from
weakly-supervised single-point clicks through interactive segmentation.
Furthermore, by exploiting the time-ordered nature of the video stream through
object tracking, we can increase the average training benefit of human
interactions by 3-4 times.","['cs.CV', 'cs.LG', 'cs.RO']",147,101
Probabilistic Graph Attention Network with Conditional Kernels for Pixel-Wise Prediction,"Multi-scale representations deeply learned via convolutional neural networks
have shown tremendous importance for various pixel-level prediction problems.
In this paper we present a novel approach that advances the state of the art on
pixel-level prediction in a fundamental aspect, i.e. structured multi-scale
features learning and fusion. In contrast to previous works directly
considering multi-scale feature maps obtained from the inner layers of a
primary CNN architecture, and simply fusing the features with weighted
averaging or concatenation, we propose a probabilistic graph attention network
structure based on a novel Attention-Gated Conditional Random Fields (AG-CRFs)
model for learning and fusing multi-scale representations in a principled
manner. In order to further improve the learning capacity of the network
structure, we propose to exploit feature dependant conditional kernels within
the deep probabilistic framework. Extensive experiments are conducted on four
publicly available datasets (i.e. BSDS500, NYUD-V2, KITTI, and Pascal-Context)
and on three challenging pixel-wise prediction problems involving both discrete
and continuous labels (i.e. monocular depth estimation, object contour
prediction, and semantic segmentation). Quantitative and qualitative results
demonstrate the effectiveness of the proposed latent AG-CRF model and the
overall probabilistic graph attention network with feature conditional kernels
for structured feature learning and pixel-wise prediction.","['cs.CV', 'eess.IV']",214,133
On the Insufficiency of the Large Margins Theory in Explaining the Performance of Ensemble Methods,"Boosting and other ensemble methods combine a large number of weak
classifiers through weighted voting to produce stronger predictive models. To
explain the successful performance of boosting algorithms, Schapire et al.
(1998) showed that AdaBoost is especially effective at increasing the margins
of the training data. Schapire et al. (1998) also developed an upper bound on
the generalization error of any ensemble based on the margins of the training
data, from which it was concluded that larger margins should lead to lower
generalization error, everything else being equal (sometimes referred to as the
``large margins theory''). Tighter bounds have been derived and have reinforced
the large margins theory hypothesis. For instance, Wang et al. (2011) suggest
that specific margin instances, such as the equilibrium margin, can better
summarize the margins distribution. These results have led many researchers to
consider direct optimization of the margins to improve ensemble generalization
error with mixed results. We show that the large margins theory is not
sufficient for explaining the performance of voting classifiers. We do this by
illustrating how it is possible to improve upon the margin distribution of an
ensemble solution, while keeping the complexity fixed, yet not improve the test
set performance.","['stat.ML', 'cs.LG', 'stat.CO']",200,122
RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation,"We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video
Frame Interpolation (VFI). Many recent flow-based VFI methods first estimate
the bi-directional optical flows, then scale and reverse them to approximate
intermediate flows, leading to artifacts on motion boundaries. RIFE uses a
neural network named IFNet that can directly estimate the intermediate flows
from coarse-to-fine with much better speed. We design a privileged distillation
scheme for training intermediate flow model, which leads to a large performance
improvement. Experiments demonstrate that RIFE is flexible and can achieve
state-of-the-art performance on several public benchmarks. The code is
available at \url{https://github.com/hzwer/arXiv2020-RIFE}","['cs.CV', 'cs.LG']",113,87
Discount Factor as a Regularizer in Reinforcement Learning,"Specifying a Reinforcement Learning (RL) task involves choosing a suitable
planning horizon, which is typically modeled by a discount factor. It is known
that applying RL algorithms with a lower discount factor can act as a
regularizer, improving performance in the limited data regime. Yet the exact
nature of this regularizer has not been investigated. In this work, we fill in
this gap. For several Temporal-Difference (TD) learning methods, we show an
explicit equivalence between using a reduced discount factor and adding an
explicit regularization term to the algorithm's loss. Motivated by the
equivalence, we empirically study this technique compared to standard $L_2$
regularization by extensive experiments in discrete and continuous domains,
using tabular and functional representations. Our experiments suggest the
regularization effectiveness is strongly related to properties of the available
data, such as size, distribution, and mixing rate.","['cs.LG', 'cs.AI', 'stat.ML']",141,99
DeepRhythm: Exposing DeepFakes with Attentional Visual Heartbeat Rhythms,"As the GAN-based face image and video generation techniques, widely known as
DeepFakes, have become more and more matured and realistic, there comes a
pressing and urgent demand for effective DeepFakes detectors. Motivated by the
fact that remote visual photoplethysmography (PPG) is made possible by
monitoring the minuscule periodic changes of skin color due to blood pumping
through the face, we conjecture that normal heartbeat rhythms found in the real
face videos will be disrupted or even entirely broken in a DeepFake video,
making it a potentially powerful indicator for DeepFake detection. In this
work, we propose DeepRhythm, a DeepFake detection technique that exposes
DeepFakes by monitoring the heartbeat rhythms. DeepRhythm utilizes
dual-spatial-temporal attention to adapt to dynamically changing face and fake
types. Extensive experiments on FaceForensics++ and DFDC-preview datasets have
confirmed our conjecture and demonstrated not only the effectiveness, but also
the generalization capability of \emph{DeepRhythm} over different datasets by
various DeepFakes generation techniques and multifarious challenging
degradations.",['cs.CV'],164,115
MinAtar: An Atari-Inspired Testbed for Thorough and Reproducible Reinforcement Learning Experiments,"The Arcade Learning Environment (ALE) is a popular platform for evaluating
reinforcement learning agents. Much of the appeal comes from the fact that
Atari games demonstrate aspects of competency we expect from an intelligent
agent and are not biased toward any particular solution approach. The challenge
of the ALE includes (1) the representation learning problem of extracting
pertinent information from raw pixels, and (2) the behavioural learning problem
of leveraging complex, delayed associations between actions and rewards. Often,
the research questions we are interested in pertain more to the latter, but the
representation learning problem adds significant computational expense. We
introduce MinAtar, short for miniature Atari, a new set of environments that
capture the general mechanics of specific Atari games while simplifying the
representational complexity to focus more on the behavioural challenges.
MinAtar consists of analogues of five Atari games: Seaquest, Breakout, Asterix,
Freeway and Space Invaders. Each MinAtar environment provides the agent with a
10x10xn binary state representation. Each game plays out on a 10x10 grid with n
channels corresponding to game-specific objects, such as ball, paddle and brick
in the game Breakout. To investigate the behavioural challenges posed by
MinAtar, we evaluated a smaller version of the DQN architecture as well as
online actor-critic with eligibility traces. With the representation learning
problem simplified, we can perform experiments with significantly less
computational expense. In our experiments, we use the saved compute time to
perform step-size parameter sweeps and more runs than is typical for the ALE.
Experiments like this improve reproducibility, and allow us to draw more
confident conclusions. We hope that MinAtar can allow researchers to thoroughly
investigate behavioural challenges similar to those inherent in the ALE.","['cs.LG', 'cs.AI']",282,174
A Novel Anomaly Detection Algorithm for Hybrid Production Systems based on Deep Learning and Timed Automata,"Performing anomaly detection in hybrid systems is a challenging task since it
requires analysis of timing behavior and mutual dependencies of both discrete
and continuous signals. Typically, it requires modeling system behavior, which
is often accomplished manually by human engineers. Using machine learning for
creating a behavioral model from observations has advantages, such as lower
development costs and fewer requirements for specific knowledge about the
system. The paper presents DAD:DeepAnomalyDetection, a new approach for
automatic model learning and anomaly detection in hybrid production systems. It
combines deep learning and timed automata for creating behavioral model from
observations. The ability of deep belief nets to extract binary features from
real-valued inputs is used for transformation of continuous to discrete
signals. These signals, together with the original discrete signals are than
handled in an identical way. Anomaly detection is performed by the comparison
of actual and predicted system behavior. The algorithm has been applied to few
data sets including two from real systems and has shown promising results.",['cs.LG'],168,108
Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection,"We present a flexible and high-performance framework, named Pyramid R-CNN,
for two-stage 3D object detection from point clouds. Current approaches
generally rely on the points or voxels of interest for RoI feature extraction
on the second stage, but cannot effectively handle the sparsity and non-uniform
distribution of those points, and this may result in failures in detecting
objects that are far away. To resolve the problems, we propose a novel
second-stage module, named pyramid RoI head, to adaptively learn the features
from the sparse points of interest. The pyramid RoI head consists of three key
components. Firstly, we propose the RoI-grid Pyramid, which mitigates the
sparsity problem by extensively collecting points of interest for each RoI in a
pyramid manner. Secondly, we propose RoI-grid Attention, a new operation that
can encode richer information from sparse points by incorporating conventional
attention-based and graph-based point operators into a unified formulation.
Thirdly, we propose the Density-Aware Radius Prediction (DARP) module, which
can adapt to different point density levels by dynamically adjusting the
focusing range of RoIs. Combining the three components, our pyramid RoI head is
robust to the sparse and imbalanced circumstances, and can be applied upon
various 3D backbones to consistently boost the detection performance. Extensive
experiments show that Pyramid R-CNN outperforms the state-of-the-art 3D
detection models by a large margin on both the KITTI dataset and the Waymo Open
dataset.",['cs.CV'],243,144
Near-Optimal Explainable $k$-Means for All Dimensions,"Many clustering algorithms are guided by certain cost functions such as the
widely-used $k$-means cost. These algorithms divide data points into clusters
with often complicated boundaries, creating difficulties in explaining the
clustering decision. In a recent work, Dasgupta, Frost, Moshkovitz, and
Rashtchian (ICML'20) introduced explainable clustering, where the cluster
boundaries are axis-parallel hyperplanes and the clustering is obtained by
applying a decision tree to the data. The central question here is: how much
does the explainability constraint increase the value of the cost function?
  Given $d$-dimensional data points, we show an efficient algorithm that finds
an explainable clustering whose $k$-means cost is at most $k^{1 -
2/d}\mathrm{poly}(d\log k)$ times the minimum cost achievable by a clustering
without the explainability constraint, assuming $k,d\ge 2$. Combining this with
an independent work by Makarychev and Shan (ICML'21), we get an improved bound
of $k^{1 - 2/d}\mathrm{polylog}(k)$, which we show is optimal for every choice
of $k,d\ge 2$ up to a poly-logarithmic factor in $k$. For $d = 2$ in
particular, we show an $O(\log k\log\log k)$ bound, improving exponentially
over the previous best bound of $\widetilde O(k)$.","['cs.LG', 'cs.CG', 'cs.DS', 'stat.ML']",208,118
Revisiting Model-Agnostic Private Learning: Faster Rates and Active Learning,"The Private Aggregation of Teacher Ensembles (PATE) framework is one of the
most promising recent approaches in differentially private learning. Existing
theoretical analysis shows that PATE consistently learns any VC-classes in the
realizable setting, but falls short in explaining its success in more general
cases where the error rate of the optimal classifier is bounded away from zero.
We fill in this gap by introducing the Tsybakov Noise Condition (TNC) and
establish stronger and more interpretable learning bounds. These bounds provide
new insights into when PATE works and improve over existing results even in the
narrower realizable setting. We also investigate the compelling idea of using
active learning for saving privacy budget, and empirical studies show the
effectiveness of this new idea. The novel components in the proofs include a
more refined analysis of the majority voting classifier -- which could be of
independent interest -- and an observation that the synthetic ""student""
learning problem is nearly realizable by construction under the Tsybakov noise
condition.","['cs.LG', 'cs.CR']",164,114
High-Level Perceptual Similarity is Enabled by Learning Diverse Tasks,"Predicting human perceptual similarity is a challenging subject of ongoing
research. The visual process underlying this aspect of human vision is thought
to employ multiple different levels of visual analysis (shapes, objects,
texture, layout, color, etc). In this paper, we postulate that the perception
of image similarity is not an explicitly learned capability, but rather one
that is a byproduct of learning others. This claim is supported by leveraging
representations learned from a diverse set of visual tasks and using them
jointly to predict perceptual similarity. This is done via simple feature
concatenation, without any further learning. Nevertheless, experiments
performed on the challenging Totally-Looks-Like (TLL) benchmark significantly
surpass recent baselines, closing much of the reported gap towards prediction
of human perceptual similarity. We provide an analysis of these results and
discuss them in a broader context of emergent visual capabilities and their
implications on the course of machine-vision research.","['cs.CV', 'cs.AI', 'cs.LG']",152,106
Identity-Preserving Realistic Talking Face Generation,"Speech-driven facial animation is useful for a variety of applications such
as telepresence, chatbots, etc. The necessary attributes of having a realistic
face animation are 1) audio-visual synchronization (2) identity preservation of
the target individual (3) plausible mouth movements (4) presence of natural eye
blinks. The existing methods mostly address the audio-visual lip
synchronization, and few recent works have addressed the synthesis of natural
eye blinks for overall video realism. In this paper, we propose a method for
identity-preserving realistic facial animation from speech. We first generate
person-independent facial landmarks from audio using DeepSpeech features for
invariance to different voices, accents, etc. To add realism, we impose eye
blinks on facial landmarks using unsupervised learning and retargets the
person-independent landmarks to person-specific landmarks to preserve the
identity-related facial structure which helps in the generation of plausible
mouth shapes of the target identity. Finally, we use LSGAN to generate the
facial texture from person-specific facial landmarks, using an attention
mechanism that helps to preserve identity-related texture. An extensive
comparison of our proposed method with the current state-of-the-art methods
demonstrates a significant improvement in terms of lip synchronization
accuracy, image reconstruction quality, sharpness, and identity-preservation. A
user study also reveals improved realism of our animation results over the
state-of-the-art methods. To the best of our knowledge, this is the first work
in speech-driven 2D facial animation that simultaneously addresses all the
above-mentioned attributes of a realistic speech-driven face animation.",['cs.CV'],257,139
Learn to See by Events: Color Frame Synthesis from Event and RGB Cameras,"Event cameras are biologically-inspired sensors that gather the temporal
evolution of the scene. They capture pixel-wise brightness variations and
output a corresponding stream of asynchronous events. Despite having multiple
advantages with respect to traditional cameras, their use is partially
prevented by the limited applicability of traditional data processing and
vision algorithms. To this aim, we present a framework which exploits the
output stream of event cameras to synthesize RGB frames, relying on an initial
or a periodic set of color key-frames and the sequence of intermediate events.
Differently from existing work, we propose a deep learning-based frame
synthesis method, consisting of an adversarial architecture combined with a
recurrent module. Qualitative results and quantitative per-pixel, perceptual,
and semantic evaluation on four public datasets confirm the quality of the
synthesized images.",['cs.CV'],134,100
Unsupervised Domain Attention Adaptation Network for Caricature Attribute Recognition,"Caricature attributes provide distinctive facial features to help research in
Psychology and Neuroscience. However, unlike the facial photo attribute
datasets that have a quantity of annotated images, the annotations of
caricature attributes are rare. To facility the research in attribute learning
of caricatures, we propose a caricature attribute dataset, namely WebCariA.
Moreover, to utilize models that trained by face attributes, we propose a novel
unsupervised domain adaptation framework for cross-modality (i.e., photos to
caricatures) attribute recognition, with an integrated inter- and intra-domain
consistency learning scheme. Specifically, the inter-domain consistency
learning scheme consisting an image-to-image translator to first fill the
domain gap between photos and caricatures by generating intermediate image
samples, and a label consistency learning module to align their semantic
information. The intra-domain consistency learning scheme integrates the common
feature consistency learning module with a novel attribute-aware
attention-consistency learning module for a more efficient alignment. We did an
extensive ablation study to show the effectiveness of the proposed method. And
the proposed method also outperforms the state-of-the-art methods by a margin.
The implementation of the proposed method is available at
https://github.com/KeleiHe/DAAN.",['cs.CV'],197,115
Segmentation and Restoration of Images on Surfaces by Parametric Active Contours with Topology Changes,"In this article, a new method for segmentation and restoration of images on
two-dimensional surfaces is given. Active contour models for image segmentation
are extended to images on surfaces. The evolving curves on the surfaces are
mathematically described using a parametric approach. For image restoration, a
diffusion equation with Neumann boundary conditions is solved in a
postprocessing step in the individual regions. Numerical schemes are presented
which allow to efficiently compute segmentations and denoised versions of
images on surfaces. Also topology changes of the evolving curves are detected
and performed using a fast sub-routine. Finally, several experiments are
presented where the developed methods are applied on different artificial and
real images defined on different surfaces.","['cs.CV', 'math.AP', 'math.NA']",117,76
AssembleNet++: Assembling Modality Representations via Attention Connections,"We create a family of powerful video models which are able to: (i) learn
interactions between semantic object information and raw appearance and motion
features, and (ii) deploy attention in order to better learn the importance of
features at each convolutional block of the network. A new network component
named peer-attention is introduced, which dynamically learns the attention
weights using another block or input modality. Even without pre-training, our
models outperform the previous work on standard public activity recognition
datasets with continuous videos, establishing new state-of-the-art. We also
confirm that our findings of having neural connections from the object modality
and the use of peer-attention is generally applicable for different existing
architectures, improving their performances. We name our model explicitly as
AssembleNet++. The code will be available at:
https://sites.google.com/corp/view/assemblenet/","['cs.CV', 'cs.LG', 'cs.NE']",141,107
SCAttNet: Semantic Segmentation Network with Spatial and Channel Attention Mechanism for High-Resolution Remote Sensing Images,"High-resolution remote sensing images (HRRSIs) contain substantial ground
object information, such as texture, shape, and spatial location. Semantic
segmentation, which is an important task for element extraction, has been
widely used in processing mass HRRSIs. However, HRRSIs often exhibit large
intraclass variance and small interclass variance due to the diversity and
complexity of ground objects, thereby bringing great challenges to a semantic
segmentation task. In this paper, we propose a new end-to-end semantic
segmentation network, which integrates lightweight spatial and channel
attention modules that can refine features adaptively. We compare our method
with several classic methods on the ISPRS Vaihingen and Potsdam datasets.
Experimental results show that our method can achieve better semantic
segmentation results. The source codes are available at
https://github.com/lehaifeng/SCAttNet.",['cs.CV'],129,103
RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles,"Region proposal algorithms play an important role in most state-of-the-art
two-stage object detection networks by hypothesizing object locations in the
image. Nonetheless, region proposal algorithms are known to be the bottleneck
in most two-stage object detection networks, increasing the processing time for
each image and resulting in slow networks not suitable for real-time
applications such as autonomous driving vehicles. In this paper we introduce
RRPN, a Radar-based real-time region proposal algorithm for object detection in
autonomous driving vehicles. RRPN generates object proposals by mapping Radar
detections to the image coordinate system and generating pre-defined anchor
boxes for each mapped Radar detection point. These anchor boxes are then
transformed and scaled based on the object's distance from the vehicle, to
provide more accurate proposals for the detected objects. We evaluate our
method on the newly released NuScenes dataset [1] using the Fast R-CNN object
detection network [2]. Compared to the Selective Search object proposal
algorithm [3], our model operates more than 100x faster while at the same time
achieves higher detection precision and recall. Code has been made publicly
available at https://github.com/mrnabati/RRPN .",['cs.CV'],196,124
Inter-Battery Topic Representation Learning,"In this paper, we present the Inter-Battery Topic Model (IBTM). Our approach
extends traditional topic models by learning a factorized latent variable
representation. The structured representation leads to a model that marries
benefits traditionally associated with a discriminative approach, such as
feature selection, with those of a generative model, such as principled
regularization and ability to handle missing data. The factorization is
provided by representing data in terms of aligned pairs of observations as
different views. This provides means for selecting a representation that
separately models topics that exist in both views from the topics that are
unique to a single view. This structured consolidation allows for efficient and
robust inference and provides a compact and efficient representation. Learning
is performed in a Bayesian fashion by maximizing a rigorous bound on the
log-likelihood. Firstly, we illustrate the benefits of the model on a synthetic
dataset,. The model is then evaluated in both uni- and multi-modality settings
on two different classification tasks with off-the-shelf convolutional neural
network (CNN) features which generate state-of-the-art results with extremely
compact representations.","['cs.LG', 'cs.CV']",184,118
A Method for Identifying Origin of Digital Images Using a Convolution Neural Network,"The rapid development of deep learning techniques has created new challenges
in identifying the origin of digital images because generative adversarial
networks and variational autoencoders can create plausible digital images whose
contents are not present in natural scenes. In this paper, we consider the
origin that can be broken down into three categories: natural photographic
image (NPI), computer generated graphic (CGG), and deep network generated image
(DGI). A method is presented for effectively identifying the origin of digital
images that is based on a convolutional neural network (CNN) and uses a
local-to-global framework to reduce training complexity. By feeding labeled
data, the CNN is trained to predict the origin of local patches cropped from an
image. The origin of the full-size image is then determined by majority voting.
Unlike previous forensic methods, the CNN takes the raw pixels as input without
the aid of ""residual map"". Experimental results revealed that not only the
high-frequency components but also the middle-frequency ones contribute to
origin identification. The proposed method achieved up to 95.21% identification
accuracy and behaved robustly against several common post-processing operations
including JPEG compression, scaling, geometric transformation, and contrast
stretching. The quantitative results demonstrate that the proposed method is
more effective than handcrafted feature-based methods.",['cs.CV'],213,147
Lifelong Graph Learning,"Graph neural networks (GNNs) are powerful models for many graph-structured
tasks. Existing models often assume that a complete structure of a graph is
available during training, however, in practice, graph-structured data is
usually formed in a streaming fashion, so that learning a graph continuously is
often necessary. In this paper, we aim to bridge GNN to lifelong learning by
converting a graph problem to a regular learning problem, so that GNN is able
to inherit the lifelong learning techniques developed for convolutional neural
networks (CNNs). To this end, we propose a new graph topology based on feature
cross-correlation, called the feature graph. It takes features as new nodes and
turns nodes into independent graphs. This successfully converts the original
problem of node classification to graph classification, in which the increasing
nodes are turned into independent training samples. In the experiments, we
demonstrate the efficiency and effectiveness of feature graph networks (FGN) by
continuously learning a sequence of classical graph datasets. We also show that
FGN achieves superior performance in human action recognition with distributed
streaming signals for wearable devices.","['cs.LG', 'stat.ML']",182,109
Self-supervised learning through the eyes of a child,"Within months of birth, children develop meaningful expectations about the
world around them. How much of this early knowledge can be explained through
generic learning mechanisms applied to sensory data, and how much of it
requires more substantive innate inductive biases? Addressing this fundamental
question in its full generality is currently infeasible, but we can hope to
make real progress in more narrowly defined domains, such as the development of
high-level visual categories, thanks to improvements in data collecting
technology and recent progress in deep learning. In this paper, our goal is
precisely to achieve such progress by utilizing modern self-supervised deep
learning methods and a recent longitudinal, egocentric video dataset recorded
from the perspective of three young children (Sullivan et al., 2020). Our
results demonstrate the emergence of powerful, high-level visual
representations from developmentally realistic natural videos using generic
self-supervised learning objectives.","['cs.CV', 'cs.LG', 'cs.NE']",147,108
Unsupervised Domain Adaptation for Learning Eye Gaze from a Million Synthetic Images: An Adversarial Approach,"With contemporary advancements of graphics engines, recent trend in deep
learning community is to train models on automatically annotated simulated
examples and apply on real data during test time. This alleviates the burden of
manual annotation. However, there is an inherent difference of distributions
between images coming from graphics engine and real world. Such domain
difference deteriorates test time performances of models trained on synthetic
examples. In this paper we address this issue with unsupervised adversarial
feature adaptation across synthetic and real domain for the special use case of
eye gaze estimation which is an essential component for various downstream HCI
tasks. We initially learn a gaze estimator on annotated synthetic samples
rendered from a 3D game engine and then adapt the features of unannotated real
samples via a zero-sum minmax adversarial game against a domain discriminator
following the recent paradigm of generative adversarial networks. Such
adversarial adaptation forces features of both domains to be indistinguishable
which enables us to use regression models trained on synthetic domain to be
used on real samples. On the challenging MPIIGaze real life dataset, we
outperform recent fully supervised methods trained on manually annotated real
samples by appreciable margins and also achieve 13\% more relative gain after
adaptation compared to the current benchmark method of SimGAN",['cs.CV'],213,135
